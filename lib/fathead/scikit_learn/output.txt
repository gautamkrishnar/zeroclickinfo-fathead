Biclustering documents with the Spectral Co-clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			This example demonstrates the Spectral Co-clustering algorithm on the twenty newsgroups dataset. The ‘comp.os.ms-windows.misc’ category is excluded because it contains many posts containing nothing but data.<br><br><pre><code>from __future__ import print_function\n\nprint(__doc__)\n\nfrom collections import defaultdict\nimport operator\nimport re\nfrom time import time\n\nimport numpy as np\n\nfrom sklearn.cluster.bicluster import SpectralCoclustering\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.externals.six import iteritems\nfrom sklearn.datasets.twenty_newsgroups import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.cluster import v_measure_score\n\n\ndef number_aware_tokenizer(doc):\n    """ Tokenizer that maps all numeric tokens to a placeholder.\n\n    For many applications, tokens that begin with a number are not directly\n    useful, but the fact that such a token exists can be relevant.  By applying\n    this form of dimensionality reduction, some methods may perform better.\n    """\n    token_pattern = re.compile(u'(?u)\\b\\w\\w+\\b')\n    tokens = token_pattern.findall(doc)\n    tokens = ["#NUMBER" if token[0] in "0123456789_" else token\n              for token in tokens]\n    return tokens\n\n# exclude 'comp.os.ms-windows.misc'\ncategories = ['alt.atheism', 'comp.graphics',\n              'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n              'comp.windows.x', 'misc.forsale', 'rec.autos',\n              'rec.motorcycles', 'rec.sport.baseball',\n              'rec.sport.hockey', 'sci.crypt', 'sci.electronics',\n              'sci.med', 'sci.space', 'soc.religion.christian',\n              'talk.politics.guns', 'talk.politics.mideast',\n              'talk.politics.misc', 'talk.religion.misc']\nnewsgroups = fetch_20newsgroups(categories=categories)\ny_true = newsgroups.target\n\nvectorizer = TfidfVectorizer(stop_words='english', min_df=5,\n                             tokenizer=number_aware_tokenizer)\ncocluster = SpectralCoclustering(n_clusters=len(categories),\n                                 svd_method='arpack', random_state=0)\nkmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,\n                         random_state=0)\n\nprint("Vectorizing...")\nX = vectorizer.fit_transform(newsgroups.data)\n\nprint("Coclustering...")\nstart_time = time()\ncocluster.fit(X)\ny_cocluster = cocluster.row_labels_\nprint("Done in {:.2f}s. V-measure: {:.4f}".format(\n    time() - start_time,\n    v_measure_score(y_cocluster, y_true)))\n\nprint("MiniBatchKMeans...")\nstart_time = time()\ny_kmeans = kmeans.fit_predict(X)\nprint("Done in {:.2f}s. V-measure: {:.4f}".format(\n    time() - start_time,\n    v_measure_score(y_kmeans, y_true)))\n\nfeature_names = vectorizer.get_feature_names()\ndocument_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n\n\ndef bicluster_ncut(i):\n    rows, cols = cocluster.get_indices(i)\n    if not (np.any(rows) and np.any(cols)):\n        import sys\n        return sys.float_info.max\n    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n    # Note: the following is identical to X[rows[:, np.newaxis], cols].sum() but\n    # much faster in scipy <= 0.16\n    weight = X[rows][:, cols].sum()\n    cut = (X[row_complement][:, cols].sum() +\n           X[rows][:, col_complement].sum())\n    return cut / weight\n\n\ndef most_common(d):\n    """Items of a defaultdict(int) with the highest values.\n\n    Like Counter.most_common in Python >=2.7.\n    """\n    return sorted(iteritems(d), key=operator.itemgetter(1), reverse=True)\n\n\nbicluster_ncuts = list(bicluster_ncut(i)\n                       for i in range(len(newsgroups.target_names)))\nbest_idx = np.argsort(bicluster_ncuts)[:5]\n\nprint()\nprint("Best biclusters:")\nprint("----------------")\nfor idx, cluster in enumerate(best_idx):\n    n_rows, n_cols = cocluster.get_shape(cluster)\n    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n    if not len(cluster_docs) or not len(cluster_words):\n        continue\n\n    # categories\n    counter = defaultdict(int)\n    for i in cluster_docs:\n        counter[document_names[i]] += 1\n    cat_string = ", ".join("{:.0f}% {}".format(float(c) / n_rows * 100, name)\n                           for name, c in most_common(counter)[:3])\n\n    # words\n    out_of_cluster_docs = cocluster.row_labels_ != cluster\n    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n    word_col = X[:, cluster_words]\n    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n                           word_col[out_of_cluster_docs, :].sum(axis=0))\n    word_scores = word_scores.ravel()\n    important_words = list(feature_names[cluster_words[i]]\n                           for i in word_scores.argsort()[:-11:-1])\n\n    print("bicluster {} : {} documents, {} words".format(\n        idx, n_rows, n_cols))\n    print("categories   : {}".format(cat_string))\n    print("words        : {}\n".format(', '.join(important_words)))</code></pre>	http://scikit-learn.org/stable/auto_examples/bicluster/bicluster_newsgroups.html
Digits Classification Exercise	A							http://scikit-learn.org/stable/auto_examples/index.html			A tutorial exercise regarding the use of classification techniques on the Digits dataset.<br><br><pre><code>print(__doc__)\n\nfrom sklearn import datasets, neighbors, linear_model\n\ndigits = datasets.load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\nn_samples = len(X_digits)\n\nX_train = X_digits[:.9 * n_samples]\ny_train = y_digits[:.9 * n_samples]\nX_test = X_digits[.9 * n_samples:]\ny_test = y_digits[.9 * n_samples:]\n\nknn = neighbors.KNeighborsClassifier()\nlogistic = linear_model.LogisticRegression()\n\nprint('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))\nprint('LogisticRegression score: %f'\n      % logistic.fit(X_train, y_train).score(X_test, y_test))</code></pre>	http://scikit-learn.org/stable/auto_examples/exercises/digits_classification_exercise.html
Classification of text documents using sparse features	A							http://scikit-learn.org/stable/auto_examples/index.html			This is an example showing how scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features and demonstrates various classifiers that can efficiently handle sparse matrices.<br><br><pre><code># Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Olivier Grisel <olivier.grisel@ensta.org>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#         Lars Buitinck <L.J.Buitinck@uva.nl>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nimport logging\nimport numpy as np\nfrom optparse import OptionParser\nimport sys\nfrom time import time\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.extmath import density\nfrom sklearn import metrics\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option("--report",\n              action="store_true", dest="print_report",\n              help="Print a detailed classification report.")\nop.add_option("--chi2_select",\n              action="store", type="int", dest="select_chi2",\n              help="Select some number of features using a chi-squared test")\nop.add_option("--confusion_matrix",\n              action="store_true", dest="print_cm",\n              help="Print the confusion matrix.")\nop.add_option("--top10",\n              action="store_true", dest="print_top10",\n              help="Print ten most discriminative terms per class"\n                   " for every classifier.")\nop.add_option("--all_categories",\n              action="store_true", dest="all_categories",\n              help="Whether to use all categories or not.")\nop.add_option("--use_hashing",\n              action="store_true",\n              help="Use a hashing vectorizer.")\nop.add_option("--n_features",\n              action="store", type=int, default=2 ** 16,\n              help="n_features when using the hashing vectorizer.")\nop.add_option("--filtered",\n              action="store_true",\n              help="Remove newsgroup information that is easily overfit: "\n                   "headers, signatures, and quoting.")\n\n(opts, args) = op.parse_args()\nif len(args) > 0:\n    op.error("this script takes no arguments.")\n    sys.exit(1)\n\nprint(__doc__)\nop.print_help()\nprint()\n\n\n###############################################################################\n# Load some categories from the training set\nif opts.all_categories:\n    categories = None\nelse:\n    categories = [\n        'alt.atheism',\n        'talk.religion.misc',\n        'comp.graphics',\n        'sci.space',\n    ]\n\nif opts.filtered:\n    remove = ('headers', 'footers', 'quotes')\nelse:\n    remove = ()\n\nprint("Loading 20 newsgroups dataset for categories:")\nprint(categories if categories else "all")\n\ndata_train = fetch_20newsgroups(subset='train', categories=categories,\n                                shuffle=True, random_state=42,\n                                remove=remove)\n\ndata_test = fetch_20newsgroups(subset='test', categories=categories,\n                               shuffle=True, random_state=42,\n                               remove=remove)\nprint('data loaded')\n\ncategories = data_train.target_names    # for case categories == None\n\n\ndef size_mb(docs):\n    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n\ndata_train_size_mb = size_mb(data_train.data)\ndata_test_size_mb = size_mb(data_test.data)\n\nprint("%d documents - %0.3fMB (training set)" % (\n    len(data_train.data), data_train_size_mb))\nprint("%d documents - %0.3fMB (test set)" % (\n    len(data_test.data), data_test_size_mb))\nprint("%d categories" % len(categories))\nprint()\n\n# split a training set and a test set\ny_train, y_test = data_train.target, data_test.target\n\nprint("Extracting features from the training data using a sparse vectorizer")\nt0 = time()\nif opts.use_hashing:\n    vectorizer = HashingVectorizer(stop_words='english', non_negative=True,\n                                   n_features=opts.n_features)\n    X_train = vectorizer.transform(data_train.data)\nelse:\n    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                 stop_words='english')\n    X_train = vectorizer.fit_transform(data_train.data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_train_size_mb / duration))\nprint("n_samples: %d, n_features: %d" % X_train.shape)\nprint()\n\nprint("Extracting features from the test data using the same vectorizer")\nt0 = time()\nX_test = vectorizer.transform(data_test.data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_test_size_mb / duration))\nprint("n_samples: %d, n_features: %d" % X_test.shape)\nprint()\n\n# mapping from integer feature name to original token string\nif opts.use_hashing:\n    feature_names = None\nelse:\n    feature_names = vectorizer.get_feature_names()\n\nif opts.select_chi2:\n    print("Extracting %d best features by a chi-squared test" %\n          opts.select_chi2)\n    t0 = time()\n    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n    X_train = ch2.fit_transform(X_train, y_train)\n    X_test = ch2.transform(X_test)\n    if feature_names:\n        # keep selected feature names\n        feature_names = [feature_names[i] for i\n                         in ch2.get_support(indices=True)]\n    print("done in %fs" % (time() - t0))\n    print()\n\nif feature_names:\n    feature_names = np.asarray(feature_names)\n\n\ndef trim(s):\n    """Trim string to fit on terminal (assuming 80-column display)"""\n    return s if len(s) <= 80 else s[:77] + "..."\n\n\n###############################################################################\n# Benchmark classifiers\ndef benchmark(clf):\n    print('_' * 80)\n    print("Training: ")\n    print(clf)\n    t0 = time()\n    clf.fit(X_train, y_train)\n    train_time = time() - t0\n    print("train time: %0.3fs" % train_time)\n\n    t0 = time()\n    pred = clf.predict(X_test)\n    test_time = time() - t0\n    print("test time:  %0.3fs" % test_time)\n\n    score = metrics.accuracy_score(y_test, pred)\n    print("accuracy:   %0.3f" % score)\n\n    if hasattr(clf, 'coef_'):\n        print("dimensionality: %d" % clf.coef_.shape[1])\n        print("density: %f" % density(clf.coef_))\n\n        if opts.print_top10 and feature_names is not None:\n            print("top 10 keywords per class:")\n            for i, category in enumerate(categories):\n                top10 = np.argsort(clf.coef_[i])[-10:]\n                print(trim("%s: %s"\n                      % (category, " ".join(feature_names[top10]))))\n        print()\n\n    if opts.print_report:\n        print("classification report:")\n        print(metrics.classification_report(y_test, pred,\n                                            target_names=categories))\n\n    if opts.print_cm:\n        print("confusion matrix:")\n        print(metrics.confusion_matrix(y_test, pred))\n\n    print()\n    clf_descr = str(clf).split('(')[0]\n    return clf_descr, score, train_time, test_time\n\n\nresults = []\nfor clf, name in (\n        (RidgeClassifier(tol=1e-2, solver="lsqr"), "Ridge Classifier"),\n        (Perceptron(n_iter=50), "Perceptron"),\n        (PassiveAggressiveClassifier(n_iter=50), "Passive-Aggressive"),\n        (KNeighborsClassifier(n_neighbors=10), "kNN"),\n        (RandomForestClassifier(n_estimators=100), "Random forest")):\n    print('=' * 80)\n    print(name)\n    results.append(benchmark(clf))\n\nfor penalty in ["l2", "l1"]:\n    print('=' * 80)\n    print("%s penalty" % penalty.upper())\n    # Train Liblinear model\n    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n                                            dual=False, tol=1e-3)))\n\n    # Train SGD model\n    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n                                           penalty=penalty)))\n\n# Train SGD with Elastic Net penalty\nprint('=' * 80)\nprint("Elastic-Net penalty")\nresults.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n                                       penalty="elasticnet")))\n\n# Train NearestCentroid without threshold\nprint('=' * 80)\nprint("NearestCentroid (aka Rocchio classifier)")\nresults.append(benchmark(NearestCentroid()))\n\n# Train sparse Naive Bayes classifiers\nprint('=' * 80)\nprint("Naive Bayes")\nresults.append(benchmark(MultinomialNB(alpha=.01)))\nresults.append(benchmark(BernoulliNB(alpha=.01)))\n\nprint('=' * 80)\nprint("LinearSVC with L1-based feature selection")\n# The smaller C, the stronger the regularization.\n# The more regularization, the more sparsity.\nresults.append(benchmark(Pipeline([\n  ('feature_selection', LinearSVC(penalty="l1", dual=False, tol=1e-3)),\n  ('classification', LinearSVC())\n])))\n\n# make some plots\n\nindices = np.arange(len(results))\n\nresults = [[x[i] for x in results] for i in range(4)]\n\nclf_names, score, training_time, test_time = results\ntraining_time = np.array(training_time) / np.max(training_time)\ntest_time = np.array(test_time) / np.max(test_time)\n\nplt.figure(figsize=(12, 8))\nplt.title("Score")\nplt.barh(indices, score, .2, label="score", color='r')\nplt.barh(indices + .3, training_time, .2, label="training time", color='g')\nplt.barh(indices + .6, test_time, .2, label="test time", color='b')\nplt.yticks(())\nplt.legend(loc='best')\nplt.subplots_adjust(left=.25)\nplt.subplots_adjust(top=.95)\nplt.subplots_adjust(bottom=.05)\n\nfor i, c in zip(indices, clf_names):\n    plt.text(-.3, i, c)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html
Clustering text documents using k-means	A							http://scikit-learn.org/stable/auto_examples/index.html			This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.<br><br><pre><code># Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck <L.J.Buitinck@uva.nl>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option("--lsa",\n              dest="n_components", type="int",\n              help="Preprocess documents with latent semantic analysis.")\nop.add_option("--no-minibatch",\n              action="store_false", dest="minibatch", default=True,\n              help="Use ordinary k-means algorithm (in batch mode).")\nop.add_option("--no-idf",\n              action="store_false", dest="use_idf", default=True,\n              help="Disable Inverse Document Frequency feature weighting.")\nop.add_option("--use-hashing",\n              action="store_true", default=False,\n              help="Use a hashing feature vectorizer")\nop.add_option("--n-features", type=int, default=10000,\n              help="Maximum number of features (dimensions)"\n                   " to extract from text.")\nop.add_option("--verbose",\n              action="store_true", dest="verbose", default=False,\n              help="Print progress reports inside k-means algorithm.")\n\nprint(__doc__)\nop.print_help()\n\n(opts, args) = op.parse_args()\nif len(args) > 0:\n    op.error("this script takes no arguments.")\n    sys.exit(1)\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n    'comp.graphics',\n    'sci.space',\n]\n# Uncomment the following to do the analysis on all the categories\n#categories = None\n\nprint("Loading 20 newsgroups dataset for categories:")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset='all', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint("%d documents" % len(dataset.data))\nprint("%d categories" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint("Extracting features from the training dataset using a sparse vectorizer")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words='english', non_negative=True,\n                                   norm=None, binary=False)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words='english',\n                                       non_negative=False, norm='l2',\n                                       binary=False)\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words='english',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint("done in %fs" % (time() - t0))\nprint("n_samples: %d, n_features: %d" % X.shape)\nprint()\n\nif opts.n_components:\n    print("Performing dimensionality reduction using LSA")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print("done in %fs" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print("Explained variance of the SVD step: {}%".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n###############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint("Clustering sparse data with %s" % km)\nt0 = time()\nkm.fit(X)\nprint("done in %0.3fs" % (time() - t0))\nprint()\n\nprint("Homogeneity: %0.3f" % metrics.homogeneity_score(labels, km.labels_))\nprint("Completeness: %0.3f" % metrics.completeness_score(labels, km.labels_))\nprint("V-measure: %0.3f" % metrics.v_measure_score(labels, km.labels_))\nprint("Adjusted Rand-Index: %.3f"\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint("Silhouette Coefficient: %0.3f"\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print("Top terms per cluster:")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print("Cluster %d:" % i, end='')\n        for ind in order_centroids[i, :10]:\n            print(' %s' % terms[ind], end='')\n        print()</code></pre>	http://scikit-learn.org/stable/auto_examples/text/document_clustering.html
Faces recognition example using eigenfaces and SVMs	A							http://scikit-learn.org/stable/auto_examples/index.html			The dataset used in this example is a preprocessed excerpt of the “Labeled Faces in the Wild”, aka LFW:<br><br><pre><code>from __future__ import print_function\n\nfrom time import time\nimport logging\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import RandomizedPCA\nfrom sklearn.svm import SVC\n\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')\n\n\n###############################################################################\n# Download the data, if not already on disk and load it as numpy arrays\n\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint("Total dataset size:")\nprint("n_samples: %d" % n_samples)\nprint("n_features: %d" % n_features)\nprint("n_classes: %d" % n_classes)\n\n\n###############################################################################\n# Split into a training set and a test set using a stratified k fold\n\n# split into a training and testing set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n\n\n###############################################################################\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n# dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 150\n\nprint("Extracting the top %d eigenfaces from %d faces"\n      % (n_components, X_train.shape[0]))\nt0 = time()\npca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\nprint("done in %0.3fs" % (time() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint("Projecting the input data on the eigenfaces orthonormal basis")\nt0 = time()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint("done in %0.3fs" % (time() - t0))\n\n\n###############################################################################\n# Train a SVM classification model\n\nprint("Fitting the classifier to the training set")\nt0 = time()\nparam_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5],\n              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint("done in %0.3fs" % (time() - t0))\nprint("Best estimator found by grid search:")\nprint(clf.best_estimator_)\n\n\n###############################################################################\n# Quantitative evaluation of the model quality on the test set\n\nprint("Predicting people's names on the test set")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint("done in %0.3fs" % (time() - t0))\n\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\n\n###############################################################################\n# Qualitative evaluation of the predictions using matplotlib\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    """Helper function to plot a gallery of portraits"""\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\n\n# plot the result of the prediction on a portion of the test set\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n    return 'predicted: %s\ntrue:      %s' % (pred_name, true_name)\n\nprediction_titles = [title(y_pred, y_test, target_names, i)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n# plot the gallery of the most significative eigenfaces\n\neigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html
Pipeline Anova SVM	A							http://scikit-learn.org/stable/auto_examples/index.html			Simple usage of Pipeline that runs successively a univariate feature selection with anova and then a C-SVM of the selected features.<br><br><pre><code>print(__doc__)\n\nfrom sklearn import svm\nfrom sklearn.datasets import samples_generator\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.pipeline import make_pipeline\n\n# import some data to play with\nX, y = samples_generator.make_classification(\n    n_features=20, n_informative=3, n_redundant=0, n_classes=4,\n    n_clusters_per_class=2)\n\n# ANOVA SVM-C\n# 1) anova filter, take 3 best ranked features\nanova_filter = SelectKBest(f_regression, k=3)\n# 2) svm\nclf = svm.SVC(kernel='linear')\n\nanova_svm = make_pipeline(anova_filter, clf)\nanova_svm.fit(X, y)\nanova_svm.predict(X)</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/feature_selection_pipeline.html
Concatenating multiple feature extraction methods	A							http://scikit-learn.org/stable/auto_examples/index.html			In many real-world examples, there are many ways to extract features from a dataset. Often it is beneficial to combine several methods to obtain good performance. This example shows how to use FeatureUnion to combine features obtained by PCA and univariate selection.<br><br><pre><code># Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#\n# License: BSD 3 clause\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\niris = load_iris()\n\nX, y = iris.data, iris.target\n\n# This dataset is way to high-dimensional. Better do PCA:\npca = PCA(n_components=2)\n\n# Maybe some original features where good, too?\nselection = SelectKBest(k=1)\n\n# Build estimator from PCA and Univariate selection:\n\ncombined_features = FeatureUnion([("pca", pca), ("univ_select", selection)])\n\n# Use combined features to transform dataset:\nX_features = combined_features.fit(X, y).transform(X)\n\nsvm = SVC(kernel="linear")\n\n# Do grid search over k, n_components and C:\n\npipeline = Pipeline([("features", combined_features), ("svm", svm)])\n\nparam_grid = dict(features__pca__n_components=[1, 2, 3],\n                  features__univ_select__k=[1, 2],\n                  svm__C=[0.1, 1, 10])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_stacker.html
Gaussian Processes regression: goodness-of-fit on the ‘diabetes’ dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			In this example, we fit a Gaussian Process model onto the diabetes dataset.<br><br><pre><code>print(__doc__)\n\n# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n# Licence: BSD 3 clause\n\nfrom sklearn import datasets\nfrom sklearn.gaussian_process import GaussianProcess\nfrom sklearn.cross_validation import cross_val_score, KFold\n\n# Load the dataset from scikit's data sets\ndiabetes = datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Instanciate a GP model\ngp = GaussianProcess(regr='constant', corr='absolute_exponential',\n                     theta0=[1e-4] * 10, thetaL=[1e-12] * 10,\n                     thetaU=[1e-2] * 10, nugget=1e-2, optimizer='Welch')\n\n# Fit the GP model to the data performing maximum likelihood estimation\ngp.fit(X, y)\n\n# Deactivate maximum likelihood estimation for the cross-validation loop\ngp.theta0 = gp.theta_  # Given correlation parameter = MLE\ngp.thetaL, gp.thetaU = None, None  # None bounds deactivate MLE\n\n# Perform a cross-validation estimate of the coefficient of determination using\n# the cross_validation module using all CPUs available on the machine\nK = 20  # folds\nR2 = cross_val_score(gp, X, y=y, cv=KFold(y.size, K), n_jobs=1).mean()\nprint("The %d-Folds estimate of the coefficient of determination is R2 = %s"\n      % (K, R2))</code></pre>	http://scikit-learn.org/stable/auto_examples/gaussian_process/gp_diabetes_dataset.html
Parameter estimation using grid search with cross-validation	A							http://scikit-learn.org/stable/auto_examples/index.html			This examples shows how a classifier is optimized by cross-validation, which is done using the sklearn.grid_search.GridSearchCV object on a development set that comprises only half of the available labeled data.<br><br><pre><code>from __future__ import print_function\n\nfrom sklearn import datasets\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\n\nprint(__doc__)\n\n# Loading the Digits dataset\ndigits = datasets.load_digits()\n\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target\n\n# Split the dataset in two equal parts\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=0)\n\n# Set the parameters by cross-validation\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print("# Tuning hyper-parameters for %s" % score)\n    print()\n\n    clf = GridSearchCV(SVC(C=1), tuned_parameters, cv=5,\n                       scoring='%s_weighted' % score)\n    clf.fit(X_train, y_train)\n\n    print("Best parameters set found on development set:")\n    print()\n    print(clf.best_params_)\n    print()\n    print("Grid scores on development set:")\n    print()\n    for params, mean_score, scores in clf.grid_scores_:\n        print("%0.3f (+/-%0.03f) for %r"\n              % (mean_score, scores.std() * 2, params))\n    print()\n\n    print("Detailed classification report:")\n    print()\n    print("The model is trained on the full development set.")\n    print("The scores are computed on the full evaluation set.")\n    print()\n    y_true, y_pred = y_test, clf.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    print()\n\n# Note the problem is too easy: the hyperparameter plateau is too flat and the\n# output model is the same for precision and recall with ties in quality.</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_digits.html
Sample pipeline for text feature extraction and evaluation	A							http://scikit-learn.org/stable/auto_examples/index.html			The dataset used in this example is the 20 newsgroups dataset which will be automatically downloaded and then cached and reused for the document classification example.<br><br><pre><code># Author: Olivier Grisel <olivier.grisel@ensta.org>\n#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Mathieu Blondel <mathieu@mblondel.org>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom pprint import pprint\nfrom time import time\nimport logging\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n\n###############################################################################\n# Load some categories from the training set\ncategories = [\n    'alt.atheism',\n    'talk.religion.misc',\n]\n# Uncomment the following to do the analysis on all the categories\n#categories = None\n\nprint("Loading 20 newsgroups dataset for categories:")\nprint(categories)\n\ndata = fetch_20newsgroups(subset='train', categories=categories)\nprint("%d documents" % len(data.filenames))\nprint("%d categories" % len(data.target_names))\nprint()\n\n###############################################################################\n# define a pipeline combining a text feature extractor with a simple\n# classifier\npipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier()),\n])\n\n# uncommenting more parameters will give better exploring power but will\n# increase processing time in a combinatorial way\nparameters = {\n    'vect__max_df': (0.5, 0.75, 1.0),\n    #'vect__max_features': (None, 5000, 10000, 50000),\n    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n    #'tfidf__use_idf': (True, False),\n    #'tfidf__norm': ('l1', 'l2'),\n    'clf__alpha': (0.00001, 0.000001),\n    'clf__penalty': ('l2', 'elasticnet'),\n    #'clf__n_iter': (10, 50, 80),\n}\n\nif __name__ == "__main__":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print("Performing grid search...")\n    print("pipeline:", [name for name, _ in pipeline.steps])\n    print("parameters:")\n    pprint(parameters)\n    t0 = time()\n    grid_search.fit(data.data, data.target)\n    print("done in %0.3fs" % (time() - t0))\n    print()\n\n    print("Best score: %0.3f" % grid_search.best_score_)\n    print("Best parameters set:")\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print("\t%s: %r" % (param_name, best_parameters[param_name]))</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html
FeatureHasher and DictVectorizer Comparison	A							http://scikit-learn.org/stable/auto_examples/index.html			Compares FeatureHasher and DictVectorizer by using both to vectorize text documents.<br><br><pre><code># Author: Lars Buitinck <L.J.Buitinck@uva.nl>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\nfrom collections import defaultdict\nimport re\nimport sys\nfrom time import time\n\nimport numpy as np\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction import DictVectorizer, FeatureHasher\n\n\ndef n_nonzero_columns(X):\n    """Returns the number of non-zero columns in a CSR matrix X."""\n    return len(np.unique(X.nonzero()[1]))\n\n\ndef tokens(doc):\n    """Extract tokens from doc.\n\n    This uses a simple regex to break strings into tokens. For a more\n    principled approach, see CountVectorizer or TfidfVectorizer.\n    """\n    return (tok.lower() for tok in re.findall(r"\w+", doc))\n\n\ndef token_freqs(doc):\n    """Extract a dict mapping tokens from doc to their frequencies."""\n    freq = defaultdict(int)\n    for tok in tokens(doc):\n        freq[tok] += 1\n    return freq\n\n\ncategories = [\n    'alt.atheism',\n    'comp.graphics',\n    'comp.sys.ibm.pc.hardware',\n    'misc.forsale',\n    'rec.autos',\n    'sci.space',\n    'talk.religion.misc',\n]\n# Uncomment the following line to use a larger set (11k+ documents)\n#categories = None\n\nprint(__doc__)\nprint("Usage: %s [n_features_for_hashing]" % sys.argv[0])\nprint("    The default number of features is 2**18.")\nprint()\n\ntry:\n    n_features = int(sys.argv[1])\nexcept IndexError:\n    n_features = 2 ** 18\nexcept ValueError:\n    print("not a valid number of features: %r" % sys.argv[1])\n    sys.exit(1)\n\n\nprint("Loading 20 newsgroups training data")\nraw_data = fetch_20newsgroups(subset='train', categories=categories).data\ndata_size_mb = sum(len(s.encode('utf-8')) for s in raw_data) / 1e6\nprint("%d documents - %0.3fMB" % (len(raw_data), data_size_mb))\nprint()\n\nprint("DictVectorizer")\nt0 = time()\nvectorizer = DictVectorizer()\nvectorizer.fit_transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))\nprint("Found %d unique terms" % len(vectorizer.get_feature_names()))\nprint()\n\nprint("FeatureHasher on frequency dicts")\nt0 = time()\nhasher = FeatureHasher(n_features=n_features)\nX = hasher.transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))\nprint("Found %d unique terms" % n_nonzero_columns(X))\nprint()\n\nprint("FeatureHasher on raw tokens")\nt0 = time()\nhasher = FeatureHasher(n_features=n_features, input_type="string")\nX = hasher.transform(tokens(d) for d in raw_data)\nduration = time() - t0\nprint("done in %fs at %0.3fMB/s" % (duration, data_size_mb / duration))\nprint("Found %d unique terms" % n_nonzero_columns(X))</code></pre>	http://scikit-learn.org/stable/auto_examples/text/hashing_vs_dict_vectorizer.html
Feature Union with Heterogeneous Data Sources	A							http://scikit-learn.org/stable/auto_examples/index.html			Datasets can often contain components of that require different feature extraction and processing pipelines. This scenario might occur when:<br><br><pre><code># Author: Matt Terry <matt.terry@gmail.com>\n#\n# License: BSD 3 clause\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\nfrom sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\n\nclass ItemSelector(BaseEstimator, TransformerMixin):\n    """For data grouped by feature, select subset of data at a provided key.\n\n    The data is expected to be stored in a 2D data structure, where the first\n    index is over features and the second is over samples.  i.e.\n\n    >> len(data[key]) == n_samples\n\n    Please note that this is the opposite convention to sklearn feature\n    matrixes (where the first index corresponds to sample).\n\n    ItemSelector only requires that the collection implement getitem\n    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n    DataFrame, numpy record array, etc.\n\n    >> data = {'a': [1, 5, 2, 5, 2, 8],\n               'b': [9, 4, 1, 4, 1, 3]}\n    >> ds = ItemSelector(key='a')\n    >> data['a'] == ds.transform(data)\n\n    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n    list of dicts).  If your data is structured this way, consider a\n    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n\n    Parameters\n    ----------\n    key : hashable, required\n        The key corresponding to the desired value in a mappable.\n    """\n    def __init__(self, key):\n        self.key = key\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_dict):\n        return data_dict[self.key]\n\n\nclass TextStats(BaseEstimator, TransformerMixin):\n    """Extract features from each document for DictVectorizer"""\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, posts):\n        return [{'length': len(text),\n                 'num_sentences': text.count('.')}\n                for text in posts]\n\n\nclass SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n    """Extract the subject & body from a usenet post in a single pass.\n\n    Takes a sequence of strings and produces a dict of sequences.  Keys are\n    `subject` and `body`.\n    """\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, posts):\n        features = np.recarray(shape=(len(posts),),\n                               dtype=[('subject', object), ('body', object)])\n        for i, text in enumerate(posts):\n            headers, _, bod = text.partition('\n\n')\n            bod = strip_newsgroup_footer(bod)\n            bod = strip_newsgroup_quoting(bod)\n            features['body'][i] = bod\n\n            prefix = 'Subject:'\n            sub = ''\n            for line in headers.split('\n'):\n                if line.startswith(prefix):\n                    sub = line[len(prefix):]\n                    break\n            features['subject'][i] = sub\n\n        return features\n\n\npipeline = Pipeline([\n    # Extract the subject & body\n    ('subjectbody', SubjectBodyExtractor()),\n\n    # Use FeatureUnion to combine the features from subject and body\n    ('union', FeatureUnion(\n        transformer_list=[\n\n            # Pipeline for pulling features from the post's subject line\n            ('subject', Pipeline([\n                ('selector', ItemSelector(key='subject')),\n                ('tfidf', TfidfVectorizer(min_df=50)),\n            ])),\n\n            # Pipeline for standard bag-of-words model for body\n            ('body_bow', Pipeline([\n                ('selector', ItemSelector(key='body')),\n                ('tfidf', TfidfVectorizer()),\n                ('best', TruncatedSVD(n_components=50)),\n            ])),\n\n            # Pipeline for pulling ad hoc features from post's body\n            ('body_stats', Pipeline([\n                ('selector', ItemSelector(key='body')),\n                ('stats', TextStats()),  # returns a list of dicts\n                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n            ])),\n\n        ],\n\n        # weight components in FeatureUnion\n        transformer_weights={\n            'subject': 0.8,\n            'body_bow': 0.5,\n            'body_stats': 1.0,\n        },\n    )),\n\n    # Use a SVC classifier on the combined features\n    ('svc', SVC(kernel='linear')),\n])\n\n# limit the list of categories to make running this exmaple faster.\ncategories = ['alt.atheism', 'talk.religion.misc']\ntrain = fetch_20newsgroups(random_state=1,\n                           subset='train',\n                           categories=categories,\n                           )\ntest = fetch_20newsgroups(random_state=1,\n                          subset='test',\n                          categories=categories,\n                          )\n\npipeline.fit(train.data, train.target)\ny = pipeline.predict(test.data)\nprint(classification_report(y, test.target))</code></pre>	http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html
Examples	A							http://scikit-learn.org/stable/auto_examples/index.html			General-purpose and introductory examples for the scikit.<br><br><pre><code></code></pre>	http://scikit-learn.org/stable/auto_examples/index.html
Lasso on dense and sparse data	A							http://scikit-learn.org/stable/auto_examples/index.html			We show that linear_model.Lasso provides the same results for dense and sparse data and that in the case of sparse data the speed is improved.<br><br><pre><code>print(__doc__)\n\nfrom time import time\nfrom scipy import sparse\nfrom scipy import linalg\n\nfrom sklearn.datasets.samples_generator import make_regression\nfrom sklearn.linear_model import Lasso\n\n\n###############################################################################\n# The two Lasso implementations on Dense data\nprint("--- Dense matrices")\n\nX, y = make_regression(n_samples=200, n_features=5000, random_state=0)\nX_sp = sparse.coo_matrix(X)\n\nalpha = 1\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\n\nt0 = time()\nsparse_lasso.fit(X_sp, y)\nprint("Sparse Lasso done in %fs" % (time() - t0))\n\nt0 = time()\ndense_lasso.fit(X, y)\nprint("Dense Lasso done in %fs" % (time() - t0))\n\nprint("Distance between coefficients : %s"\n      % linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))\n\n###############################################################################\n# The two Lasso implementations on Sparse data\nprint("--- Sparse matrices")\n\nXs = X.copy()\nXs[Xs < 2.5] = 0.0\nXs = sparse.coo_matrix(Xs)\nXs = Xs.tocsc()\n\nprint("Matrix density : %s %%" % (Xs.nnz / float(X.size) * 100))\n\nalpha = 0.1\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = time()\nsparse_lasso.fit(Xs, y)\nprint("Sparse Lasso done in %fs" % (time() - t0))\n\nt0 = time()\ndense_lasso.fit(Xs.toarray(), y)\nprint("Dense Lasso done in %fs" % (time() - t0))\n\nprint("Distance between coefficients : %s"\n      % linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/lasso_dense_vs_sparse_data.html
Imputing missing values before building an estimator	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows that imputing the missing values can give better results than discarding the samples containing any missing value. Imputing does not always improve the predictions, so please check via cross-validation. Sometimes dropping rows or using marker values is more effective.<br><br><pre><code>import numpy as np\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.cross_validation import cross_val_score\n\nrng = np.random.RandomState(0)\n\ndataset = load_boston()\nX_full, y_full = dataset.data, dataset.target\nn_samples = X_full.shape[0]\nn_features = X_full.shape[1]\n\n# Estimate the score on the entire dataset, with no missing values\nestimator = RandomForestRegressor(random_state=0, n_estimators=100)\nscore = cross_val_score(estimator, X_full, y_full).mean()\nprint("Score with the entire dataset = %.2f" % score)\n\n# Add missing values in 75% of the lines\nmissing_rate = 0.75\nn_missing_samples = np.floor(n_samples * missing_rate)\nmissing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,\n                                      dtype=np.bool),\n                             np.ones(n_missing_samples,\n                                     dtype=np.bool)))\nrng.shuffle(missing_samples)\nmissing_features = rng.randint(0, n_features, n_missing_samples)\n\n# Estimate the score without the lines containing missing values\nX_filtered = X_full[~missing_samples, :]\ny_filtered = y_full[~missing_samples]\nestimator = RandomForestRegressor(random_state=0, n_estimators=100)\nscore = cross_val_score(estimator, X_filtered, y_filtered).mean()\nprint("Score without the samples containing missing values = %.2f" % score)\n\n# Estimate the score after imputation of the missing values\nX_missing = X_full.copy()\nX_missing[np.where(missing_samples)[0], missing_features] = 0\ny_missing = y_full.copy()\nestimator = Pipeline([("imputer", Imputer(missing_values=0,\n                                          strategy="mean",\n                                          axis=0)),\n                      ("forest", RandomForestRegressor(random_state=0,\n                                                       n_estimators=100))])\nscore = cross_val_score(estimator, X_missing, y_missing).mean()\nprint("Score after imputation of the missing values = %.2f" % score)</code></pre>	http://scikit-learn.org/stable/auto_examples/missing_values.html
Classification of text documents: using a MLComp dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			This is an example showing how the scikit-learn can be used to classify documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays.<br><br><pre><code># Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom time import time\nimport sys\nimport os\nimport numpy as np\nimport scipy.sparse as sp\nimport pylab as pl\n\nfrom sklearn.datasets import load_mlcomp\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\n\n\nprint(__doc__)\n\nif 'MLCOMP_DATASETS_HOME' not in os.environ:\n    print("MLCOMP_DATASETS_HOME not set; please follow the above instructions")\n    sys.exit(0)\n\n# Load the training set\nprint("Loading 20 newsgroups training set... ")\nnews_train = load_mlcomp('20news-18828', 'train')\nprint(news_train.DESCR)\nprint("%d documents" % len(news_train.filenames))\nprint("%d categories" % len(news_train.target_names))\n\nprint("Extracting features from the dataset using a sparse vectorizer")\nt0 = time()\nvectorizer = TfidfVectorizer(encoding='latin1')\nX_train = vectorizer.fit_transform((open(f).read()\n                                    for f in news_train.filenames))\nprint("done in %fs" % (time() - t0))\nprint("n_samples: %d, n_features: %d" % X_train.shape)\nassert sp.issparse(X_train)\ny_train = news_train.target\n\nprint("Loading 20 newsgroups test set... ")\nnews_test = load_mlcomp('20news-18828', 'test')\nt0 = time()\nprint("done in %fs" % (time() - t0))\n\nprint("Predicting the labels of the test set...")\nprint("%d documents" % len(news_test.filenames))\nprint("%d categories" % len(news_test.target_names))\n\nprint("Extracting features from the dataset using the same vectorizer")\nt0 = time()\nX_test = vectorizer.transform((open(f).read() for f in news_test.filenames))\ny_test = news_test.target\nprint("done in %fs" % (time() - t0))\nprint("n_samples: %d, n_features: %d" % X_test.shape)\n\n\n###############################################################################\n# Benchmark classifiers\ndef benchmark(clf_class, params, name):\n    print("parameters:", params)\n    t0 = time()\n    clf = clf_class(**params).fit(X_train, y_train)\n    print("done in %fs" % (time() - t0))\n\n    if hasattr(clf, 'coef_'):\n        print("Percentage of non zeros coef: %f"\n              % (np.mean(clf.coef_ != 0) * 100))\n    print("Predicting the outcomes of the testing set")\n    t0 = time()\n    pred = clf.predict(X_test)\n    print("done in %fs" % (time() - t0))\n\n    print("Classification report on test set for classifier:")\n    print(clf)\n    print()\n    print(classification_report(y_test, pred,\n                                target_names=news_test.target_names))\n\n    cm = confusion_matrix(y_test, pred)\n    print("Confusion matrix:")\n    print(cm)\n\n    # Show confusion matrix\n    pl.matshow(cm)\n    pl.title('Confusion matrix of the %s classifier' % name)\n    pl.colorbar()\n\n\nprint("Testbenching a linear classifier...")\nparameters = {\n    'loss': 'hinge',\n    'penalty': 'l2',\n    'n_iter': 50,\n    'alpha': 0.00001,\n    'fit_intercept': True,\n}\n\nbenchmark(SGDClassifier, parameters, 'SGD')\n\nprint("Testbenching a MultinomialNB classifier...")\nparameters = {'alpha': 0.01}\n\nbenchmark(MultinomialNB, parameters, 'MultinomialNB')\n\npl.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/text/mlcomp_sparse_document_classification.html
Discrete versus Real AdaBoost	A							http://scikit-learn.org/stable/auto_examples/index.html			This example is based on Figure 10.2 from Hastie et al 2009 [1] and illustrates the difference in performance between the discrete SAMME [2] boosting algorithm and real SAMME.R boosting algorithm. Both algorithms are evaluated on a binary classification task where the target Y is a non-linear function of 10 input features.<br><br><pre><code>print(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,\n#         Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import zero_one_loss\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nn_estimators = 400\n# A learning rate of 1. may not be optimal for both SAMME and SAMME.R\nlearning_rate = 1.\n\nX, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n\nX_test, y_test = X[2000:], y[2000:]\nX_train, y_train = X[:2000], y[:2000]\n\ndt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\ndt_stump.fit(X_train, y_train)\ndt_stump_err = 1.0 - dt_stump.score(X_test, y_test)\n\ndt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\ndt.fit(X_train, y_train)\ndt_err = 1.0 - dt.score(X_test, y_test)\n\nada_discrete = AdaBoostClassifier(\n    base_estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm="SAMME")\nada_discrete.fit(X_train, y_train)\n\nada_real = AdaBoostClassifier(\n    base_estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm="SAMME.R")\nada_real.fit(X_train, y_train)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nax.plot([1, n_estimators], [dt_stump_err] * 2, 'k-',\n        label='Decision Stump Error')\nax.plot([1, n_estimators], [dt_err] * 2, 'k--',\n        label='Decision Tree Error')\n\nada_discrete_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n    ada_discrete_err[i] = zero_one_loss(y_pred, y_test)\n\nada_discrete_err_train = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n    ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train)\n\nada_real_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n    ada_real_err[i] = zero_one_loss(y_pred, y_test)\n\nada_real_err_train = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_train)):\n    ada_real_err_train[i] = zero_one_loss(y_pred, y_train)\n\nax.plot(np.arange(n_estimators) + 1, ada_discrete_err,\n        label='Discrete AdaBoost Test Error',\n        color='red')\nax.plot(np.arange(n_estimators) + 1, ada_discrete_err_train,\n        label='Discrete AdaBoost Train Error',\n        color='blue')\nax.plot(np.arange(n_estimators) + 1, ada_real_err,\n        label='Real AdaBoost Test Error',\n        color='orange')\nax.plot(np.arange(n_estimators) + 1, ada_real_err_train,\n        label='Real AdaBoost Train Error',\n        color='green')\n\nax.set_ylim((0.0, 0.5))\nax.set_xlabel('n_estimators')\nax.set_ylabel('error rate')\n\nleg = ax.legend(loc='upper right', fancybox=True)\nleg.get_frame().set_alpha(0.7)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html
Multi-class AdaBoosted Decision Trees	A							http://scikit-learn.org/stable/auto_examples/index.html			This example reproduces Figure 1 of Zhu et al [1] and shows how boosting can improve prediction accuracy on a multi-class problem. The classification dataset is constructed by taking a ten-dimensional standard normal distribution and defining three classes separated by nested concentric ten-dimensional spheres such that roughly equal numbers of samples are in each class (quantiles of the...<br><br><pre><code>print(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nfrom sklearn.externals.six.moves import zip\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_gaussian_quantiles\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nX, y = make_gaussian_quantiles(n_samples=13000, n_features=10,\n                               n_classes=3, random_state=1)\n\nn_split = 3000\n\nX_train, X_test = X[:n_split], X[n_split:]\ny_train, y_test = y[:n_split], y[n_split:]\n\nbdt_real = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=600,\n    learning_rate=1)\n\nbdt_discrete = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=600,\n    learning_rate=1.5,\n    algorithm="SAMME")\n\nbdt_real.fit(X_train, y_train)\nbdt_discrete.fit(X_train, y_train)\n\nreal_test_errors = []\ndiscrete_test_errors = []\n\nfor real_test_predict, discrete_train_predict in zip(\n        bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):\n    real_test_errors.append(\n        1. - accuracy_score(real_test_predict, y_test))\n    discrete_test_errors.append(\n        1. - accuracy_score(discrete_train_predict, y_test))\n\nn_trees_discrete = len(bdt_discrete)\nn_trees_real = len(bdt_real)\n\n# Boosting might terminate early, but the following arrays are always\n# n_estimators long. We crop them to the actual number of trees here:\ndiscrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]\nreal_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]\ndiscrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nplt.plot(range(1, n_trees_discrete + 1),\n         discrete_test_errors, c='black', label='SAMME')\nplt.plot(range(1, n_trees_real + 1),\n         real_test_errors, c='black',\n         linestyle='dashed', label='SAMME.R')\nplt.legend()\nplt.ylim(0.18, 0.62)\nplt.ylabel('Test Error')\nplt.xlabel('Number of Trees')\n\nplt.subplot(132)\nplt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,\n         "b", label='SAMME', alpha=.5)\nplt.plot(range(1, n_trees_real + 1), real_estimator_errors,\n         "r", label='SAMME.R', alpha=.5)\nplt.legend()\nplt.ylabel('Error')\nplt.xlabel('Number of Trees')\nplt.ylim((.2,\n         max(real_estimator_errors.max(),\n             discrete_estimator_errors.max()) * 1.2))\nplt.xlim((-20, len(bdt_discrete) + 20))\n\nplt.subplot(133)\nplt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,\n         "b", label='SAMME')\nplt.legend()\nplt.ylabel('Weight')\nplt.xlabel('Number of Trees')\nplt.ylim((0, discrete_estimator_weights.max() * 1.2))\nplt.xlim((-20, n_trees_discrete + 20))\n\n# prevent overlapping y-axis labels\nplt.subplots_adjust(wspace=0.25)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_multiclass.html
Decision Tree Regression with AdaBoost	A							http://scikit-learn.org/stable/auto_examples/index.html			A decision tree is boosted using the AdaBoost.R2 [1] algorithm on a 1D sinusoidal dataset with a small amount of Gaussian noise. 299 boosts (300 decision trees) is compared with a single decision tree regressor. As the number of boosts is increased the regressor can fit more detail.<br><br><pre><code>print(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\n# importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Create the dataset\nrng = np.random.RandomState(1)\nX = np.linspace(0, 6, 100)[:, np.newaxis]\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=4)\n\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n                          n_estimators=300, random_state=rng)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c="k", label="training samples")\nplt.plot(X, y_1, c="g", label="n_estimators=1", linewidth=2)\nplt.plot(X, y_2, c="r", label="n_estimators=300", linewidth=2)\nplt.xlabel("data")\nplt.ylabel("target")\nplt.title("Boosted Decision Tree Regression")\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html
Two-class AdaBoost	A							http://scikit-learn.org/stable/auto_examples/index.html			This example fits an AdaBoosted decision stump on a non-linearly separable classification dataset composed of two “Gaussian quantiles” clusters (see sklearn.datasets.make_gaussian_quantiles) and plots the decision boundary and decision scores. The distributions of decision scores are shown separately for samples of class A and B. The predicted class label for each sample is determined by the sign...<br><br><pre><code>print(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_gaussian_quantiles\n\n\n# Construct dataset\nX1, y1 = make_gaussian_quantiles(cov=2.,\n                                 n_samples=200, n_features=2,\n                                 n_classes=2, random_state=1)\nX2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n                                 n_samples=300, n_features=2,\n                                 n_classes=2, random_state=1)\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, - y2 + 1))\n\n# Create and fit an AdaBoosted decision tree\nbdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n                         algorithm="SAMME",\n                         n_estimators=200)\n\nbdt.fit(X, y)\n\nplot_colors = "br"\nplot_step = 0.02\nclass_names = "AB"\n\nplt.figure(figsize=(10, 5))\n\n# Plot the decision boundaries\nplt.subplot(121)\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                     np.arange(y_min, y_max, plot_step))\n\nZ = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.axis("tight")\n\n# Plot the training points\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1],\n                c=c, cmap=plt.cm.Paired,\n                label="Class %s" % n)\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.legend(loc='upper right')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Decision Boundary')\n\n# Plot the two-class decision scores\ntwoclass_output = bdt.decision_function(X)\nplot_range = (twoclass_output.min(), twoclass_output.max())\nplt.subplot(122)\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    plt.hist(twoclass_output[y == i],\n             bins=10,\n             range=plot_range,\n             facecolor=c,\n             label='Class %s' % n,\n             alpha=.5)\nx1, x2, y1, y2 = plt.axis()\nplt.axis((x1, x2, y1, y2 * 1.2))\nplt.legend(loc='upper right')\nplt.ylabel('Samples')\nplt.xlabel('Score')\nplt.title('Decision Scores')\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.35)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html
Adjustment for chance in clustering performance evaluation	A							http://scikit-learn.org/stable/auto_examples/index.html			The following plots demonstrate the impact of the number of clusters and number of samples on various clustering performance evaluation metrics.<br><br><pre><code>print(__doc__)\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom sklearn import metrics\n\n\ndef uniform_labelings_scores(score_func, n_samples, n_clusters_range,\n                             fixed_n_classes=None, n_runs=5, seed=42):\n    """Compute score for 2 random uniform cluster labelings.\n\n    Both random labelings have the same number of clusters for each value\n    possible value in ``n_clusters_range``.\n\n    When fixed_n_classes is not None the first labeling is considered a ground\n    truth class assignment with fixed number of classes.\n    """\n    random_labels = np.random.RandomState(seed).random_integers\n    scores = np.zeros((len(n_clusters_range), n_runs))\n\n    if fixed_n_classes is not None:\n        labels_a = random_labels(low=0, high=fixed_n_classes - 1,\n                                 size=n_samples)\n\n    for i, k in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            if fixed_n_classes is None:\n                labels_a = random_labels(low=0, high=k - 1, size=n_samples)\n            labels_b = random_labels(low=0, high=k - 1, size=n_samples)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores\n\nscore_funcs = [\n    metrics.adjusted_rand_score,\n    metrics.v_measure_score,\n    metrics.adjusted_mutual_info_score,\n    metrics.mutual_info_score,\n]\n\n# 2 independent random clusterings with equal cluster number\n\nn_samples = 100\nn_clusters_range = np.linspace(2, n_samples, 10).astype(np.int)\n\nplt.figure(1)\n\nplots = []\nnames = []\nfor score_func in score_funcs:\n    print("Computing %s for %d values of n_clusters and n_samples=%d"\n          % (score_func.__name__, len(n_clusters_range), n_samples))\n\n    t0 = time()\n    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)\n    print("done in %0.3fs" % (time() - t0))\n    plots.append(plt.errorbar(\n        n_clusters_range, np.median(scores, axis=1), scores.std(axis=1))[0])\n    names.append(score_func.__name__)\n\nplt.title("Clustering measures for 2 random uniform labelings\n"\n          "with equal number of clusters")\nplt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)\nplt.ylabel('Score value')\nplt.legend(plots, names)\nplt.ylim(ymin=-0.05, ymax=1.05)\n\n\n# Random labeling with varying n_clusters against ground class labels\n# with fixed number of clusters\n\nn_samples = 1000\nn_clusters_range = np.linspace(2, 100, 10).astype(np.int)\nn_classes = 10\n\nplt.figure(2)\n\nplots = []\nnames = []\nfor score_func in score_funcs:\n    print("Computing %s for %d values of n_clusters and n_samples=%d"\n          % (score_func.__name__, len(n_clusters_range), n_samples))\n\n    t0 = time()\n    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range,\n                                      fixed_n_classes=n_classes)\n    print("done in %0.3fs" % (time() - t0))\n    plots.append(plt.errorbar(\n        n_clusters_range, scores.mean(axis=1), scores.std(axis=1))[0])\n    names.append(score_func.__name__)\n\nplt.title("Clustering measures for random uniform labeling\n"\n          "against reference assignment with %d classes" % n_classes)\nplt.xlabel('Number of clusters (Number of samples is fixed to %d)' % n_samples)\nplt.ylabel('Score value')\nplt.ylim(ymin=-0.05, ymax=1.05)\nplt.legend(plots, names)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html
Demo of affinity propagation clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			Reference: Brendan J. Frey and Delbert Dueck, “Clustering by Passing Messages Between Data Points”, Science Feb. 2007<br><br><pre><code>print(__doc__)\n\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn import metrics\nfrom sklearn.datasets.samples_generator import make_blobs\n\n##############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,\n                            random_state=0)\n\n##############################################################################\n# Compute Affinity Propagation\naf = AffinityPropagation(preference=-50).fit(X)\ncluster_centers_indices = af.cluster_centers_indices_\nlabels = af.labels_\n\nn_clusters_ = len(cluster_centers_indices)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))\nprint("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))\nprint("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))\nprint("Adjusted Rand Index: %0.3f"\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint("Adjusted Mutual Information: %0.3f"\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint("Silhouette Coefficient: %0.3f"\n      % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n\n##############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.close('all')\nplt.figure(1)\nplt.clf()\n\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X[cluster_centers_indices[k]]\n    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\n    for x in X[class_members]:\n        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html
Agglomerative clustering with and without structure	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows the effect of imposing a connectivity graph to capture local structure in the data. The graph is simply the graph of 20 nearest neighbors.<br><br><pre><code># Authors: Gael Varoquaux, Nelle Varoquaux\n# License: BSD 3 clause\n\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.neighbors import kneighbors_graph\n\n# Generate sample data\nn_samples = 1500\nnp.random.seed(0)\nt = 1.5 * np.pi * (1 + 3 * np.random.rand(1, n_samples))\nx = t * np.cos(t)\ny = t * np.sin(t)\n\n\nX = np.concatenate((x, y))\nX += .7 * np.random.randn(2, n_samples)\nX = X.T\n\n# Create a graph capturing local connectivity. Larger number of neighbors\n# will give more homogeneous clusters to the cost of computation\n# time. A very large number of neighbors gives more evenly distributed\n# cluster sizes, but may not impose the local manifold structure of\n# the data\nknn_graph = kneighbors_graph(X, 30, include_self=False)\n\nfor connectivity in (None, knn_graph):\n    for n_clusters in (30, 3):\n        plt.figure(figsize=(10, 4))\n        for index, linkage in enumerate(('average', 'complete', 'ward')):\n            plt.subplot(1, 3, index + 1)\n            model = AgglomerativeClustering(linkage=linkage,\n                                            connectivity=connectivity,\n                                            n_clusters=n_clusters)\n            t0 = time.time()\n            model.fit(X)\n            elapsed_time = time.time() - t0\n            plt.scatter(X[:, 0], X[:, 1], c=model.labels_,\n                        cmap=plt.cm.spectral)\n            plt.title('linkage=%s (time %.2fs)' % (linkage, elapsed_time),\n                      fontdict=dict(verticalalignment='top'))\n            plt.axis('equal')\n            plt.axis('off')\n\n            plt.subplots_adjust(bottom=0, top=.89, wspace=0,\n                                left=0, right=1)\n            plt.suptitle('n_cluster=%i, connectivity=%r' %\n                         (n_clusters, connectivity is not None), size=17)\n\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering.html
Agglomerative clustering with different metrics	A							http://scikit-learn.org/stable/auto_examples/index.html			Demonstrates the effect of different metrics on the hierarchical clustering.<br><br><pre><code># Author: Gael Varoquaux\n# License: BSD 3-Clause or CC-0\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import pairwise_distances\n\nnp.random.seed(0)\n\n# Generate waveform data\nn_features = 2000\nt = np.pi * np.linspace(0, 1, n_features)\n\n\ndef sqr(x):\n    return np.sign(np.cos(x))\n\nX = list()\ny = list()\nfor i, (phi, a) in enumerate([(.5, .15), (.5, .6), (.3, .2)]):\n    for _ in range(30):\n        phase_noise = .01 * np.random.normal()\n        amplitude_noise = .04 * np.random.normal()\n        additional_noise = 1 - 2 * np.random.rand(n_features)\n        # Make the noise sparse\n        additional_noise[np.abs(additional_noise) < .997] = 0\n\n        X.append(12 * ((a + amplitude_noise)\n                 * (sqr(6 * (t + phi + phase_noise)))\n                 + additional_noise))\n        y.append(i)\n\nX = np.array(X)\ny = np.array(y)\n\nn_clusters = 3\n\nlabels = ('Waveform 1', 'Waveform 2', 'Waveform 3')\n\n# Plot the ground-truth labelling\nplt.figure()\nplt.axes([0, 0, 1, 1])\nfor l, c, n in zip(range(n_clusters), 'rgb',\n                   labels):\n    lines = plt.plot(X[y == l].T, c=c, alpha=.5)\n    lines[0].set_label(n)\n\nplt.legend(loc='best')\n\nplt.axis('tight')\nplt.axis('off')\nplt.suptitle("Ground truth", size=20)\n\n\n# Plot the distances\nfor index, metric in enumerate(["cosine", "euclidean", "cityblock"]):\n    avg_dist = np.zeros((n_clusters, n_clusters))\n    plt.figure(figsize=(5, 4.5))\n    for i in range(n_clusters):\n        for j in range(n_clusters):\n            avg_dist[i, j] = pairwise_distances(X[y == i], X[y == j],\n                                                metric=metric).mean()\n    avg_dist /= avg_dist.max()\n    for i in range(n_clusters):\n        for j in range(n_clusters):\n            plt.text(i, j, '%5.3f' % avg_dist[i, j],\n                     verticalalignment='center',\n                     horizontalalignment='center')\n\n    plt.imshow(avg_dist, interpolation='nearest', cmap=plt.cm.gnuplot2,\n               vmin=0)\n    plt.xticks(range(n_clusters), labels, rotation=45)\n    plt.yticks(range(n_clusters), labels)\n    plt.colorbar()\n    plt.suptitle("Interclass %s distances" % metric, size=18)\n    plt.tight_layout()\n\n\n# Plot clustering results\nfor index, metric in enumerate(["cosine", "euclidean", "cityblock"]):\n    model = AgglomerativeClustering(n_clusters=n_clusters,\n                                    linkage="average", affinity=metric)\n    model.fit(X)\n    plt.figure()\n    plt.axes([0, 0, 1, 1])\n    for l, c in zip(np.arange(model.n_clusters), 'rgbk'):\n        plt.plot(X[model.labels_ == l].T, c=c, alpha=.5)\n    plt.axis('tight')\n    plt.axis('off')\n    plt.suptitle("AgglomerativeClustering(affinity=%s)" % metric, size=20)\n\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html
Hyper-parameters of Approximate Nearest Neighbors	A							http://scikit-learn.org/stable/auto_examples/index.html			This example demonstrates the behaviour of the accuracy of the nearest neighbor queries of Locality Sensitive Hashing Forest as the number of candidates and the number of estimators (trees) vary.<br><br><pre><code>from __future__ import division\nprint(__doc__)\n\n# Author: Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>\n#\n# License: BSD 3 clause\n\n\n###############################################################################\nimport numpy as np\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.neighbors import LSHForest\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\n\n\n# Initialize size of the database, iterations and required neighbors.\nn_samples = 10000\nn_features = 100\nn_queries = 30\nrng = np.random.RandomState(42)\n\n# Generate sample data\nX, _ = make_blobs(n_samples=n_samples + n_queries,\n                  n_features=n_features, centers=10,\n                  random_state=0)\nX_index = X[:n_samples]\nX_query = X[n_samples:]\n# Get exact neighbors\nnbrs = NearestNeighbors(n_neighbors=1, algorithm='brute',\n                        metric='cosine').fit(X_index)\nneighbors_exact = nbrs.kneighbors(X_query, return_distance=False)\n\n# Set `n_candidate` values\nn_candidates_values = np.linspace(10, 500, 5).astype(np.int)\nn_estimators_for_candidate_value = [1, 5, 10]\nn_iter = 10\nstds_accuracies = np.zeros((len(n_estimators_for_candidate_value),\n                            n_candidates_values.shape[0]),\n                           dtype=float)\naccuracies_c = np.zeros((len(n_estimators_for_candidate_value),\n                         n_candidates_values.shape[0]), dtype=float)\n\n# LSH Forest is a stochastic index: perform several iteration to estimate\n# expected accuracy and standard deviation displayed as error bars in\n# the plots\nfor j, value in enumerate(n_estimators_for_candidate_value):\n    for i, n_candidates in enumerate(n_candidates_values):\n        accuracy_c = []\n        for seed in range(n_iter):\n            lshf = LSHForest(n_estimators=value,\n                             n_candidates=n_candidates, n_neighbors=1,\n                             random_state=seed)\n            # Build the LSH Forest index\n            lshf.fit(X_index)\n            # Get neighbors\n            neighbors_approx = lshf.kneighbors(X_query,\n                                               return_distance=False)\n            accuracy_c.append(np.sum(np.equal(neighbors_approx,\n                                              neighbors_exact)) /\n                              n_queries)\n\n        stds_accuracies[j, i] = np.std(accuracy_c)\n        accuracies_c[j, i] = np.mean(accuracy_c)\n\n# Set `n_estimators` values\nn_estimators_values = [1, 5, 10, 20, 30, 40, 50]\naccuracies_trees = np.zeros(len(n_estimators_values), dtype=float)\n\n# Calculate average accuracy for each value of `n_estimators`\nfor i, n_estimators in enumerate(n_estimators_values):\n    lshf = LSHForest(n_estimators=n_estimators, n_neighbors=1)\n    # Build the LSH Forest index\n    lshf.fit(X_index)\n    # Get neighbors\n    neighbors_approx = lshf.kneighbors(X_query, return_distance=False)\n    accuracies_trees[i] = np.sum(np.equal(neighbors_approx,\n                                          neighbors_exact))/n_queries\n\n###############################################################################\n# Plot the accuracy variation with `n_candidates`\nplt.figure()\ncolors = ['c', 'm', 'y']\nfor i, n_estimators in enumerate(n_estimators_for_candidate_value):\n    label = 'n_estimators = %d ' % n_estimators\n    plt.plot(n_candidates_values, accuracies_c[i, :],\n             'o-', c=colors[i], label=label)\n    plt.errorbar(n_candidates_values, accuracies_c[i, :],\n                 stds_accuracies[i, :], c=colors[i])\n\nplt.legend(loc='upper left', fontsize='small')\nplt.ylim([0, 1.2])\nplt.xlim(min(n_candidates_values), max(n_candidates_values))\nplt.ylabel("Accuracy")\nplt.xlabel("n_candidates")\nplt.grid(which='both')\nplt.title("Accuracy variation with n_candidates")\n\n# Plot the accuracy variation with `n_estimators`\nplt.figure()\nplt.scatter(n_estimators_values, accuracies_trees, c='k')\nplt.plot(n_estimators_values, accuracies_trees, c='g')\nplt.ylim([0, 1.2])\nplt.xlim(min(n_estimators_values), max(n_estimators_values))\nplt.ylabel("Accuracy")\nplt.xlabel("n_estimators")\nplt.grid(which='both')\nplt.title("Accuracy variation with n_estimators")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_approximate_nearest_neighbors_hyperparameters.html
Scalability of Approximate Nearest Neighbors	A							http://scikit-learn.org/stable/auto_examples/index.html			This example studies the scalability profile of approximate 10-neighbors queries using the LSHForest with n_estimators=20 and n_candidates=200 when varying the number of samples in the dataset.<br><br><pre><code>from __future__ import division\nprint(__doc__)\n\n# Authors: Maheshakya Wijewardena <maheshakya.10@cse.mrt.ac.lk>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#\n# License: BSD 3 clause\n\n\n###############################################################################\nimport time\nimport numpy as np\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.neighbors import LSHForest\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\n\n# Parameters of the study\nn_samples_min = int(1e3)\nn_samples_max = int(1e5)\nn_features = 100\nn_centers = 100\nn_queries = 100\nn_steps = 6\nn_iter = 5\n\n# Initialize the range of `n_samples`\nn_samples_values = np.logspace(np.log10(n_samples_min),\n                               np.log10(n_samples_max),\n                               n_steps).astype(np.int)\n\n# Generate some structured data\nrng = np.random.RandomState(42)\nall_data, _ = make_blobs(n_samples=n_samples_max + n_queries,\n                         n_features=n_features, centers=n_centers, shuffle=True,\n                         random_state=0)\nqueries = all_data[:n_queries]\nindex_data = all_data[n_queries:]\n\n# Metrics to collect for the plots\naverage_times_exact = []\naverage_times_approx = []\nstd_times_approx = []\naccuracies = []\nstd_accuracies = []\naverage_speedups = []\nstd_speedups = []\n\n# Calculate the average query time\nfor n_samples in n_samples_values:\n    X = index_data[:n_samples]\n    # Initialize LSHForest for queries of a single neighbor\n    lshf = LSHForest(n_estimators=20, n_candidates=200,\n                     n_neighbors=10).fit(X)\n    nbrs = NearestNeighbors(algorithm='brute', metric='cosine',\n                            n_neighbors=10).fit(X)\n    time_approx = []\n    time_exact = []\n    accuracy = []\n\n    for i in range(n_iter):\n        # pick one query at random to study query time variability in LSHForest\n        query = queries[[rng.randint(0, n_queries)]]\n\n        t0 = time.time()\n        exact_neighbors = nbrs.kneighbors(query, return_distance=False)\n        time_exact.append(time.time() - t0)\n\n        t0 = time.time()\n        approx_neighbors = lshf.kneighbors(query, return_distance=False)\n        time_approx.append(time.time() - t0)\n\n        accuracy.append(np.in1d(approx_neighbors, exact_neighbors).mean())\n\n    average_time_exact = np.mean(time_exact)\n    average_time_approx = np.mean(time_approx)\n    speedup = np.array(time_exact) / np.array(time_approx)\n    average_speedup = np.mean(speedup)\n    mean_accuracy = np.mean(accuracy)\n    std_accuracy = np.std(accuracy)\n    print("Index size: %d, exact: %0.3fs, LSHF: %0.3fs, speedup: %0.1f, "\n          "accuracy: %0.2f +/-%0.2f" %\n          (n_samples, average_time_exact, average_time_approx, average_speedup,\n           mean_accuracy, std_accuracy))\n\n    accuracies.append(mean_accuracy)\n    std_accuracies.append(std_accuracy)\n    average_times_exact.append(average_time_exact)\n    average_times_approx.append(average_time_approx)\n    std_times_approx.append(np.std(time_approx))\n    average_speedups.append(average_speedup)\n    std_speedups.append(np.std(speedup))\n\n# Plot average query time against n_samples\nplt.figure()\nplt.errorbar(n_samples_values, average_times_approx, yerr=std_times_approx,\n             fmt='o-', c='r', label='LSHForest')\nplt.plot(n_samples_values, average_times_exact, c='b',\n         label="NearestNeighbors(algorithm='brute', metric='cosine')")\nplt.legend(loc='upper left', fontsize='small')\nplt.ylim(0, None)\nplt.ylabel("Average query time in seconds")\nplt.xlabel("n_samples")\nplt.grid(which='both')\nplt.title("Impact of index size on response time for first "\n          "nearest neighbors queries")\n\n# Plot average query speedup versus index size\nplt.figure()\nplt.errorbar(n_samples_values, average_speedups, yerr=std_speedups,\n             fmt='o-', c='r')\nplt.ylim(0, None)\nplt.ylabel("Average speedup")\nplt.xlabel("n_samples")\nplt.grid(which='both')\nplt.title("Speedup of the approximate NN queries vs brute force")\n\n# Plot average precision versus index size\nplt.figure()\nplt.errorbar(n_samples_values, accuracies, std_accuracies, fmt='o-', c='c')\nplt.ylim(0, 1.1)\nplt.ylabel("precision@10")\nplt.xlabel("n_samples")\nplt.grid(which='both')\nplt.title("precision of 10-nearest-neighbors queries with index size")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_approximate_nearest_neighbors_scalability.html
Automatic Relevance Determination Regression (ARD)	A							http://scikit-learn.org/stable/auto_examples/index.html			Fit regression model with Bayesian Ridge Regression.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import ARDRegression, LinearRegression\n\n###############################################################################\n# Generating simulated data with Gaussian weights\n\n# Parameters of the example\nnp.random.seed(0)\nn_samples, n_features = 100, 100\n# Create Gaussian data\nX = np.random.randn(n_samples, n_features)\n# Create weigts with a precision lambda_ of 4.\nlambda_ = 4.\nw = np.zeros(n_features)\n# Only keep 10 weights of interest\nrelevant_features = np.random.randint(0, n_features, 10)\nfor i in relevant_features:\n    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n# Create noite with a precision alpha of 50.\nalpha_ = 50.\nnoise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n# Create the target\ny = np.dot(X, w) + noise\n\n###############################################################################\n# Fit the ARD Regression\nclf = ARDRegression(compute_score=True)\nclf.fit(X, y)\n\nols = LinearRegression()\nols.fit(X, y)\n\n###############################################################################\n# Plot the true weights, the estimated weights and the histogram of the\n# weights\nplt.figure(figsize=(6, 5))\nplt.title("Weights of the model")\nplt.plot(clf.coef_, 'b-', label="ARD estimate")\nplt.plot(ols.coef_, 'r--', label="OLS estimate")\nplt.plot(w, 'g-', label="Ground truth")\nplt.xlabel("Features")\nplt.ylabel("Values of the weights")\nplt.legend(loc=1)\n\nplt.figure(figsize=(6, 5))\nplt.title("Histogram of the weights")\nplt.hist(clf.coef_, bins=n_features, log=True)\nplt.plot(clf.coef_[relevant_features], 5 * np.ones(len(relevant_features)),\n         'ro', label="Relevant features")\nplt.ylabel("Features")\nplt.xlabel("Values of the weights")\nplt.legend(loc=1)\n\nplt.figure(figsize=(6, 5))\nplt.title("Marginal log-likelihood")\nplt.plot(clf.scores_)\nplt.ylabel("Score")\nplt.xlabel("Iterations")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ard.html
Bayesian Ridge Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			Computes a Bayesian Ridge Regression on a synthetic dataset.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import BayesianRidge, LinearRegression\n\n###############################################################################\n# Generating simulated data with Gaussian weigthts\nnp.random.seed(0)\nn_samples, n_features = 100, 100\nX = np.random.randn(n_samples, n_features)  # Create Gaussian data\n# Create weigts with a precision lambda_ of 4.\nlambda_ = 4.\nw = np.zeros(n_features)\n# Only keep 10 weights of interest\nrelevant_features = np.random.randint(0, n_features, 10)\nfor i in relevant_features:\n    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n# Create noise with a precision alpha of 50.\nalpha_ = 50.\nnoise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n# Create the target\ny = np.dot(X, w) + noise\n\n###############################################################################\n# Fit the Bayesian Ridge Regression and an OLS for comparison\nclf = BayesianRidge(compute_score=True)\nclf.fit(X, y)\n\nols = LinearRegression()\nols.fit(X, y)\n\n###############################################################################\n# Plot true weights, estimated weights and histogram of the weights\nplt.figure(figsize=(6, 5))\nplt.title("Weights of the model")\nplt.plot(clf.coef_, 'b-', label="Bayesian Ridge estimate")\nplt.plot(w, 'g-', label="Ground truth")\nplt.plot(ols.coef_, 'r--', label="OLS estimate")\nplt.xlabel("Features")\nplt.ylabel("Values of the weights")\nplt.legend(loc="best", prop=dict(size=12))\n\nplt.figure(figsize=(6, 5))\nplt.title("Histogram of the weights")\nplt.hist(clf.coef_, bins=n_features, log=True)\nplt.plot(clf.coef_[relevant_features], 5 * np.ones(len(relevant_features)),\n         'ro', label="Relevant features")\nplt.ylabel("Features")\nplt.xlabel("Values of the weights")\nplt.legend(loc="lower left")\n\nplt.figure(figsize=(6, 5))\nplt.title("Marginal log-likelihood")\nplt.plot(clf.scores_)\nplt.ylabel("Score")\nplt.xlabel("Iterations")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge.html
Single estimator versus bagging: bias-variance decomposition	A							http://scikit-learn.org/stable/auto_examples/index.html			This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble.<br><br><pre><code>print(__doc__)\n\n# Author: Gilles Louppe <g.louppe@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Settings\nn_repeat = 50       # Number of iterations for computing expectations\nn_train = 50        # Size of the training set\nn_test = 1000       # Size of the test set\nnoise = 0.1         # Standard deviation of the noise\nnp.random.seed(0)\n\n# Change this for exploring the bias-variance decomposition of other\n# estimators. This should work well for estimators with high variance (e.g.,\n# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n# linear models).\nestimators = [("Tree", DecisionTreeRegressor()),\n              ("Bagging(Tree)", BaggingRegressor(DecisionTreeRegressor()))]\n\nn_estimators = len(estimators)\n\n# Generate data\ndef f(x):\n    x = x.ravel()\n\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n\ndef generate(n_samples, noise, n_repeat=1):\n    X = np.random.rand(n_samples) * 10 - 5\n    X = np.sort(X)\n\n    if n_repeat == 1:\n        y = f(X) + np.random.normal(0.0, noise, n_samples)\n    else:\n        y = np.zeros((n_samples, n_repeat))\n\n        for i in range(n_repeat):\n            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n\n    X = X.reshape((n_samples, 1))\n\n    return X, y\n\nX_train = []\ny_train = []\n\nfor i in range(n_repeat):\n    X, y = generate(n_samples=n_train, noise=noise)\n    X_train.append(X)\n    y_train.append(y)\n\nX_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n\n# Loop over estimators to compare\nfor n, (name, estimator) in enumerate(estimators):\n    # Compute predictions\n    y_predict = np.zeros((n_test, n_repeat))\n\n    for i in range(n_repeat):\n        estimator.fit(X_train[i], y_train[i])\n        y_predict[:, i] = estimator.predict(X_test)\n\n    # Bias^2 + Variance + Noise decomposition of the mean squared error\n    y_error = np.zeros(n_test)\n\n    for i in range(n_repeat):\n        for j in range(n_repeat):\n            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n\n    y_error /= (n_repeat * n_repeat)\n\n    y_noise = np.var(y_test, axis=1)\n    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n    y_var = np.var(y_predict, axis=1)\n\n    print("{0}: {1:.4f} (error) = {2:.4f} (bias^2) "\n          " + {3:.4f} (var) + {4:.4f} (noise)".format(name,\n                                                      np.mean(y_error),\n                                                      np.mean(y_bias),\n                                                      np.mean(y_var),\n                                                      np.mean(y_noise)))\n\n    # Plot figures\n    plt.subplot(2, n_estimators, n + 1)\n    plt.plot(X_test, f(X_test), "b", label="$f(x)$")\n    plt.plot(X_train[0], y_train[0], ".b", label="LS ~ $y = f(x)+noise$")\n\n    for i in range(n_repeat):\n        if i == 0:\n            plt.plot(X_test, y_predict[:, i], "r", label="$\^y(x)$")\n        else:\n            plt.plot(X_test, y_predict[:, i], "r", alpha=0.05)\n\n    plt.plot(X_test, np.mean(y_predict, axis=1), "c",\n             label="$\mathbb{E}_{LS} \^y(x)$")\n\n    plt.xlim([-5, 5])\n    plt.title(name)\n\n    if n == 0:\n        plt.legend(loc="upper left", prop={"size": 11})\n\n    plt.subplot(2, n_estimators, n_estimators + n + 1)\n    plt.plot(X_test, y_error, "r", label="$error(x)$")\n    plt.plot(X_test, y_bias, "b", label="$bias^2(x)$"),\n    plt.plot(X_test, y_var, "g", label="$variance(x)$"),\n    plt.plot(X_test, y_noise, "c", label="$noise(x)$")\n\n    plt.xlim([-5, 5])\n    plt.ylim([0, 0.1])\n\n    if n == 0:\n        plt.legend(loc="upper left", prop={"size": 11})\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html
Compare BIRCH and MiniBatchKMeans	A							http://scikit-learn.org/stable/auto_examples/index.html			This example compares the timing of Birch (with and without the global clustering step) and MiniBatchKMeans on a synthetic dataset having 100,000 samples and 2 features generated using make_blobs.<br><br><pre><code># Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com\n#          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nfrom itertools import cycle\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import Birch, MiniBatchKMeans\nfrom sklearn.datasets.samples_generator import make_blobs\n\n\n# Generate centers for the blobs so that it forms a 10 X 10 grid.\nxx = np.linspace(-22, 22, 10)\nyy = np.linspace(-22, 22, 10)\nxx, yy = np.meshgrid(xx, yy)\nn_centres = np.hstack((np.ravel(xx)[:, np.newaxis],\n                       np.ravel(yy)[:, np.newaxis]))\n\n# Generate blobs to do a comparison between MiniBatchKMeans and Birch.\nX, y = make_blobs(n_samples=100000, centers=n_centres, random_state=0)\n   \n\n# Use all colors that matplotlib provides by default.\ncolors_ = cycle(colors.cnames.keys())\n\nfig = plt.figure(figsize=(12, 4))\nfig.subplots_adjust(left=0.04, right=0.98, bottom=0.1, top=0.9)\n\n# Compute clustering with Birch with and without the final clustering step\n# and plot.\nbirch_models = [Birch(threshold=1.7, n_clusters=None),\n                Birch(threshold=1.7, n_clusters=100)]\nfinal_step = ['without global clustering', 'with global clustering']\n\nfor ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):\n    t = time()\n    birch_model.fit(X)\n    time_ = time() - t\n    print("Birch %s as the final step took %0.2f seconds" % (\n          info, (time() - t)))\n\n    # Plot result\n    labels = birch_model.labels_\n    centroids = birch_model.subcluster_centers_\n    n_clusters = np.unique(labels).size\n    print("n_clusters : %d" % n_clusters)\n\n    ax = fig.add_subplot(1, 3, ind + 1)\n    for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):\n        mask = labels == k\n        ax.plot(X[mask, 0], X[mask, 1], 'w',\n                markerfacecolor=col, marker='.')\n        if birch_model.n_clusters is None:\n            ax.plot(this_centroid[0], this_centroid[1], '+', markerfacecolor=col,\n                    markeredgecolor='k', markersize=5)\n    ax.set_ylim([-25, 25])\n    ax.set_xlim([-25, 25])\n    ax.set_autoscaley_on(False)\n    ax.set_title('Birch %s' % info)\n\n# Compute clustering with MiniBatchKMeans.\nmbk = MiniBatchKMeans(init='k-means++', n_clusters=100, batch_size=100,\n                      n_init=10, max_no_improvement=10, verbose=0,\n                      random_state=0)\nt0 = time()\nmbk.fit(X)\nt_mini_batch = time() - t0\nprint("Time taken to run MiniBatchKMeans %0.2f seconds" % t_mini_batch)\nmbk_means_labels_unique = np.unique(mbk.labels_)\n\nax = fig.add_subplot(1, 3, 3)\nfor this_centroid, k, col in zip(mbk.cluster_centers_,\n                                 range(n_clusters), colors_):\n    mask = mbk.labels_ == k\n    ax.plot(X[mask, 0], X[mask, 1], 'w', markerfacecolor=col, marker='.')\n    ax.plot(this_centroid[0], this_centroid[1], '+', markeredgecolor='k',\n            markersize=5)\nax.set_xlim([-25, 25])\nax.set_ylim([-25, 25])\nax.set_title("MiniBatchKMeans")\nax.set_autoscaley_on(False)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_birch_vs_minibatchkmeans.html
Probability calibration of classifiers	A							http://scikit-learn.org/stable/auto_examples/index.html			When performing classification you often want to predict not only the class label, but also the associated probability. This probability gives you some kind of confidence on the prediction. However, not all classifiers provide well-calibrated probabilities, some being over-confident while others being under-confident. Thus, a separate calibration of predicted probabilities is often desirable as a...<br><br><pre><code>print(__doc__)\n\n# Author: Mathieu Blondel <mathieu@mblondel.org>\n#         Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Balazs Kegl <balazs.kegl@gmail.com>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.cross_validation import train_test_split\n\n\nn_samples = 50000\nn_bins = 3  # use 3 bins for calibration_curve as we have 3 clusters here\n\n# Generate 3 blobs with 2 classes where the second blob contains\n# half positive samples and half negative samples. Probability in this\n# blob is therefore 0.5.\ncenters = [(-5, -5), (0, 0), (5, 5)]\nX, y = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0,\n                  centers=centers, shuffle=False, random_state=42)\n\ny[:n_samples // 2] = 0\ny[n_samples // 2:] = 1\nsample_weight = np.random.RandomState(42).rand(y.shape[0])\n\n# split train, test for calibration\nX_train, X_test, y_train, y_test, sw_train, sw_test = \\n    train_test_split(X, y, sample_weight, test_size=0.9, random_state=42)\n\n# Gaussian Naive-Bayes with no calibration\nclf = GaussianNB()\nclf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\nprob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n# Gaussian Naive-Bayes with isotonic calibration\nclf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')\nclf_isotonic.fit(X_train, y_train, sw_train)\nprob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n\n# Gaussian Naive-Bayes with sigmoid calibration\nclf_sigmoid = CalibratedClassifierCV(clf, cv=2, method='sigmoid')\nclf_sigmoid.fit(X_train, y_train, sw_train)\nprob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\n\nprint("Brier scores: (the smaller the better)")\n\nclf_score = brier_score_loss(y_test, prob_pos_clf, sw_test)\nprint("No calibration: %1.3f" % clf_score)\n\nclf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sw_test)\nprint("With isotonic calibration: %1.3f" % clf_isotonic_score)\n\nclf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sw_test)\nprint("With sigmoid calibration: %1.3f" % clf_sigmoid_score)\n\n###############################################################################\n# Plot the data and the predicted probabilities\nplt.figure()\ny_unique = np.unique(y)\ncolors = cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\nfor this_y, color in zip(y_unique, colors):\n    this_X = X_train[y_train == this_y]\n    this_sw = sw_train[y_train == this_y]\n    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50, c=color, alpha=0.5,\n                label="Class %s" % this_y)\nplt.legend(loc="best")\nplt.title("Data")\n\nplt.figure()\norder = np.lexsort((prob_pos_clf, ))\nplt.plot(prob_pos_clf[order], 'r', label='No calibration (%1.3f)' % clf_score)\nplt.plot(prob_pos_isotonic[order], 'g', linewidth=3,\n         label='Isotonic calibration (%1.3f)' % clf_isotonic_score)\nplt.plot(prob_pos_sigmoid[order], 'b', linewidth=3,\n         label='Sigmoid calibration (%1.3f)' % clf_sigmoid_score)\nplt.plot(np.linspace(0, y_test.size, 51)[1::2],\n         y_test[order].reshape(25, -1).mean(1),\n         'k', linewidth=3, label=r'Empirical')\nplt.ylim([-0.05, 1.05])\nplt.xlabel("Instances sorted according to predicted probability "\n           "(uncalibrated GNB)")\nplt.ylabel("P(y=1)")\nplt.legend(loc="upper left")\nplt.title("Gaussian naive Bayes probabilities")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html
Probability Calibration curves	A							http://scikit-learn.org/stable/auto_examples/index.html			When performing classification one often wants to predict not only the class label, but also the associated probability. This probability gives some kind of confidence on the prediction. This example demonstrates how to display how well calibrated the predicted probabilities are and how to calibrate an uncalibrated classifier.<br><br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n                             f1_score)\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.cross_validation import train_test_split\n\n\n# Create dataset of classification task with many redundant and few\n# informative features\nX, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=2, n_redundant=10,\n                                    random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,\n                                                    random_state=42)\n\n\ndef plot_calibration_curve(est, name, fig_index):\n    """Plot calibration curve for est w/o and with calibration. """\n    # Calibrated with isotonic calibration\n    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n\n    # Calibrated with sigmoid calibration\n    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n\n    # Logistic regression with no calibration as baseline\n    lr = LogisticRegression(C=1., solver='lbfgs')\n\n    fig = plt.figure(fig_index, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n\n    ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")\n    for clf, name in [(lr, 'Logistic'),\n                      (est, name),\n                      (isotonic, name + ' + Isotonic'),\n                      (sigmoid, name + ' + Sigmoid')]:\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        if hasattr(clf, "predict_proba"):\n            prob_pos = clf.predict_proba(X_test)[:, 1]\n        else:  # use decision function\n            prob_pos = clf.decision_function(X_test)\n            prob_pos = \\n                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n\n        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n        print("%s:" % name)\n        print("\tBrier: %1.3f" % (clf_score))\n        print("\tPrecision: %1.3f" % precision_score(y_test, y_pred))\n        print("\tRecall: %1.3f" % recall_score(y_test, y_pred))\n        print("\tF1: %1.3f\n" % f1_score(y_test, y_pred))\n\n        fraction_of_positives, mean_predicted_value = \\n            calibration_curve(y_test, prob_pos, n_bins=10)\n\n        ax1.plot(mean_predicted_value, fraction_of_positives, "s-",\n                 label="%s (%1.3f)" % (name, clf_score))\n\n        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n                 histtype="step", lw=2)\n\n    ax1.set_ylabel("Fraction of positives")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc="lower right")\n    ax1.set_title('Calibration plots  (reliability curve)')\n\n    ax2.set_xlabel("Mean predicted value")\n    ax2.set_ylabel("Count")\n    ax2.legend(loc="upper center", ncol=2)\n\n    plt.tight_layout()\n\n# Plot calibration cuve for Gaussian Naive Bayes\nplot_calibration_curve(GaussianNB(), "Naive Bayes", 1)\n\n# Plot calibration cuve for Linear SVC\nplot_calibration_curve(LinearSVC(), "SVC", 2)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_curve.html
Probability Calibration for 3-class classification	A							http://scikit-learn.org/stable/auto_examples/index.html			This example illustrates how sigmoid calibration changes predicted probabilities for a 3-class classification problem. Illustrated is the standard 2-simplex, where the three corners correspond to the three classes. Arrows point from the probability vectors predicted by an uncalibrated classifier to the probability vectors predicted by the same classifier after sigmoid calibration on a hold-out...<br><br><pre><code>print(__doc__)\n\n# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import log_loss\n\nnp.random.seed(0)\n\n# Generate data\nX, y = make_blobs(n_samples=1000, n_features=2, random_state=42,\n                  cluster_std=5.0)\nX_train, y_train = X[:600], y[:600]\nX_valid, y_valid = X[600:800], y[600:800]\nX_train_valid, y_train_valid = X[:800], y[:800]\nX_test, y_test = X[800:], y[800:]\n\n# Train uncalibrated random forest classifier on whole train and validation\n# data and evaluate on test data\nclf = RandomForestClassifier(n_estimators=25)\nclf.fit(X_train_valid, y_train_valid)\nclf_probs = clf.predict_proba(X_test)\nscore = log_loss(y_test, clf_probs)\n\n# Train random forest classifier, calibrate on validation data and evaluate\n# on test data\nclf = RandomForestClassifier(n_estimators=25)\nclf.fit(X_train, y_train)\nclf_probs = clf.predict_proba(X_test)\nsig_clf = CalibratedClassifierCV(clf, method="sigmoid", cv="prefit")\nsig_clf.fit(X_valid, y_valid)\nsig_clf_probs = sig_clf.predict_proba(X_test)\nsig_score = log_loss(y_test, sig_clf_probs)\n\n# Plot changes in predicted probabilities via arrows\nplt.figure(0)\ncolors = ["r", "g", "b"]\nfor i in range(clf_probs.shape[0]):\n    plt.arrow(clf_probs[i, 0], clf_probs[i, 1],\n              sig_clf_probs[i, 0] - clf_probs[i, 0],\n              sig_clf_probs[i, 1] - clf_probs[i, 1],\n              color=colors[y_test[i]], head_width=1e-2)\n\n# Plot perfect predictions\nplt.plot([1.0], [0.0], 'ro', ms=20, label="Class 1")\nplt.plot([0.0], [1.0], 'go', ms=20, label="Class 2")\nplt.plot([0.0], [0.0], 'bo', ms=20, label="Class 3")\n\n# Plot boundaries of unit simplex\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")\n\n# Annotate points on the simplex\nplt.annotate(r'($\frac{1}{3}$, $\frac{1}{3}$, $\frac{1}{3}$)',\n             xy=(1.0/3, 1.0/3), xytext=(1.0/3, .23), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.plot([1.0/3], [1.0/3], 'ko', ms=5)\nplt.annotate(r'($\frac{1}{2}$, $0$, $\frac{1}{2}$)',\n             xy=(.5, .0), xytext=(.5, .1), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($0$, $\frac{1}{2}$, $\frac{1}{2}$)',\n             xy=(.0, .5), xytext=(.1, .5), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($\frac{1}{2}$, $\frac{1}{2}$, $0$)',\n             xy=(.5, .5), xytext=(.6, .6), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($0$, $0$, $1$)',\n             xy=(0, 0), xytext=(.1, .1), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($1$, $0$, $0$)',\n             xy=(1, 0), xytext=(1, .1), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\nplt.annotate(r'($0$, $1$, $0$)',\n             xy=(0, 1), xytext=(.1, 1), xycoords='data',\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             horizontalalignment='center', verticalalignment='center')\n# Add grid\nplt.grid("off")\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    plt.plot([0, x], [x, 0], 'k', alpha=0.2)\n    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)\n    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)\n\nplt.title("Change of predicted probabilities after sigmoid calibration")\nplt.xlabel("Probability class 1")\nplt.ylabel("Probability class 2")\nplt.xlim(-0.05, 1.05)\nplt.ylim(-0.05, 1.05)\nplt.legend(loc="best")\n\nprint("Log-loss of")\nprint(" * uncalibrated classifier trained on 800 datapoints: %.3f "\n      % score)\nprint(" * classifier trained on 600 datapoints and calibrated on "\n      "200 datapoint: %.3f" % sig_score)\n\n# Illustrate calibrator\nplt.figure(1)\n# generate grid over 2-simplex\np1d = np.linspace(0, 1, 20)\np0, p1 = np.meshgrid(p1d, p1d)\np2 = 1 - p0 - p1\np = np.c_[p0.ravel(), p1.ravel(), p2.ravel()]\np = p[p[:, 2] >= 0]\n\ncalibrated_classifier = sig_clf.calibrated_classifiers_[0]\nprediction = np.vstack([calibrator.predict(this_p)\n                        for calibrator, this_p in\n                        zip(calibrated_classifier.calibrators_, p.T)]).T\nprediction /= prediction.sum(axis=1)[:, None]\n\n# Ploit modifications of calibrator\nfor i in range(prediction.shape[0]):\n    plt.arrow(p[i, 0], p[i, 1],\n              prediction[i, 0] - p[i, 0], prediction[i, 1] - p[i, 1],\n              head_width=1e-2, color=colors[np.argmax(p[i])])\n# Plot boundaries of unit simplex\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], 'k', label="Simplex")\n\nplt.grid("off")\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    plt.plot([0, x], [x, 0], 'k', alpha=0.2)\n    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], 'k', alpha=0.2)\n    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], 'k', alpha=0.2)\n\nplt.title("Illustration of sigmoid calibrator")\nplt.xlabel("Probability class 1")\nplt.ylabel("Probability class 2")\nplt.xlim(-0.05, 1.05)\nplt.ylim(-0.05, 1.05)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/calibration/plot_calibration_multiclass.html
Nearest Neighbors Classification	A							http://scikit-learn.org/stable/auto_examples/index.html			Sample usage of Nearest Neighbors classification. It will plot the decision boundaries for each class.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title("3-Class classification (k = %i, weights = '%s')"\n              % (n_neighbors, weights))\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html
Plot classification probability	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier, L1 and L2 penalized logistic regression with either a One-Vs-Rest or multinomial setting.<br><br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:, 0:2]  # we only take the first two features for visualization\ny = iris.target\n\nn_features = X.shape[1]\n\nC = 1.0\n\n# Create different classifiers. The logistic regression cannot do\n# multiclass out of the box.\nclassifiers = {'L1 logistic': LogisticRegression(C=C, penalty='l1'),\n               'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2'),\n               'Linear SVC': SVC(kernel='linear', C=C, probability=True,\n                                 random_state=0),\n               'L2 logistic (Multinomial)': LogisticRegression(\n                C=C, solver='lbfgs', multi_class='multinomial'\n                )}\n\nn_classifiers = len(classifiers)\n\nplt.figure(figsize=(3 * 2, n_classifiers * 2))\nplt.subplots_adjust(bottom=.2, top=.95)\n\nxx = np.linspace(3, 9, 100)\nyy = np.linspace(1, 5, 100).T\nxx, yy = np.meshgrid(xx, yy)\nXfull = np.c_[xx.ravel(), yy.ravel()]\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classifier.fit(X, y)\n\n    y_pred = classifier.predict(X)\n    classif_rate = np.mean(y_pred.ravel() == y.ravel()) * 100\n    print("classif_rate for %s : %f " % (name, classif_rate))\n\n    # View probabilities=\n    probas = classifier.predict_proba(Xfull)\n    n_classes = np.unique(y_pred).size\n    for k in range(n_classes):\n        plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)\n        plt.title("Class %d" % k)\n        if k == 0:\n            plt.ylabel(name)\n        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),\n                                   extent=(3, 9, 1, 5), origin='lower')\n        plt.xticks(())\n        plt.yticks(())\n        idx = (y_pred == k)\n        if idx.any():\n            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='k')\n\nax = plt.axes([0.15, 0.04, 0.7, 0.05])\nplt.title("Probability")\nplt.colorbar(imshow_handle, cax=ax, orientation='horizontal')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_classification_probability.html
Classifier comparison	A							http://scikit-learn.org/stable/auto_examples/index.html			A comparison of a several classifiers in scikit-learn on synthetic datasets. The point of this example is to illustrate the nature of decision boundaries of different classifiers. This should be taken with a grain of salt, as the intuition conveyed by these examples does not necessarily carry over to real datasets.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n#              Andreas Müller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\n\nnames = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Decision Tree",\n         "Random Forest", "AdaBoost", "Naive Bayes", "Linear Discriminant Analysis",\n         "Quadratic Discriminant Analysis"]\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel="linear", C=0.025),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds in datasets:\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n    # and testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, m_max]x[y_min, y_max].\n        if hasattr(clf, "decision_function"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot also the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n        # and testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nfigure.subplots_adjust(left=.02, right=.98)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html
Comparing different clustering algorithms on toy datasets	A							http://scikit-learn.org/stable/auto_examples/index.html			This example aims at showing characteristics of different clustering algorithms on datasets that are “interesting” but still in 2D. The last dataset is an example of a ‘null’ situation for clustering: the data is homogeneous, and there is no good clustering.<br><br><pre><code>print(__doc__)\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, datasets\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\nnp.random.seed(0)\n\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\nn_samples = 1500\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nno_structure = np.random.rand(n_samples, 2), None\n\ncolors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\ncolors = np.hstack([colors] * 20)\n\nclustering_names = [\n    'MiniBatchKMeans', 'AffinityPropagation', 'MeanShift',\n    'SpectralClustering', 'Ward', 'AgglomerativeClustering',\n    'DBSCAN', 'Birch']\n\nplt.figure(figsize=(len(clustering_names) * 2 + 3, 9.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n                    hspace=.01)\n\nplot_num = 1\n\ndatasets = [noisy_circles, noisy_moons, blobs, no_structure]\nfor i_dataset, dataset in enumerate(datasets):\n    X, y = dataset\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=0.3)\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # create clustering estimators\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(n_clusters=2)\n    ward = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward',\n                                           connectivity=connectivity)\n    spectral = cluster.SpectralClustering(n_clusters=2,\n                                          eigen_solver='arpack',\n                                          affinity="nearest_neighbors")\n    dbscan = cluster.DBSCAN(eps=.2)\n    affinity_propagation = cluster.AffinityPropagation(damping=.9,\n                                                       preference=-200)\n\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage="average", affinity="cityblock", n_clusters=2,\n        connectivity=connectivity)\n\n    birch = cluster.Birch(n_clusters=2)\n    clustering_algorithms = [\n        two_means, affinity_propagation, ms, spectral, ward, average_linkage,\n        dbscan, birch]\n\n    for name, algorithm in zip(clustering_names, clustering_algorithms):\n        # predict cluster memberships\n        t0 = time.time()\n        algorithm.fit(X)\n        t1 = time.time()\n        if hasattr(algorithm, 'labels_'):\n            y_pred = algorithm.labels_.astype(np.int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        # plot\n        plt.subplot(4, len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n        plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n\n        if hasattr(algorithm, 'cluster_centers_'):\n            centers = algorithm.cluster_centers_\n            center_colors = colors[:len(centers)]\n            plt.scatter(centers[:, 0], centers[:, 1], s=100, c=center_colors)\n        plt.xlim(-2, 2)\n        plt.ylim(-2, 2)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n                 transform=plt.gca().transAxes, size=15,\n                 horizontalalignment='right')\n        plot_num += 1\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html
K-means Clustering	A							http://scikit-learn.org/stable/auto_examples/index.html			The plots display firstly what a K-means algorithm would yield using three clusters. It is then shown what the effect of a bad initialization is on the classification process: By setting n_init to only 1 (default is 10), the amount of times that the algorithm will be run with different centroid seeds is reduced. The next plot displays what using eight clusters would deliver and finally the ground...<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\n\nnp.random.seed(5)\n\ncenters = [[1, 1], [-1, -1], [1, -1]]\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nestimators = {'k_means_iris_3': KMeans(n_clusters=3),\n              'k_means_iris_8': KMeans(n_clusters=8),\n              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n                                              init='random')}\n\n\nfignum = 1\nfor name, est in estimators.items():\n    fig = plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\n    plt.cla()\n    est.fit(X)\n    labels = est.labels_\n\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n    ax.set_xlabel('Petal width')\n    ax.set_ylabel('Sepal length')\n    ax.set_zlabel('Petal length')\n    fignum = fignum + 1\n\n# Plot the ground truth\nfig = plt.figure(fignum, figsize=(4, 3))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\n\nfor name, label in [('Setosa', 0),\n                    ('Versicolour', 1),\n                    ('Virginica', 2)]:\n    ax.text3D(X[y == label, 3].mean(),\n              X[y == label, 0].mean() + 1.5,\n              X[y == label, 2].mean(), name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n# Reorder the labels to have colors matching the cluster results\ny = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel('Petal width')\nax.set_ylabel('Sepal length')\nax.set_zlabel('Petal length')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html
Color Quantization using K-Means	A							http://scikit-learn.org/stable/auto_examples/index.html			Performs a pixel-wise Vector Quantization (VQ) of an image of the summer palace (China), reducing the number of colors required to show the image from 96,615 unique colors to 64, while preserving the overall appearance quality.<br><br><pre><code># Authors: Robert Layton <robertlayton@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nprint(__doc__)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.utils import shuffle\nfrom time import time\n\nn_colors = 64\n\n# Load the Summer Palace photo\nchina = load_sample_image("china.jpg")\n\n# Convert to floats instead of the default 8 bits integer coding. Dividing by\n# 255 is important so that plt.imshow behaves works well on float data (need to\n# be in the range [0-1]\nchina = np.array(china, dtype=np.float64) / 255\n\n# Load Image and transform to a 2D numpy array.\nw, h, d = original_shape = tuple(china.shape)\nassert d == 3\nimage_array = np.reshape(china, (w * h, d))\n\nprint("Fitting model on a small sub-sample of the data")\nt0 = time()\nimage_array_sample = shuffle(image_array, random_state=0)[:1000]\nkmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\nprint("done in %0.3fs." % (time() - t0))\n\n# Get labels for all points\nprint("Predicting color indices on the full image (k-means)")\nt0 = time()\nlabels = kmeans.predict(image_array)\nprint("done in %0.3fs." % (time() - t0))\n\n\ncodebook_random = shuffle(image_array, random_state=0)[:n_colors + 1]\nprint("Predicting color indices on the full image (random)")\nt0 = time()\nlabels_random = pairwise_distances_argmin(codebook_random,\n                                          image_array,\n                                          axis=0)\nprint("done in %0.3fs." % (time() - t0))\n\n\ndef recreate_image(codebook, labels, w, h):\n    """Recreate the (compressed) image from the code book & labels"""\n    d = codebook.shape[1]\n    image = np.zeros((w, h, d))\n    label_idx = 0\n    for i in range(w):\n        for j in range(h):\n            image[i][j] = codebook[labels[label_idx]]\n            label_idx += 1\n    return image\n\n# Display all results, alongside original image\nplt.figure(1)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Original image (96,615 colors)')\nplt.imshow(china)\n\nplt.figure(2)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Quantized image (64 colors, K-Means)')\nplt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n\nplt.figure(3)\nplt.clf()\nax = plt.axes([0, 0, 1, 1])\nplt.axis('off')\nplt.title('Quantized image (64 colors, Random)')\nplt.imshow(recreate_image(codebook_random, labels_random, w, h))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html
Comparison of Calibration of Classifiers	A							http://scikit-learn.org/stable/auto_examples/index.html			Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approx. 80% actually belong to the positive class.<br><br><pre><code>print(__doc__)\n\n# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport numpy as np\nnp.random.seed(0)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\n\nX, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=2, n_redundant=2)\n\ntrain_samples = 100  # Samples used for training the models\n\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\n\n# Create classifiers\nlr = LogisticRegression()\ngnb = GaussianNB()\nsvc = LinearSVC(C=1.0)\nrfc = RandomForestClassifier(n_estimators=100)\n\n\n###############################################################################\n# Plot calibration plots\n\nplt.figure(figsize=(10, 10))\nax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")\nfor clf, name in [(lr, 'Logistic'),\n                  (gnb, 'Naive Bayes'),\n                  (svc, 'Support Vector Classification'),\n                  (rfc, 'Random Forest')]:\n    clf.fit(X_train, y_train)\n    if hasattr(clf, "predict_proba"):\n        prob_pos = clf.predict_proba(X_test)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(X_test)\n        prob_pos = \\n            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n    fraction_of_positives, mean_predicted_value = \\n        calibration_curve(y_test, prob_pos, n_bins=10)\n\n    ax1.plot(mean_predicted_value, fraction_of_positives, "s-",\n             label="%s" % (name, ))\n\n    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n             histtype="step", lw=2)\n\nax1.set_ylabel("Fraction of positives")\nax1.set_ylim([-0.05, 1.05])\nax1.legend(loc="lower right")\nax1.set_title('Calibration plots  (reliability curve)')\n\nax2.set_xlabel("Mean predicted value")\nax2.set_ylabel("Count")\nax2.legend(loc="upper center", ncol=2)\n\nplt.tight_layout()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html
Compare cross decomposition methods	A							http://scikit-learn.org/stable/auto_examples/index.html			Simple usage of various cross decomposition algorithms: - PLSCanonical - PLSRegression, with multivariate response, a.k.a. PLS2 - PLSRegression, with univariate response, a.k.a. PLS1 - CCA<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_decomposition import PLSCanonical, PLSRegression, CCA\n\n###############################################################################\n# Dataset based latent variables model\n\nn = 500\n# 2 latents vars:\nl1 = np.random.normal(size=n)\nl2 = np.random.normal(size=n)\n\nlatents = np.array([l1, l1, l2, l2]).T\nX = latents + np.random.normal(size=4 * n).reshape((n, 4))\nY = latents + np.random.normal(size=4 * n).reshape((n, 4))\n\nX_train = X[:n / 2]\nY_train = Y[:n / 2]\nX_test = X[n / 2:]\nY_test = Y[n / 2:]\n\nprint("Corr(X)")\nprint(np.round(np.corrcoef(X.T), 2))\nprint("Corr(Y)")\nprint(np.round(np.corrcoef(Y.T), 2))\n\n###############################################################################\n# Canonical (symmetric) PLS\n\n# Transform data\n# ~~~~~~~~~~~~~~\nplsca = PLSCanonical(n_components=2)\nplsca.fit(X_train, Y_train)\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)\n\n# Scatter plot of scores\n# ~~~~~~~~~~~~~~~~~~~~~~\n# 1) On diagonal plot X vs Y scores on each components\nplt.figure(figsize=(12, 8))\nplt.subplot(221)\nplt.plot(X_train_r[:, 0], Y_train_r[:, 0], "ob", label="train")\nplt.plot(X_test_r[:, 0], Y_test_r[:, 0], "or", label="test")\nplt.xlabel("x scores")\nplt.ylabel("y scores")\nplt.title('Comp. 1: X vs Y (test corr = %.2f)' %\n          np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1])\nplt.xticks(())\nplt.yticks(())\nplt.legend(loc="best")\n\nplt.subplot(224)\nplt.plot(X_train_r[:, 1], Y_train_r[:, 1], "ob", label="train")\nplt.plot(X_test_r[:, 1], Y_test_r[:, 1], "or", label="test")\nplt.xlabel("x scores")\nplt.ylabel("y scores")\nplt.title('Comp. 2: X vs Y (test corr = %.2f)' %\n          np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1])\nplt.xticks(())\nplt.yticks(())\nplt.legend(loc="best")\n\n# 2) Off diagonal plot components 1 vs 2 for X and Y\nplt.subplot(222)\nplt.plot(X_train_r[:, 0], X_train_r[:, 1], "*b", label="train")\nplt.plot(X_test_r[:, 0], X_test_r[:, 1], "*r", label="test")\nplt.xlabel("X comp. 1")\nplt.ylabel("X comp. 2")\nplt.title('X comp. 1 vs X comp. 2 (test corr = %.2f)'\n          % np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1])\nplt.legend(loc="best")\nplt.xticks(())\nplt.yticks(())\n\nplt.subplot(223)\nplt.plot(Y_train_r[:, 0], Y_train_r[:, 1], "*b", label="train")\nplt.plot(Y_test_r[:, 0], Y_test_r[:, 1], "*r", label="test")\nplt.xlabel("Y comp. 1")\nplt.ylabel("Y comp. 2")\nplt.title('Y comp. 1 vs Y comp. 2 , (test corr = %.2f)'\n          % np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1])\nplt.legend(loc="best")\nplt.xticks(())\nplt.yticks(())\nplt.show()\n\n###############################################################################\n# PLS regression, with multivariate response, a.k.a. PLS2\n\nn = 1000\nq = 3\np = 10\nX = np.random.normal(size=n * p).reshape((n, p))\nB = np.array([[1, 2] + [0] * (p - 2)] * q).T\n# each Yj = 1*X1 + 2*X2 + noize\nY = np.dot(X, B) + np.random.normal(size=n * q).reshape((n, q)) + 5\n\npls2 = PLSRegression(n_components=3)\npls2.fit(X, Y)\nprint("True B (such that: Y = XB + Err)")\nprint(B)\n# compare pls2.coef_ with B\nprint("Estimated B")\nprint(np.round(pls2.coef_, 1))\npls2.predict(X)\n\n###############################################################################\n# PLS regression, with univariate response, a.k.a. PLS1\n\nn = 1000\np = 10\nX = np.random.normal(size=n * p).reshape((n, p))\ny = X[:, 0] + 2 * X[:, 1] + np.random.normal(size=n * 1) + 5\npls1 = PLSRegression(n_components=3)\npls1.fit(X, y)\n# note that the number of compements exceeds 1 (the dimension of y)\nprint("Estimated betas")\nprint(np.round(pls1.coef_, 1))\n\n###############################################################################\n# CCA (PLS mode B with symmetric deflation)\n\ncca = CCA(n_components=2)\ncca.fit(X_train, Y_train)\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)</code></pre>	http://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_compare_cross_decomposition.html
Comparison of Manifold Learning methods	A							http://scikit-learn.org/stable/auto_examples/index.html			An illustration of dimensionality reduction on the S-curve dataset with various manifold learning methods.<br><br><pre><code># Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>\n\nprint(__doc__)\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold, datasets\n\n# Next line to silence pyflakes. This import is needed.\nAxes3D\n\nn_points = 1000\nX, color = datasets.samples_generator.make_s_curve(n_points, random_state=0)\nn_neighbors = 10\nn_components = 2\n\nfig = plt.figure(figsize=(15, 8))\nplt.suptitle("Manifold Learning with %i points, %i neighbors"\n             % (1000, n_neighbors), fontsize=14)\n\ntry:\n    # compatibility matplotlib < 1.0\n    ax = fig.add_subplot(251, projection='3d')\n    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n    ax.view_init(4, -72)\nexcept:\n    ax = fig.add_subplot(251, projection='3d')\n    plt.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)\n\nmethods = ['standard', 'ltsa', 'hessian', 'modified']\nlabels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']\n\nfor i, method in enumerate(methods):\n    t0 = time()\n    Y = manifold.LocallyLinearEmbedding(n_neighbors, n_components,\n                                        eigen_solver='auto',\n                                        method=method).fit_transform(X)\n    t1 = time()\n    print("%s: %.2g sec" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n    plt.title("%s (%.2g sec)" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis('tight')\n\nt0 = time()\nY = manifold.Isomap(n_neighbors, n_components).fit_transform(X)\nt1 = time()\nprint("Isomap: %.2g sec" % (t1 - t0))\nax = fig.add_subplot(257)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title("Isomap (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n\nt0 = time()\nmds = manifold.MDS(n_components, max_iter=100, n_init=1)\nY = mds.fit_transform(X)\nt1 = time()\nprint("MDS: %.2g sec" % (t1 - t0))\nax = fig.add_subplot(258)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title("MDS (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=n_components,\n                                n_neighbors=n_neighbors)\nY = se.fit_transform(X)\nt1 = time()\nprint("SpectralEmbedding: %.2g sec" % (t1 - t0))\nax = fig.add_subplot(259)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title("SpectralEmbedding (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nt0 = time()\ntsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\nY = tsne.fit_transform(X)\nt1 = time()\nprint("t-SNE: %.2g sec" % (t1 - t0))\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title("t-SNE (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html
Confusion matrix	A							http://scikit-learn.org/stable/auto_examples/index.html			Example of confusion matrix usage to evaluate the quality of the output of a classifier on the iris data set. The diagonal elements represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier. The higher the diagonal values of the confusion matrix the better, indicating many correct...<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into a training set and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Run classifier, using a model that is too regularized (C too low) to see\n# the impact on the results\nclassifier = svm.SVC(kernel='linear', C=0.01)\ny_pred = classifier.fit(X_train, y_train).predict(X_test)\n\n\ndef plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(iris.target_names))\n    plt.xticks(tick_marks, iris.target_names, rotation=45)\n    plt.yticks(tick_marks, iris.target_names)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure()\nplot_confusion_matrix(cm)\n\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nprint('Normalized confusion matrix')\nprint(cm_normalized)\nplt.figure()\nplot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html
Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood	A							http://scikit-learn.org/stable/auto_examples/index.html			When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the sklearn.covariance.EmpiricalCovariance. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example...<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\n\nfrom sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \\n    log_likelihood, empirical_covariance\nfrom sklearn.grid_search import GridSearchCV\n\n\n###############################################################################\n# Generate sample data\nn_features, n_samples = 40, 20\nnp.random.seed(42)\nbase_X_train = np.random.normal(size=(n_samples, n_features))\nbase_X_test = np.random.normal(size=(n_samples, n_features))\n\n# Color samples\ncoloring_matrix = np.random.normal(size=(n_features, n_features))\nX_train = np.dot(base_X_train, coloring_matrix)\nX_test = np.dot(base_X_test, coloring_matrix)\n\n###############################################################################\n# Compute the likelihood on test data\n\n# spanning a range of possible shrinkage coefficient values\nshrinkages = np.logspace(-2, 0, 30)\nnegative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)\n                    for s in shrinkages]\n\n# under the ground-truth model, which we would not have access to in real\n# settings\nreal_cov = np.dot(coloring_matrix.T, coloring_matrix)\nemp_cov = empirical_covariance(X_train)\nloglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))\n\n###############################################################################\n# Compare different approaches to setting the parameter\n\n# GridSearch for an optimal shrinkage coefficient\ntuned_parameters = [{'shrinkage': shrinkages}]\ncv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\ncv.fit(X_train)\n\n# Ledoit-Wolf optimal shrinkage coefficient estimate\nlw = LedoitWolf()\nloglik_lw = lw.fit(X_train).score(X_test)\n\n# OAS coefficient estimate\noa = OAS()\nloglik_oa = oa.fit(X_train).score(X_test)\n\n###############################################################################\n# Plot results\nfig = plt.figure()\nplt.title("Regularized covariance: likelihood and shrinkage coefficient")\nplt.xlabel('Regularizaton parameter: shrinkage coefficient')\nplt.ylabel('Error: negative log-likelihood on test data')\n# range shrinkage curve\nplt.loglog(shrinkages, negative_logliks, label="Negative log-likelihood")\n\nplt.plot(plt.xlim(), 2 * [loglik_real], '--r',\n         label="Real covariance likelihood")\n\n# adjust view\nlik_max = np.amax(negative_logliks)\nlik_min = np.amin(negative_logliks)\nymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))\nymax = lik_max + 10. * np.log(lik_max - lik_min)\nxmin = shrinkages[0]\nxmax = shrinkages[-1]\n# LW likelihood\nplt.vlines(lw.shrinkage_, ymin, -loglik_lw, color='magenta',\n           linewidth=3, label='Ledoit-Wolf estimate')\n# OAS likelihood\nplt.vlines(oa.shrinkage_, ymin, -loglik_oa, color='purple',\n           linewidth=3, label='OAS estimate')\n# best CV estimator likelihood\nplt.vlines(cv.best_estimator_.shrinkage, ymin,\n           -cv.best_estimator_.score(X_test), color='cyan',\n           linewidth=3, label='Cross-validation best estimate')\n\nplt.ylim(ymin, ymax)\nplt.xlim(xmin, xmax)\nplt.legend()\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_covariance_estimation.html
SVM with custom kernel	A							http://scikit-learn.org/stable/auto_examples/index.html			Simple usage of Support Vector Machines to classify a sample. It will plot the decision surface and the support vectors.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\nY = iris.target\n\n\ndef my_kernel(X, Y):\n    """\n    We create a custom kernel:\n\n                 (2  0)\n    k(X, Y) = X  (    ) Y.T\n                 (0  1)\n    """\n    M = np.array([[2, 0], [0, 1.0]])\n    return np.dot(np.dot(X, M), Y.T)\n\n\nh = .02  # step size in the mesh\n\n# we create an instance of SVM and fit out data.\nclf = svm.SVC(kernel=my_kernel)\nclf.fit(X, Y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\nplt.title('3-Class classification using Support Vector Machine with custom'\n          ' kernel')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html
Cross-validation on diabetes Dataset Exercise	A							http://scikit-learn.org/stable/auto_examples/index.html			A tutorial exercise which uses cross-validation with linear models.<br><br><pre><code>from __future__ import print_function\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cross_validation, datasets, linear_model\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data[:150]\ny = diabetes.target[:150]\n\nlasso = linear_model.Lasso()\nalphas = np.logspace(-4, -.5, 30)\n\nscores = list()\nscores_std = list()\n\nfor alpha in alphas:\n    lasso.alpha = alpha\n    this_scores = cross_validation.cross_val_score(lasso, X, y, n_jobs=1)\n    scores.append(np.mean(this_scores))\n    scores_std.append(np.std(this_scores))\n\nplt.figure(figsize=(4, 3))\nplt.semilogx(alphas, scores)\n# plot error lines showing +/- std. errors of the scores\nplt.semilogx(alphas, np.array(scores) + np.array(scores_std) / np.sqrt(len(X)),\n             'b--')\nplt.semilogx(alphas, np.array(scores) - np.array(scores_std) / np.sqrt(len(X)),\n             'b--')\nplt.ylabel('CV score')\nplt.xlabel('alpha')\nplt.axhline(np.max(scores), linestyle='--', color='.5')\n\n##############################################################################\n# Bonus: how much can you trust the selection of alpha?\n\n# To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\nlasso_cv = linear_model.LassoCV(alphas=alphas)\nk_fold = cross_validation.KFold(len(X), 3)\n\nprint("Answer to the bonus question:",\n      "how much can you trust the selection of alpha?")\nprint()\nprint("Alpha parameters maximising the generalization score on different")\nprint("subsets of the data:")\nfor k, (train, test) in enumerate(k_fold):\n    lasso_cv.fit(X[train], y[train])\n    print("[fold {0}] alpha: {1:.5f}, score: {2:.5f}".\n          format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))\nprint()\nprint("Answer: Not very much since we obtained different alphas for different")\nprint("subsets of the data and moreover, the scores for these alphas differ")\nprint("quite substantially.")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html
Cross-validation on Digits Dataset Exercise	A							http://scikit-learn.org/stable/auto_examples/index.html			A tutorial exercise using Cross-validation with an SVM on the Digits dataset.<br><br><pre><code>print(__doc__)\n\n\nimport numpy as np\nfrom sklearn import cross_validation, datasets, svm\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\nsvc = svm.SVC(kernel='linear')\nC_s = np.logspace(-10, 0, 10)\n\nscores = list()\nscores_std = list()\nfor C in C_s:\n    svc.C = C\n    this_scores = cross_validation.cross_val_score(svc, X, y, n_jobs=1)\n    scores.append(np.mean(this_scores))\n    scores_std.append(np.std(this_scores))\n\n# Do the plotting\nimport matplotlib.pyplot as plt\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.semilogx(C_s, scores)\nplt.semilogx(C_s, np.array(scores) + np.array(scores_std), 'b--')\nplt.semilogx(C_s, np.array(scores) - np.array(scores_std), 'b--')\nlocs, labels = plt.yticks()\nplt.yticks(locs, list(map(lambda x: "%g" % x, locs)))\nplt.ylabel('CV score')\nplt.xlabel('Parameter C')\nplt.ylim(0, 1.1)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_digits.html
Plotting Cross-Validated Predictions	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows how to use cross_val_predict to visualize prediction errors.<br><br><pre><code>from sklearn import datasets\nfrom sklearn.cross_validation import cross_val_predict\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\nlr = linear_model.LinearRegression()\nboston = datasets.load_boston()\ny = boston.target\n\n# cross_val_predict returns an array of the same size as `y` where each entry\n# is a prediction obtained by cross validated:\npredicted = cross_val_predict(lr, boston.data, y, cv=10)\n\nfig, ax = plt.subplots()\nax.scatter(y, predicted)\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html
Demo of DBSCAN clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			Finds core samples of high density and expands clusters from them.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n\n\n##############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n                            random_state=0)\n\nX = StandardScaler().fit_transform(X)\n\n##############################################################################\n# Compute DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))\nprint("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))\nprint("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))\nprint("Adjusted Rand Index: %0.3f"\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint("Adjusted Mutual Information: %0.3f"\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint("Silhouette Coefficient: %0.3f"\n      % metrics.silhouette_score(X, labels))\n\n##############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = 'k'\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html
Online learning of a dictionary of parts of faces	A							http://scikit-learn.org/stable/auto_examples/index.html			This example uses a large dataset of faces to learn a set of 20 x 20 images patches that constitute faces.<br><br><pre><code>print(__doc__)\n\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nfrom sklearn import datasets\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.image import extract_patches_2d\n\nfaces = datasets.fetch_olivetti_faces()\n\n###############################################################################\n# Learn the dictionary of images\n\nprint('Learning the dictionary... ')\nrng = np.random.RandomState(0)\nkmeans = MiniBatchKMeans(n_clusters=81, random_state=rng, verbose=True)\npatch_size = (20, 20)\n\nbuffer = []\nindex = 1\nt0 = time.time()\n\n# The online learning part: cycle over the whole dataset 6 times\nindex = 0\nfor _ in range(6):\n    for img in faces.images:\n        data = extract_patches_2d(img, patch_size, max_patches=50,\n                                  random_state=rng)\n        data = np.reshape(data, (len(data), -1))\n        buffer.append(data)\n        index += 1\n        if index % 10 == 0:\n            data = np.concatenate(buffer, axis=0)\n            data -= np.mean(data, axis=0)\n            data /= np.std(data, axis=0)\n            kmeans.partial_fit(data)\n            buffer = []\n        if index % 100 == 0:\n            print('Partial fit of %4i out of %i'\n                  % (index, 6 * len(faces.images)))\n\ndt = time.time() - t0\nprint('done in %.2fs.' % dt)\n\n###############################################################################\n# Plot the results\nplt.figure(figsize=(4.2, 4))\nfor i, patch in enumerate(kmeans.cluster_centers_):\n    plt.subplot(9, 9, i + 1)\n    plt.imshow(patch.reshape(patch_size), cmap=plt.cm.gray,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\n\n\nplt.suptitle('Patches of faces\nTrain time %.1fs on %d patches' %\n             (dt, 8 * len(faces.images)), fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_dict_face_patches.html
Feature agglomeration	A							http://scikit-learn.org/stable/auto_examples/index.html			These images how similar features are merged together using feature agglomeration.<br><br><pre><code>print(__doc__)\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets, cluster\nfrom sklearn.feature_extraction.image import grid_to_graph\n\ndigits = datasets.load_digits()\nimages = digits.images\nX = np.reshape(images, (len(images), -1))\nconnectivity = grid_to_graph(*images[0].shape)\n\nagglo = cluster.FeatureAgglomeration(connectivity=connectivity,\n                                     n_clusters=32)\n\nagglo.fit(X)\nX_reduced = agglo.transform(X)\n\nX_restored = agglo.inverse_transform(X_reduced)\nimages_restored = np.reshape(X_restored, images.shape)\nplt.figure(1, figsize=(4, 3.5))\nplt.clf()\nplt.subplots_adjust(left=.01, right=.99, bottom=.01, top=.91)\nfor i in range(4):\n    plt.subplot(3, 4, i + 1)\n    plt.imshow(images[i], cmap=plt.cm.gray, vmax=16, interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\n    if i == 1:\n        plt.title('Original data')\n    plt.subplot(3, 4, 4 + i + 1)\n    plt.imshow(images_restored[i], cmap=plt.cm.gray, vmax=16,\n               interpolation='nearest')\n    if i == 1:\n        plt.title('Agglomerated data')\n    plt.xticks(())\n    plt.yticks(())\n\nplt.subplot(3, 4, 10)\nplt.imshow(np.reshape(agglo.labels_, images[0].shape),\n           interpolation='nearest', cmap=plt.cm.spectral)\nplt.xticks(())\nplt.yticks(())\nplt.title('Labels')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_agglomeration.html
Recognizing hand-written digits	A							http://scikit-learn.org/stable/auto_examples/index.html			An example showing how the scikit-learn can be used to recognize images of hand-written digits.<br><br><pre><code>print(__doc__)\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# License: BSD 3 clause\n\n# Standard scientific Python imports\nimport matplotlib.pyplot as plt\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, metrics\n\n# The digits dataset\ndigits = datasets.load_digits()\n\n# The data that we are interested in is made of 8x8 images of digits, let's\n# have a look at the first 3 images, stored in the `images` attribute of the\n# dataset.  If we were working from image files, we could load them using\n# pylab.imread.  Note that each image must have the same size. For these\n# images, we know which digit they represent: it is given in the 'target' of\n# the dataset.\nimages_and_labels = list(zip(digits.images, digits.target))\nfor index, (image, label) in enumerate(images_and_labels[:4]):\n    plt.subplot(2, 4, index + 1)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Training: %i' % label)\n\n# To apply a classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# We learn the digits on the first half of the digits\nclassifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])\n\n# Now predict the value of the digit on the second half:\nexpected = digits.target[n_samples / 2:]\npredicted = classifier.predict(data[n_samples / 2:])\n\nprint("Classification report for classifier %s:\n%s\n"\n      % (classifier, metrics.classification_report(expected, predicted)))\nprint("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))\n\nimages_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))\nfor index, (image, prediction) in enumerate(images_and_predictions[:4]):\n    plt.subplot(2, 4, index + 5)\n    plt.axis('off')\n    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n    plt.title('Prediction: %i' % prediction)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html
Kernel Density Estimation	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows how kernel density estimation (KDE), a powerful non-parametric density estimation technique, can be used to learn a generative model for a dataset. With this generative model in place, new samples can be drawn. These new samples reflect the underlying model of the data.<br><br><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.decomposition import PCA\nfrom sklearn.grid_search import GridSearchCV\n\n# load the data\ndigits = load_digits()\ndata = digits.data\n\n# project the 64-dimensional data to a lower dimension\npca = PCA(n_components=15, whiten=False)\ndata = pca.fit_transform(digits.data)\n\n# use grid search cross-validation to optimize the bandwidth\nparams = {'bandwidth': np.logspace(-1, 1, 20)}\ngrid = GridSearchCV(KernelDensity(), params)\ngrid.fit(data)\n\nprint("best bandwidth: {0}".format(grid.best_estimator_.bandwidth))\n\n# use the best estimator to compute the kernel density estimate\nkde = grid.best_estimator_\n\n# sample 44 new points from the data\nnew_data = kde.sample(44, random_state=0)\nnew_data = pca.inverse_transform(new_data)\n\n# turn data into a 4x11 grid\nnew_data = new_data.reshape((4, 11, -1))\nreal_data = digits.data[:44].reshape((4, 11, -1))\n\n# plot real digits and resampled digits\nfig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\nfor j in range(11):\n    ax[4, j].set_visible(False)\n    for i in range(4):\n        im = ax[i, j].imshow(real_data[i, j].reshape((8, 8)),\n                             cmap=plt.cm.binary, interpolation='nearest')\n        im.set_clim(0, 16)\n        im = ax[i + 5, j].imshow(new_data[i, j].reshape((8, 8)),\n                                 cmap=plt.cm.binary, interpolation='nearest')\n        im.set_clim(0, 16)\n\nax[0, 5].set_title('Selection from the input data')\nax[5, 5].set_title('"New" digits drawn from the kernel density model')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_digits_kde_sampling.html
The Digit Dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we’d have to first transform it into a feature vector with length 64.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nfrom sklearn import datasets\n\nimport matplotlib.pyplot as plt\n\n#Load the digits dataset\ndigits = datasets.load_digits()\n\n#Display the first digit\nplt.figure(1, figsize=(3, 3))\nplt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html
Various Agglomerative Clustering on a 2D embedding of digits	A							http://scikit-learn.org/stable/auto_examples/index.html			An illustration of various linkage option for agglomerative clustering on a 2D embedding of the digits dataset.<br><br><pre><code># Authors: Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2014\n\nprint(__doc__)\nfrom time import time\n\nimport numpy as np\nfrom scipy import ndimage\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import manifold, datasets\n\ndigits = datasets.load_digits(n_class=10)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\n\nnp.random.seed(0)\n\ndef nudge_images(X, y):\n    # Having a larger dataset shows more clearly the behavior of the\n    # methods, but we multiply the size of the dataset only by 2, as the\n    # cost of the hierarchical clustering methods are strongly\n    # super-linear in n_samples\n    shift = lambda x: ndimage.shift(x.reshape((8, 8)),\n                                  .3 * np.random.normal(size=2),\n                                  mode='constant',\n                                  ).ravel()\n    X = np.concatenate([X, np.apply_along_axis(shift, 1, X)])\n    Y = np.concatenate([y, y], axis=0)\n    return X, Y\n\n\nX, y = nudge_images(X, y)\n\n\n#----------------------------------------------------------------------\n# Visualize the clustering\ndef plot_clustering(X_red, X, labels, title=None):\n    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n    X_red = (X_red - x_min) / (x_max - x_min)\n\n    plt.figure(figsize=(6, 4))\n    for i in range(X_red.shape[0]):\n        plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),\n                 color=plt.cm.spectral(labels[i] / 10.),\n                 fontdict={'weight': 'bold', 'size': 9})\n\n    plt.xticks([])\n    plt.yticks([])\n    if title is not None:\n        plt.title(title, size=17)\n    plt.axis('off')\n    plt.tight_layout()\n\n#----------------------------------------------------------------------\n# 2D embedding of the digits dataset\nprint("Computing embedding")\nX_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)\nprint("Done.")\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nfor linkage in ('ward', 'average', 'complete'):\n    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=10)\n    t0 = time()\n    clustering.fit(X_red)\n    print("%s : %.2fs" % (linkage, time() - t0))\n\n    plot_clustering(X_red, X, clustering.labels_, "%s linkage" % linkage)\n\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html
Pipelining: chaining a PCA and a logistic regression	A							http://scikit-learn.org/stable/auto_examples/index.html			The PCA does an unsupervised dimensionality reduction, while the logistic regression does the prediction.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model, decomposition, datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\n\nlogistic = linear_model.LogisticRegression()\n\npca = decomposition.PCA()\npipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])\n\ndigits = datasets.load_digits()\nX_digits = digits.data\ny_digits = digits.target\n\n###############################################################################\n# Plot the PCA spectrum\npca.fit(X_digits)\n\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_')\n\n###############################################################################\n# Prediction\n\nn_components = [20, 40, 64]\nCs = np.logspace(-4, 4, 3)\n\n#Parameters of pipelines can be set using ‘__’ separated parameter names:\n\nestimator = GridSearchCV(pipe,\n                         dict(pca__n_components=n_components,\n                              logistic__C=Cs))\nestimator.fit(X_digits, y_digits)\n\nplt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,\n            linestyle=':', label='n_components chosen')\nplt.legend(prop=dict(size=12))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_digits_pipe.html
OOB Errors for Random Forests	A							http://scikit-learn.org/stable/auto_examples/index.html			The RandomForestClassifier is trained using bootstrap aggregation, where each new tree is fit from a bootstrap sample of the training observations . The out-of-bag (OOB) error is the average error for each calculated using predictions from the trees that do not contain in their respective bootstrap sample. This allows the RandomForestClassifier to be fit and validated whilst being trained [1].<br><br><pre><code>import matplotlib.pyplot as plt\n\nfrom collections import OrderedDict\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\n# Author: Kian Ho <hui.kian.ho@gmail.com>\n#         Gilles Louppe <g.louppe@gmail.com>\n#         Andreas Mueller <amueller@ais.uni-bonn.de>\n#\n# License: BSD 3 Clause\n\nprint(__doc__)\n\nRANDOM_STATE = 123\n\n# Generate a binary classification dataset.\nX, y = make_classification(n_samples=500, n_features=25,\n                           n_clusters_per_class=1, n_informative=15,\n                           random_state=RANDOM_STATE)\n\n# NOTE: Setting the `warm_start` construction parameter to `True` disables\n# support for paralellised ensembles but is necessary for tracking the OOB\n# error trajectory during training.\nensemble_clfs = [\n    ("RandomForestClassifier, max_features='sqrt'",\n        RandomForestClassifier(warm_start=True, oob_score=True,\n                               max_features="sqrt",\n                               random_state=RANDOM_STATE)),\n    ("RandomForestClassifier, max_features='log2'",\n        RandomForestClassifier(warm_start=True, max_features='log2',\n                               oob_score=True,\n                               random_state=RANDOM_STATE)),\n    ("RandomForestClassifier, max_features=None",\n        RandomForestClassifier(warm_start=True, max_features=None,\n                               oob_score=True,\n                               random_state=RANDOM_STATE))\n]\n\n# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\nerror_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n\n# Range of `n_estimators` values to explore.\nmin_estimators = 15\nmax_estimators = 175\n\nfor label, clf in ensemble_clfs:\n    for i in range(min_estimators, max_estimators + 1):\n        clf.set_params(n_estimators=i)\n        clf.fit(X, y)\n\n        # Record the OOB error for each `n_estimators=i` setting.\n        oob_error = 1 - clf.oob_score_\n        error_rate[label].append((i, oob_error))\n\n# Generate the "OOB error rate" vs. "n_estimators" plot.\nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label)\n\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel("n_estimators")\nplt.ylabel("OOB error rate")\nplt.legend(loc="upper right")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html
Vector Quantization Example	A							http://scikit-learn.org/stable/auto_examples/index.html			Face, a 1024 x 768 size image of a raccoon face, is used here to illustrate how k-means is used for vector quantization.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.fixes import sp_version\n\nif sp_version < (0, 12):\n    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "\n                   "thus does not include the scipy.misc.face() image.")\n\ntry:\n    face = sp.face(gray=True)\nexcept AttributeError:\n    # Newer versions of scipy have face in misc\n    from scipy import misc\n    face = misc.face(gray=True)\n\nn_clusters = 5\nnp.random.seed(0)\n    \nX = face.reshape((-1, 1))  # We need an (n_sample, n_feature) array\nk_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)\nk_means.fit(X)\nvalues = k_means.cluster_centers_.squeeze()\nlabels = k_means.labels_\n\n# create an array from labels and values\nface_compressed = np.choose(labels, values)\nface_compressed.shape = face.shape\n\nvmin = face.min()\nvmax = face.max()\n\n# original face\nplt.figure(1, figsize=(3, 2.2))\nplt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n\n# compressed face\nplt.figure(2, figsize=(3, 2.2))\nplt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n\n# equal bins face\nregular_values = np.linspace(0, 256, n_clusters + 1)\nregular_labels = np.searchsorted(regular_values, face) - 1\nregular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\nregular_face = np.choose(regular_labels.ravel(), regular_values, mode="clip")\nregular_face.shape = face.shape\nplt.figure(3, figsize=(3, 2.2))\nplt.imshow(regular_face, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n\n# histogram\nplt.figure(4, figsize=(3, 2.2))\nplt.clf()\nplt.axes([.01, .01, .98, .98])\nplt.hist(X, bins=256, color='.5', edgecolor='.5')\nplt.yticks(())\nplt.xticks(regular_values)\nvalues = np.sort(values)\nfor center_1, center_2 in zip(values[:-1], values[1:]):\n    plt.axvline(.5 * (center_1 + center_2), color='b')\n\nfor center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_face_compress.html
Segmenting the picture of a raccoon face in regions	A							http://scikit-learn.org/stable/auto_examples/index.html			This example uses Spectral clustering on a graph created from voxel-to-voxel difference on an image to break this image into multiple partly-homogeneous regions.<br><br><pre><code>print(__doc__)\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>, Brian Cheung\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import spectral_clustering\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.fixes import sp_version\n\nif sp_version < (0, 12):\n    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "\n                   "thus does not include the scipy.misc.face() image.")\n\n\n# load the raccoon face as a numpy array\ntry:\n    face = sp.face(gray=True)\nexcept AttributeError:\n    # Newer versions of scipy have face in misc\n    from scipy import misc\n    face = misc.face(gray=True)\n\n# Resize it to 10% of the original size to speed up the processing\nface = sp.misc.imresize(face, 0.10) / 255.\n\n# Convert the image into a graph with the value of the gradient on the\n# edges.\ngraph = image.img_to_graph(face)\n\n# Take a decreasing function of the gradient: an exponential\n# The smaller beta is, the more independent the segmentation is of the\n# actual image. For beta=1, the segmentation is close to a voronoi\nbeta = 5\neps = 1e-6\ngraph.data = np.exp(-beta * graph.data / graph.data.std()) + eps\n\n# Apply spectral clustering (this step goes much faster if you have pyamg\n# installed)\nN_REGIONS = 25\n\n#############################################################################\n# Visualize the resulting regions\n\nfor assign_labels in ('kmeans', 'discretize'):\n    t0 = time.time()\n    labels = spectral_clustering(graph, n_clusters=N_REGIONS,\n                                 assign_labels=assign_labels, random_state=1)\n    t1 = time.time()\n    labels = labels.reshape(face.shape)\n\n    plt.figure(figsize=(5, 5))\n    plt.imshow(face, cmap=plt.cm.gray)\n    for l in range(N_REGIONS):\n        plt.contour(labels == l, contours=1,\n                    colors=[plt.cm.spectral(l / float(N_REGIONS))])\n    plt.xticks(())\n    plt.yticks(())\n    title = 'Spectral clustering: %s, %.2fs' % (assign_labels, (t1 - t0))\n    print(title)\n    plt.title(title)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_face_segmentation.html
A demo of structured Ward hierarchical clustering on a raccoon face image	A							http://scikit-learn.org/stable/auto_examples/index.html			Compute the segmentation of a 2D image with Ward hierarchical clustering. The clustering is spatially constrained in order for each segmented region to be in one piece.<br><br><pre><code># Author : Vincent Michel, 2010\n#          Alexandre Gramfort, 2011\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport time as time\n\nimport numpy as np\nimport scipy as sp\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.image import grid_to_graph\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.fixes import sp_version\n\nif sp_version < (0, 12):\n    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "\n                   "thus does not include the scipy.misc.face() image.")\n\n\n###############################################################################\n# Generate data\ntry:\n    face = sp.face(gray=True)\nexcept AttributeError:\n    # Newer versions of scipy have face in misc\n    from scipy import misc\n    face = misc.face(gray=True)\n\n# Resize it to 10% of the original size to speed up the processing\nface = sp.misc.imresize(face, 0.10) / 255.\n\nX = np.reshape(face, (-1, 1))\n\n###############################################################################\n# Define the structure A of the data. Pixels connected to their neighbors.\nconnectivity = grid_to_graph(*face.shape)\n\n###############################################################################\n# Compute clustering\nprint("Compute structured hierarchical clustering...")\nst = time.time()\nn_clusters = 15  # number of regions\nward = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward',\n                               connectivity=connectivity)\nward.fit(X)\nlabel = np.reshape(ward.labels_, face.shape)\nprint("Elapsed time: ", time.time() - st)\nprint("Number of pixels: ", label.size)\nprint("Number of clusters: ", np.unique(label).size)\n\n###############################################################################\n# Plot the results on an image\nplt.figure(figsize=(5, 5))\nplt.imshow(face, cmap=plt.cm.gray)\nfor l in range(n_clusters):\n    plt.contour(label == l, contours=1,\n                colors=[plt.cm.spectral(l / float(n_clusters)), ])\nplt.xticks(())\nplt.yticks(())\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_face_ward_segmentation.html
Faces dataset decompositions	A							http://scikit-learn.org/stable/auto_examples/index.html			This example applies to The Olivetti faces dataset different unsupervised matrix decomposition (dimension reduction) methods from the module sklearn.decomposition (see the documentation chapter Decomposing signals in components (matrix factorization problems)) .<br><br><pre><code>print(__doc__)\n\n# Authors: Vlad Niculae, Alexandre Gramfort\n# License: BSD 3 clause\n\nimport logging\nfrom time import time\n\nfrom numpy.random import RandomState\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn import decomposition\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\nn_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\nrng = RandomState(0)\n\n###############################################################################\n# Load faces data\ndataset = fetch_olivetti_faces(shuffle=True, random_state=rng)\nfaces = dataset.data\n\nn_samples, n_features = faces.shape\n\n# global centering\nfaces_centered = faces - faces.mean(axis=0)\n\n# local centering\nfaces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n\nprint("Dataset consists of %d faces" % n_samples)\n\n\n###############################################################################\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row):\n    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n    plt.suptitle(title, size=16)\n    for i, comp in enumerate(images):\n        plt.subplot(n_row, n_col, i + 1)\n        vmax = max(comp.max(), -comp.min())\n        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n                   interpolation='nearest',\n                   vmin=-vmax, vmax=vmax)\n        plt.xticks(())\n        plt.yticks(())\n    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n\n###############################################################################\n# List of the different estimators, whether to center and transpose the\n# problem, and whether the transformer uses the clustering API.\nestimators = [\n    ('Eigenfaces - RandomizedPCA',\n     decomposition.RandomizedPCA(n_components=n_components, whiten=True),\n     True),\n\n    ('Non-negative components - NMF',\n     decomposition.NMF(n_components=n_components, init='nndsvda', tol=5e-3),\n     False),\n\n    ('Independent components - FastICA',\n     decomposition.FastICA(n_components=n_components, whiten=True),\n     True),\n\n    ('Sparse comp. - MiniBatchSparsePCA',\n     decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,\n                                      n_iter=100, batch_size=3,\n                                      random_state=rng),\n     True),\n\n    ('MiniBatchDictionaryLearning',\n        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n                                                  n_iter=50, batch_size=3,\n                                                  random_state=rng),\n     True),\n\n    ('Cluster centers - MiniBatchKMeans',\n        MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,\n                        max_iter=50, random_state=rng),\n     True),\n\n    ('Factor Analysis components - FA',\n     decomposition.FactorAnalysis(n_components=n_components, max_iter=2),\n     True),\n]\n\n\n###############################################################################\n# Plot a sample of the input data\n\nplot_gallery("First centered Olivetti faces", faces_centered[:n_components])\n\n###############################################################################\n# Do the estimation and plot it\n\nfor name, estimator, center in estimators:\n    print("Extracting the top %d %s..." % (n_components, name))\n    t0 = time()\n    data = faces\n    if center:\n        data = faces_centered\n    estimator.fit(data)\n    train_time = (time() - t0)\n    print("done in %0.3fs" % train_time)\n    if hasattr(estimator, 'cluster_centers_'):\n        components_ = estimator.cluster_centers_\n    else:\n        components_ = estimator.components_\n    if hasattr(estimator, 'noise_variance_'):\n        plot_gallery("Pixelwise variance",\n                     estimator.noise_variance_.reshape(1, -1), n_col=1,\n                     n_row=1)\n    plot_gallery('%s - Train time %.1fs' % (name, train_time),\n                 components_[:n_components])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html
Feature agglomeration vs. univariate selection	A							http://scikit-learn.org/stable/auto_examples/index.html			This example compares 2 dimensionality reduction strategies:<br><br><pre><code># Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport shutil\nimport tempfile\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg, ndimage\n\nfrom sklearn.feature_extraction.image import grid_to_graph\nfrom sklearn import feature_selection\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.externals.joblib import Memory\nfrom sklearn.cross_validation import KFold\n\n###############################################################################\n# Generate data\nn_samples = 200\nsize = 40  # image size\nroi_size = 15\nsnr = 5.\nnp.random.seed(0)\nmask = np.ones([size, size], dtype=np.bool)\n\ncoef = np.zeros((size, size))\ncoef[0:roi_size, 0:roi_size] = -1.\ncoef[-roi_size:, -roi_size:] = 1.\n\nX = np.random.randn(n_samples, size ** 2)\nfor x in X:  # smooth data\n    x[:] = ndimage.gaussian_filter(x.reshape(size, size), sigma=1.0).ravel()\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\ny = np.dot(X, coef.ravel())\nnoise = np.random.randn(y.shape[0])\nnoise_coef = (linalg.norm(y, 2) / np.exp(snr / 20.)) / linalg.norm(noise, 2)\ny += noise_coef * noise  # add noise\n\n###############################################################################\n# Compute the coefs of a Bayesian Ridge with GridSearch\ncv = KFold(len(y), 2)  # cross-validation generator for model selection\nridge = BayesianRidge()\ncachedir = tempfile.mkdtemp()\nmem = Memory(cachedir=cachedir, verbose=1)\n\n# Ward agglomeration followed by BayesianRidge\nconnectivity = grid_to_graph(n_x=size, n_y=size)\nward = FeatureAgglomeration(n_clusters=10, connectivity=connectivity,\n                            memory=mem)\nclf = Pipeline([('ward', ward), ('ridge', ridge)])\n# Select the optimal number of parcels with grid search\nclf = GridSearchCV(clf, {'ward__n_clusters': [10, 20, 30]}, n_jobs=1, cv=cv)\nclf.fit(X, y)  # set the best parameters\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_)\ncoef_agglomeration_ = coef_.reshape(size, size)\n\n# Anova univariate feature selection followed by BayesianRidge\nf_regression = mem.cache(feature_selection.f_regression)  # caching function\nanova = feature_selection.SelectPercentile(f_regression)\nclf = Pipeline([('anova', anova), ('ridge', ridge)])\n# Select the optimal percentage of features with grid search\nclf = GridSearchCV(clf, {'anova__percentile': [5, 10, 20]}, cv=cv)\nclf.fit(X, y)  # set the best parameters\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_.reshape(1, -1))\ncoef_selection_ = coef_.reshape(size, size)\n\n###############################################################################\n# Inverse the transformation to plot the results on an image\nplt.close('all')\nplt.figure(figsize=(7.3, 2.7))\nplt.subplot(1, 3, 1)\nplt.imshow(coef, interpolation="nearest", cmap=plt.cm.RdBu_r)\nplt.title("True weights")\nplt.subplot(1, 3, 2)\nplt.imshow(coef_selection_, interpolation="nearest", cmap=plt.cm.RdBu_r)\nplt.title("Feature Selection")\nplt.subplot(1, 3, 3)\nplt.imshow(coef_agglomeration_, interpolation="nearest", cmap=plt.cm.RdBu_r)\nplt.title("Feature Agglomeration")\nplt.subplots_adjust(0.04, 0.0, 0.98, 0.94, 0.16, 0.26)\nplt.show()\n\n# Attempt to remove the temporary cachedir, but don't worry if it fails\nshutil.rmtree(cachedir, ignore_errors=True)</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_feature_agglomeration_vs_univariate_selection.html
Univariate Feature Selection	A							http://scikit-learn.org/stable/auto_examples/index.html			An example showing univariate feature selection.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets, svm\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\n###############################################################################\n# import some data to play with\n\n# The iris dataset\niris = datasets.load_iris()\n\n# Some noisy data not correlated\nE = np.random.uniform(0, 0.1, size=(len(iris.data), 20))\n\n# Add the noisy data to the informative features\nX = np.hstack((iris.data, E))\ny = iris.target\n\n###############################################################################\nplt.figure(1)\nplt.clf()\n\nX_indices = np.arange(X.shape[-1])\n\n###############################################################################\n# Univariate feature selection with F-test for feature scoring\n# We use the default selection function: the 10% most significant features\nselector = SelectPercentile(f_classif, percentile=10)\nselector.fit(X, y)\nscores = -np.log10(selector.pvalues_)\nscores /= scores.max()\nplt.bar(X_indices - .45, scores, width=.2,\n        label=r'Univariate score ($-Log(p_{value})$)', color='g')\n\n###############################################################################\n# Compare to the weights of an SVM\nclf = svm.SVC(kernel='linear')\nclf.fit(X, y)\n\nsvm_weights = (clf.coef_ ** 2).sum(axis=0)\nsvm_weights /= svm_weights.max()\n\nplt.bar(X_indices - .25, svm_weights, width=.2, label='SVM weight', color='r')\n\nclf_selected = svm.SVC(kernel='linear')\nclf_selected.fit(selector.transform(X), y)\n\nsvm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\nsvm_weights_selected /= svm_weights_selected.max()\n\nplt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n        width=.2, label='SVM weights after selection', color='b')\n\n\nplt.title("Comparing feature selection")\nplt.xlabel('Feature number')\nplt.yticks(())\nplt.axis('tight')\nplt.legend(loc='upper right')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html
Feature transformations with ensembles of trees	A							http://scikit-learn.org/stable/auto_examples/index.html			Transform your features into a higher dimensional, sparse space. Then train a linear model on these features.<br><br><pre><code># Author: Tim Head <betatim@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nnp.random.seed(10)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n                              GradientBoostingClassifier)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.pipeline import make_pipeline\n\nn_estimator = 10\nX, y = make_classification(n_samples=80000)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n# It is important to train the ensemble of trees on a different subset\n# of the training data than the linear regression model to avoid\n# overfitting, in particular if the total number of leaves is\n# similar to the number of training samples\nX_train, X_train_lr, y_train, y_train_lr = train_test_split(X_train,\n                                                            y_train,\n                                                            test_size=0.5)\n\n# Unsupervised transformation based on totally random trees\nrt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,\n	random_state=0)\n\nrt_lm = LogisticRegression()\npipeline = make_pipeline(rt, rt_lm)\npipeline.fit(X_train, y_train)\ny_pred_rt = pipeline.predict_proba(X_test)[:, 1]\nfpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)\n\n# Supervised transformation based on random forests\nrf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\nrf_enc = OneHotEncoder()\nrf_lm = LogisticRegression()\nrf.fit(X_train, y_train)\nrf_enc.fit(rf.apply(X_train))\nrf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n\ny_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\nfpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n\ngrd = GradientBoostingClassifier(n_estimators=n_estimator)\ngrd_enc = OneHotEncoder()\ngrd_lm = LogisticRegression()\ngrd.fit(X_train, y_train)\ngrd_enc.fit(grd.apply(X_train)[:, :, 0])\ngrd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n\ny_pred_grd_lm = grd_lm.predict_proba(\n    grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\nfpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n\n\n# The gradient boosted model by itself\ny_pred_grd = grd.predict_proba(X_test)[:, 1]\nfpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n\n\n# The random forest model by itself\ny_pred_rf = rf.predict_proba(X_test)[:, 1]\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\nplt.plot(fpr_rf, tpr_rf, label='RF')\nplt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\nplt.plot(fpr_grd, tpr_grd, label='GBT')\nplt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n\nplt.figure(2)\nplt.xlim(0, 0.2)\nplt.ylim(0.8, 1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_rt_lm, tpr_rt_lm, label='RT + LR')\nplt.plot(fpr_rf, tpr_rf, label='RF')\nplt.plot(fpr_rf_lm, tpr_rf_lm, label='RF + LR')\nplt.plot(fpr_grd, tpr_grd, label='GBT')\nplt.plot(fpr_grd_lm, tpr_grd_lm, label='GBT + LR')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve (zoomed in at top left)')\nplt.legend(loc='best')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html
Feature importances with forests of trees	A							http://scikit-learn.org/stable/auto_examples/index.html			This examples shows the use of forests of trees to evaluate the importance of features on an artificial classification task. The red bars are the feature importances of the forest, along with their inter-trees variability.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000,\n                           n_features=10,\n                           n_informative=3,\n                           n_redundant=0,\n                           n_repeated=0,\n                           n_classes=2,\n                           random_state=0,\n                           shuffle=False)\n\n# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint("Feature ranking:")\n\nfor f in range(X.shape[1]):\n    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title("Feature importances")\nplt.bar(range(X.shape[1]), importances[indices],\n       color="r", yerr=std[indices], align="center")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html
Pixel importances with a parallel forest of trees	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows the use of forests of trees to evaluate the importance of the pixels in an image classification task (faces). The hotter the pixel, the more important.<br><br><pre><code>print(__doc__)\n\nfrom time import time\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Number of cores to use to perform parallel fitting of the forest model\nn_jobs = 1\n\n# Load the faces dataset\ndata = fetch_olivetti_faces()\nX = data.images.reshape((len(data.images), -1))\ny = data.target\n\nmask = y < 5  # Limit to 5 classes\nX = X[mask]\ny = y[mask]\n\n# Build a forest and compute the pixel importances\nprint("Fitting ExtraTreesClassifier on faces data with %d cores..." % n_jobs)\nt0 = time()\nforest = ExtraTreesClassifier(n_estimators=1000,\n                              max_features=128,\n                              n_jobs=n_jobs,\n                              random_state=0)\n\nforest.fit(X, y)\nprint("done in %0.3fs" % (time() - t0))\nimportances = forest.feature_importances_\nimportances = importances.reshape(data.images[0].shape)\n\n# Plot pixel importances\nplt.matshow(importances, cmap=plt.cm.hot)\nplt.title("Pixel importances with forests of trees")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html
Plot the decision surfaces of ensembles of trees on the iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the decision surfaces of forests of randomized trees trained on pairs of features of the iris dataset.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import clone\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\nfrom sklearn.externals.six.moves import xrange\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Parameters\nn_classes = 3\nn_estimators = 30\nplot_colors = "ryb"\ncmap = plt.cm.RdYlBu\nplot_step = 0.02  # fine step width for decision surface contours\nplot_step_coarser = 0.5  # step widths for coarse classifier guesses\nRANDOM_SEED = 13  # fix the seed on each iteration\n\n# Load data\niris = load_iris()\n\nplot_idx = 1\n\nmodels = [DecisionTreeClassifier(max_depth=None),\n          RandomForestClassifier(n_estimators=n_estimators),\n          ExtraTreesClassifier(n_estimators=n_estimators),\n          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n                             n_estimators=n_estimators)]\n\nfor pair in ([0, 1], [0, 2], [2, 3]):\n    for model in models:\n        # We only take the two corresponding features\n        X = iris.data[:, pair]\n        y = iris.target\n\n        # Shuffle\n        idx = np.arange(X.shape[0])\n        np.random.seed(RANDOM_SEED)\n        np.random.shuffle(idx)\n        X = X[idx]\n        y = y[idx]\n\n        # Standardize\n        mean = X.mean(axis=0)\n        std = X.std(axis=0)\n        X = (X - mean) / std\n\n        # Train\n        clf = clone(model)\n        clf = model.fit(X, y)\n\n        scores = clf.score(X, y)\n        # Create a title for each column and the console by using str() and\n        # slicing away useless parts of the string\n        model_title = str(type(model)).split(".")[-1][:-2][:-len("Classifier")]\n        model_details = model_title\n        if hasattr(model, "estimators_"):\n            model_details += " with {} estimators".format(len(model.estimators_))\n        print( model_details + " with features", pair, "has a score of", scores )\n\n        plt.subplot(3, 4, plot_idx)\n        if plot_idx <= len(models):\n            # Add a title at the top of each column\n            plt.title(model_title)\n\n        # Now plot the decision boundary using a fine mesh as input to a\n        # filled contour plot\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                             np.arange(y_min, y_max, plot_step))\n\n        # Plot either a single DecisionTreeClassifier or alpha blend the\n        # decision surfaces of the ensemble of classifiers\n        if isinstance(model, DecisionTreeClassifier):\n            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n        else:\n            # Choose alpha blend level with respect to the number of estimators\n            # that are in use (noting that AdaBoost can use fewer estimators\n            # than its maximum if it achieves a good enough fit early on)\n            estimator_alpha = 1.0 / len(model.estimators_)\n            for tree in model.estimators_:\n                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n\n        # Build a coarser grid to plot a set of ensemble classifications\n        # to show how these are different to what we see in the decision\n        # surfaces. These points are regularly space and do not have a black outline\n        xx_coarser, yy_coarser = np.meshgrid(np.arange(x_min, x_max, plot_step_coarser),\n                                             np.arange(y_min, y_max, plot_step_coarser))\n        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(), yy_coarser.ravel()]).reshape(xx_coarser.shape)\n        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15, c=Z_points_coarser, cmap=cmap, edgecolors="none")\n\n        # Plot the training points, these are clustered together and have a\n        # black outline\n        for i, c in zip(xrange(n_classes), plot_colors):\n            idx = np.where(y == i)\n            plt.scatter(X[idx, 0], X[idx, 1], c=c, label=iris.target_names[i],\n                        cmap=cmap)\n\n        plot_idx += 1  # move on to the next plot in sequence\n\nplt.suptitle("Classifiers on feature subsets of the Iris dataset")\nplt.axis("tight")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html
Using FunctionTransformer to select columns	A							http://scikit-learn.org/stable/auto_examples/index.html			Shows how to use a function transformer in a pipeline. If you know your dataset’s first principle component is irrelevant for a classification task, you can use the FunctionTransformer to select all but the first column of the PCA transformed data.<br><br><pre><code>import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\n\ndef _generate_vector(shift=0.5, noise=15):\n    return np.arange(1000) + (np.random.rand(1000) - shift) * noise\n\n\ndef generate_dataset():\n    """\n    This dataset is two lines with a slope ~ 1, where one has\n    a y offset of ~100\n    """\n    return np.vstack((\n        np.vstack((\n            _generate_vector(),\n            _generate_vector() + 100,\n        )).T,\n        np.vstack((\n            _generate_vector(),\n            _generate_vector(),\n        )).T,\n    )), np.hstack((np.zeros(1000), np.ones(1000)))\n\n\ndef all_but_first_column(X):\n    return X[:, 1:]\n\n\ndef drop_first_component(X, y):\n    """\n    Create a pipeline with PCA and the column selector and use it to\n    transform the dataset.\n    """\n    pipeline = make_pipeline(\n        PCA(), FunctionTransformer(all_but_first_column),\n    )\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    pipeline.fit(X_train, y_train)\n    return pipeline.transform(X_test), y_test\n\n\nif __name__ == '__main__':\n    X, y = generate_dataset()\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n    plt.show()\n    X_transformed, y_transformed = drop_first_component(*generate_dataset())\n    plt.scatter(\n        X_transformed[:, 0],\n        np.zeros(len(X_transformed)),\n        c=y_transformed,\n        s=50,\n    )\n    plt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/preprocessing/plot_function_transformer.html
Gaussian Mixture Model Ellipsoids	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the confidence ellipsoids of a mixture of two Gaussians with EM and variational Dirichlet process.<br><br><pre><code>import itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\n# Number of samples per component\nn_samples = 500\n\n# Generate random sample, two components\nnp.random.seed(0)\nC = np.array([[0., -0.1], [1.7, .4]])\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n\n# Fit a mixture of Gaussians with EM using five components\ngmm = mixture.GMM(n_components=5, covariance_type='full')\ngmm.fit(X)\n\n# Fit a Dirichlet process mixture of Gaussians using five components\ndpgmm = mixture.DPGMM(n_components=5, covariance_type='full')\ndpgmm.fit(X)\n\ncolor_iter = itertools.cycle(['r', 'g', 'b', 'c', 'm'])\n\nfor i, (clf, title) in enumerate([(gmm, 'GMM'),\n                                  (dpgmm, 'Dirichlet Process GMM')]):\n    splot = plt.subplot(2, 1, 1 + i)\n    Y_ = clf.predict(X)\n    for i, (mean, covar, color) in enumerate(zip(\n            clf.means_, clf._get_covars(), color_iter)):\n        v, w = linalg.eigh(covar)\n        u = w[0] / linalg.norm(w[0])\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn't plot the redundant\n        # components.\n        if not np.any(Y_ == i):\n            continue\n        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = np.arctan(u[1] / u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(0.5)\n        splot.add_artist(ell)\n\n    plt.xlim(-10, 10)\n    plt.ylim(-3, 6)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(title)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm.html
GMM classification	A							http://scikit-learn.org/stable/auto_examples/index.html			Demonstration of Gaussian mixture models for classification.<br><br><pre><code>print(__doc__)\n\n# Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux\n# License: BSD 3 clause\n\n# $Id$\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.externals.six.moves import xrange\nfrom sklearn.mixture import GMM\n\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate('rgb'):\n        v, w = np.linalg.eigh(gmm._get_covars()[n][:2, :2])\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v *= 9\n        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n                                  180 + angle, color=color)\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n\niris = datasets.load_iris()\n\n# Break up the dataset into non-overlapping training (75%) and testing\n# (25%) sets.\nskf = StratifiedKFold(iris.target, n_folds=4)\n# Only take the first fold.\ntrain_index, test_index = next(iter(skf))\n\n\nX_train = iris.data[train_index]\ny_train = iris.target[train_index]\nX_test = iris.data[test_index]\ny_test = iris.target[test_index]\n\nn_classes = len(np.unique(y_train))\n\n# Try GMMs using different types of covariances.\nclassifiers = dict((covar_type, GMM(n_components=n_classes,\n                    covariance_type=covar_type, init_params='wc', n_iter=20))\n                   for covar_type in ['spherical', 'diag', 'tied', 'full'])\n\nn_classifiers = len(classifiers)\n\nplt.figure(figsize=(3 * n_classifiers / 2, 6))\nplt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n                    left=.01, right=.99)\n\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    # Since we have class labels for the training data, we can\n    # initialize the GMM parameters in a supervised manner.\n    classifier.means_ = np.array([X_train[y_train == i].mean(axis=0)\n                                  for i in xrange(n_classes)])\n\n    # Train the other parameters using the EM algorithm.\n    classifier.fit(X_train)\n\n    h = plt.subplot(2, n_classifiers / 2, index + 1)\n    make_ellipses(classifier, h)\n\n    for n, color in enumerate('rgb'):\n        data = iris.data[iris.target == n]\n        plt.scatter(data[:, 0], data[:, 1], 0.8, color=color,\n                    label=iris.target_names[n])\n    # Plot the test data with crosses\n    for n, color in enumerate('rgb'):\n        data = X_test[y_test == n]\n        plt.plot(data[:, 0], data[:, 1], 'x', color=color)\n\n    y_train_pred = classifier.predict(X_train)\n    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n    plt.text(0.05, 0.9, 'Train accuracy: %.1f' % train_accuracy,\n             transform=h.transAxes)\n\n    y_test_pred = classifier.predict(X_test)\n    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n    plt.text(0.05, 0.8, 'Test accuracy: %.1f' % test_accuracy,\n             transform=h.transAxes)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(name)\n\nplt.legend(loc='lower right', prop=dict(size=12))\n\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_classifier.html
Density Estimation for a mixture of Gaussians	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the density estimation of a mixture of two Gaussians. Data is generated from two Gaussians with different centers and covariance matrices.<br><br><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\nfrom sklearn import mixture\n\nn_samples = 300\n\n# generate random sample, two components\nnp.random.seed(0)\n\n# generate spherical data centered on (20, 20)\nshifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])\n\n# generate zero centered stretched Gaussian data\nC = np.array([[0., -0.7], [3.5, .7]])\nstretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)\n\n# concatenate the two datasets into the final training set\nX_train = np.vstack([shifted_gaussian, stretched_gaussian])\n\n# fit a Gaussian Mixture Model with two components\nclf = mixture.GMM(n_components=2, covariance_type='full')\nclf.fit(X_train)\n\n# display predicted scores by the model as a contour plot\nx = np.linspace(-20.0, 30.0)\ny = np.linspace(-20.0, 40.0)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)[0]\nZ = Z.reshape(X.shape)\n\nCS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n                 levels=np.logspace(0, 3, 10))\nCB = plt.colorbar(CS, shrink=0.8, extend='both')\nplt.scatter(X_train[:, 0], X_train[:, 1], .8)\n\nplt.title('Negative log-likelihood predicted by a GMM')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html
Gaussian Mixture Model Selection	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows that model selection can be performed with Gaussian Mixture Models using information-theoretic criteria (BIC). Model selection concerns both the covariance type and the number of components in the model. In that case, AIC also provides the right result (not shown to save time), but BIC is better suited if the problem is to identify the right model. Unlike Bayesian procedures,...<br><br><pre><code>print(__doc__)\n\nimport itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\n# Number of samples per component\nn_samples = 500\n\n# Generate random sample, two components\nnp.random.seed(0)\nC = np.array([[0., -0.1], [1.7, .4]])\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a mixture of Gaussians with EM\n        gmm = mixture.GMM(n_components=n_components, covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.bic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n\nbic = np.array(bic)\ncolor_iter = itertools.cycle(['k', 'r', 'g', 'b', 'c', 'm', 'y'])\nclf = best_gmm\nbars = []\n\n# Plot the BIC scores\nspl = plt.subplot(2, 1, 1)\nfor i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n    xpos = np.array(n_components_range) + .2 * (i - 2)\n    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n                                  (i + 1) * len(n_components_range)],\n                        width=.2, color=color))\nplt.xticks(n_components_range)\nplt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\nplt.title('BIC score per model')\nxpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\n    .2 * np.floor(bic.argmin() / len(n_components_range))\nplt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\nspl.set_xlabel('Number of components')\nspl.legend([b[0] for b in bars], cv_types)\n\n# Plot the winner\nsplot = plt.subplot(2, 1, 2)\nY_ = clf.predict(X)\nfor i, (mean, covar, color) in enumerate(zip(clf.means_, clf.covars_,\n                                             color_iter)):\n    v, w = linalg.eigh(covar)\n    if not np.any(Y_ == i):\n        continue\n    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n    # Plot an ellipse to show the Gaussian component\n    angle = np.arctan2(w[0][1], w[0][0])\n    angle = 180 * angle / np.pi  # convert to degrees\n    v *= 4\n    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)\n    ell.set_clip_box(splot.bbox)\n    ell.set_alpha(.5)\n    splot.add_artist(ell)\n\nplt.xlim(-10, 10)\nplt.ylim(-3, 6)\nplt.xticks(())\nplt.yticks(())\nplt.title('Selected GMM: full model, 2 components')\nplt.subplots_adjust(hspace=.35, bottom=.02)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html
Gaussian Mixture Model Sine Curve	A							http://scikit-learn.org/stable/auto_examples/index.html			This example highlights the advantages of the Dirichlet Process: complexity control and dealing with sparse data. The dataset is formed by 100 points loosely spaced following a noisy sine curve. The fit by the GMM class, using the expectation-maximization algorithm to fit a mixture of 10 Gaussian components, finds too-small components and very little structure. The fits by the Dirichlet process,...<br><br><pre><code>import itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\nfrom sklearn.externals.six.moves import xrange\n\n# Number of samples per component\nn_samples = 100\n\n# Generate random sample following a sine curve\nnp.random.seed(0)\nX = np.zeros((n_samples, 2))\nstep = 4 * np.pi / n_samples\n\nfor i in xrange(X.shape[0]):\n    x = i * step - 6\n    X[i, 0] = x + np.random.normal(0, 0.1)\n    X[i, 1] = 3 * (np.sin(x) + np.random.normal(0, .2))\n\n\ncolor_iter = itertools.cycle(['r', 'g', 'b', 'c', 'm'])\n\n\nfor i, (clf, title) in enumerate([\n        (mixture.GMM(n_components=10, covariance_type='full', n_iter=100),\n         "Expectation-maximization"),\n        (mixture.DPGMM(n_components=10, covariance_type='full', alpha=0.01,\n                       n_iter=100),\n         "Dirichlet Process,alpha=0.01"),\n        (mixture.DPGMM(n_components=10, covariance_type='diag', alpha=100.,\n                       n_iter=100),\n         "Dirichlet Process,alpha=100.")]):\n\n    clf.fit(X)\n    splot = plt.subplot(3, 1, 1 + i)\n    Y_ = clf.predict(X)\n    for i, (mean, covar, color) in enumerate(zip(\n            clf.means_, clf._get_covars(), color_iter)):\n        v, w = linalg.eigh(covar)\n        u = w[0] / linalg.norm(w[0])\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn't plot the redundant\n        # components.\n        if not np.any(Y_ == i):\n            continue\n        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = np.arctan(u[1] / u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180 + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(0.5)\n        splot.add_artist(ell)\n\n    plt.xlim(-6, 4 * np.pi - 6)\n    plt.ylim(-5, 5)\n    plt.title(title)\n    plt.xticks(())\n    plt.yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_sin.html
Gaussian Processes classification example: exploiting the probabilistic output	A							http://scikit-learn.org/stable/auto_examples/index.html			A two-dimensional regression exercise with a post-processing allowing for probabilistic classification thanks to the Gaussian property of the prediction.<br><br><pre><code>print(__doc__)\n\n# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n# Licence: BSD 3 clause\n\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.gaussian_process import GaussianProcess\nfrom matplotlib import pyplot as pl\nfrom matplotlib import cm\n\n# Standard normal distribution functions\nphi = stats.distributions.norm().pdf\nPHI = stats.distributions.norm().cdf\nPHIinv = stats.distributions.norm().ppf\n\n# A few constants\nlim = 8\n\n\ndef g(x):\n    """The function to predict (classification will then consist in predicting\n    whether g(x) <= 0 or not)"""\n    return 5. - x[:, 1] - .5 * x[:, 0] ** 2.\n\n# Design of experiments\nX = np.array([[-4.61611719, -6.00099547],\n              [4.10469096, 5.32782448],\n              [0.00000000, -0.50000000],\n              [-6.17289014, -4.6984743],\n              [1.3109306, -6.93271427],\n              [-5.03823144, 3.10584743],\n              [-2.87600388, 6.74310541],\n              [5.21301203, 4.26386883]])\n\n# Observations\ny = g(X)\n\n# Instanciate and fit Gaussian Process Model\ngp = GaussianProcess(theta0=5e-1)\n\n# Don't perform MLE or you'll get a perfect prediction for this simple example!\ngp.fit(X, y)\n\n# Evaluate real function, the prediction and its MSE on a grid\nres = 50\nx1, x2 = np.meshgrid(np.linspace(- lim, lim, res),\n                     np.linspace(- lim, lim, res))\nxx = np.vstack([x1.reshape(x1.size), x2.reshape(x2.size)]).T\n\ny_true = g(xx)\ny_pred, MSE = gp.predict(xx, eval_MSE=True)\nsigma = np.sqrt(MSE)\ny_true = y_true.reshape((res, res))\ny_pred = y_pred.reshape((res, res))\nsigma = sigma.reshape((res, res))\nk = PHIinv(.975)\n\n# Plot the probabilistic classification iso-values using the Gaussian property\n# of the prediction\nfig = pl.figure(1)\nax = fig.add_subplot(111)\nax.axes.set_aspect('equal')\npl.xticks([])\npl.yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\npl.xlabel('$x_1$')\npl.ylabel('$x_2$')\n\ncax = pl.imshow(np.flipud(PHI(- y_pred / sigma)), cmap=cm.gray_r, alpha=0.8,\n                extent=(- lim, lim, - lim, lim))\nnorm = pl.matplotlib.colors.Normalize(vmin=0., vmax=0.9)\ncb = pl.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)\ncb.set_label('${\\rm \mathbb{P}}\left[\widehat{G}(\mathbf{x}) \leq 0\\right]$')\n\npl.plot(X[y <= 0, 0], X[y <= 0, 1], 'r.', markersize=12)\n\npl.plot(X[y > 0, 0], X[y > 0, 1], 'b.', markersize=12)\n\ncs = pl.contour(x1, x2, y_true, [0.], colors='k', linestyles='dashdot')\n\ncs = pl.contour(x1, x2, PHI(- y_pred / sigma), [0.025], colors='b',\n                linestyles='solid')\npl.clabel(cs, fontsize=11)\n\ncs = pl.contour(x1, x2, PHI(- y_pred / sigma), [0.5], colors='k',\n                linestyles='dashed')\npl.clabel(cs, fontsize=11)\n\ncs = pl.contour(x1, x2, PHI(- y_pred / sigma), [0.975], colors='r',\n                linestyles='solid')\npl.clabel(cs, fontsize=11)\n\npl.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.html
Gaussian Processes regression: basic introductory example	A							http://scikit-learn.org/stable/auto_examples/index.html			A simple one-dimensional regression exercise computed in two different ways:<br><br><pre><code>print(__doc__)\n\n# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n#         Jake Vanderplas <vanderplas@astro.washington.edu>\n# Licence: BSD 3 clause\n\nimport numpy as np\nfrom sklearn.gaussian_process import GaussianProcess\nfrom matplotlib import pyplot as pl\n\nnp.random.seed(1)\n\n\ndef f(x):\n    """The function to predict."""\n    return x * np.sin(x)\n\n#----------------------------------------------------------------------\n#  First the noiseless case\nX = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T\n\n# Observations\ny = f(X).ravel()\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n\n# Instanciate a Gaussian Process model\ngp = GaussianProcess(corr='cubic', theta0=1e-2, thetaL=1e-4, thetaU=1e-1,\n                     random_start=100)\n\n# Fit to data using Maximum Likelihood Estimation of the parameters\ngp.fit(X, y)\n\n# Make the prediction on the meshed x-axis (ask for MSE as well)\ny_pred, MSE = gp.predict(x, eval_MSE=True)\nsigma = np.sqrt(MSE)\n\n# Plot the function, the prediction and the 95% confidence interval based on\n# the MSE\nfig = pl.figure()\npl.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')\npl.plot(X, y, 'r.', markersize=10, label=u'Observations')\npl.plot(x, y_pred, 'b-', label=u'Prediction')\npl.fill(np.concatenate([x, x[::-1]]),\n        np.concatenate([y_pred - 1.9600 * sigma,\n                       (y_pred + 1.9600 * sigma)[::-1]]),\n        alpha=.5, fc='b', ec='None', label='95% confidence interval')\npl.xlabel('$x$')\npl.ylabel('$f(x)$')\npl.ylim(-10, 20)\npl.legend(loc='upper left')\n\n#----------------------------------------------------------------------\n# now the noisy case\nX = np.linspace(0.1, 9.9, 20)\nX = np.atleast_2d(X).T\n\n# Observations and noise\ny = f(X).ravel()\ndy = 0.5 + 1.0 * np.random.random(y.shape)\nnoise = np.random.normal(0, dy)\ny += noise\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n\n# Instanciate a Gaussian Process model\ngp = GaussianProcess(corr='squared_exponential', theta0=1e-1,\n                     thetaL=1e-3, thetaU=1,\n                     nugget=(dy / y) ** 2,\n                     random_start=100)\n\n# Fit to data using Maximum Likelihood Estimation of the parameters\ngp.fit(X, y)\n\n# Make the prediction on the meshed x-axis (ask for MSE as well)\ny_pred, MSE = gp.predict(x, eval_MSE=True)\nsigma = np.sqrt(MSE)\n\n# Plot the function, the prediction and the 95% confidence interval based on\n# the MSE\nfig = pl.figure()\npl.plot(x, f(x), 'r:', label=u'$f(x) = x\,\sin(x)$')\npl.errorbar(X.ravel(), y, dy, fmt='r.', markersize=10, label=u'Observations')\npl.plot(x, y_pred, 'b-', label=u'Prediction')\npl.fill(np.concatenate([x, x[::-1]]),\n        np.concatenate([y_pred - 1.9600 * sigma,\n                       (y_pred + 1.9600 * sigma)[::-1]]),\n        alpha=.5, fc='b', ec='None', label='95% confidence interval')\npl.xlabel('$x$')\npl.ylabel('$f(x)$')\npl.ylim(-10, 20)\npl.legend(loc='upper left')\n\npl.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_regression.html
Gradient Boosting Out-of-Bag estimates	A							http://scikit-learn.org/stable/auto_examples/index.html			Out-of-bag (OOB) estimates can be a useful heuristic to estimate the “optimal” number of boosting iterations. OOB estimates are almost identical to cross-validation estimates but they can be computed on-the-fly without the need for repeated model fitting. OOB estimates are only available for Stochastic Gradient Boosting (i.e. subsample < 1.0), the estimates are derived from the improvement in...<br><br><pre><code>print(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn.cross_validation import KFold\nfrom sklearn.cross_validation import train_test_split\n\n\n# Generate data (adapted from G. Ridgeway's gbm example)\nn_samples = 1000\nrandom_state = np.random.RandomState(13)\nx1 = random_state.uniform(size=n_samples)\nx2 = random_state.uniform(size=n_samples)\nx3 = random_state.randint(0, 4, size=n_samples)\n\np = 1 / (1.0 + np.exp(-(np.sin(3 * x1) - 4 * x2 + x3)))\ny = random_state.binomial(1, p, size=n_samples)\n\nX = np.c_[x1, x2, x3]\n\nX = X.astype(np.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n                                                    random_state=9)\n\n# Fit classifier with out-of-bag estimates\nparams = {'n_estimators': 1200, 'max_depth': 3, 'subsample': 0.5,\n          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\nclf = ensemble.GradientBoostingClassifier(**params)\n\nclf.fit(X_train, y_train)\nacc = clf.score(X_test, y_test)\nprint("Accuracy: {:.4f}".format(acc))\n\nn_estimators = params['n_estimators']\nx = np.arange(n_estimators) + 1\n\n\ndef heldout_score(clf, X_test, y_test):\n    """compute deviance scores on ``X_test`` and ``y_test``. """\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n        score[i] = clf.loss_(y_test, y_pred)\n    return score\n\n\ndef cv_estimate(n_folds=3):\n    cv = KFold(n=X_train.shape[0], n_folds=n_folds)\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n    for train, test in cv:\n        cv_clf.fit(X_train[train], y_train[train])\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n    val_scores /= n_folds\n    return val_scores\n\n\n# Estimate best n_estimator using cross-validation\ncv_score = cv_estimate(3)\n\n# Compute best n_estimator for test data\ntest_score = heldout_score(clf, X_test, y_test)\n\n# negative cumulative sum of oob improvements\ncumsum = -np.cumsum(clf.oob_improvement_)\n\n# min loss according to OOB\noob_best_iter = x[np.argmin(cumsum)]\n\n# min loss according to test (normalize such that first loss is 0)\ntest_score -= test_score[0]\ntest_best_iter = x[np.argmin(test_score)]\n\n# min loss according to cv (normalize such that first loss is 0)\ncv_score -= cv_score[0]\ncv_best_iter = x[np.argmin(cv_score)]\n\n# color brew for the three curves\noob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\ntest_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\ncv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n\n# plot curves and vertical lines for best iterations\nplt.plot(x, cumsum, label='OOB loss', color=oob_color)\nplt.plot(x, test_score, label='Test loss', color=test_color)\nplt.plot(x, cv_score, label='CV loss', color=cv_color)\nplt.axvline(x=oob_best_iter, color=oob_color)\nplt.axvline(x=test_best_iter, color=test_color)\nplt.axvline(x=cv_best_iter, color=cv_color)\n\n# add three vertical lines to xticks\nxticks = plt.xticks()\nxticks_pos = np.array(xticks[0].tolist() +\n                      [oob_best_iter, cv_best_iter, test_best_iter])\nxticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +\n                        ['OOB', 'CV', 'Test'])\nind = np.argsort(xticks_pos)\nxticks_pos = xticks_pos[ind]\nxticks_label = xticks_label[ind]\nplt.xticks(xticks_pos, xticks_label)\n\nplt.legend(loc='upper right')\nplt.ylabel('normalized loss')\nplt.xlabel('number of iterations')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html
Prediction Intervals for Gradient Boosting Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows how quantile regression can be used to create prediction intervals.<br><br><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nnp.random.seed(1)\n\n\ndef f(x):\n    """The function to predict."""\n    return x * np.sin(x)\n\n#----------------------------------------------------------------------\n#  First the noiseless case\nX = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T\nX = X.astype(np.float32)\n\n# Observations\ny = f(X).ravel()\n\ndy = 1.5 + 1.0 * np.random.random(y.shape)\nnoise = np.random.normal(0, dy)\ny += noise\ny = y.astype(np.float32)\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nxx = np.atleast_2d(np.linspace(0, 10, 1000)).T\nxx = xx.astype(np.float32)\n\nalpha = 0.95\n\nclf = GradientBoostingRegressor(loss='quantile', alpha=alpha,\n                                n_estimators=250, max_depth=3,\n                                learning_rate=.1, min_samples_leaf=9,\n                                min_samples_split=9)\n\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_upper = clf.predict(xx)\n\nclf.set_params(alpha=1.0 - alpha)\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_lower = clf.predict(xx)\n\nclf.set_params(loss='ls')\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_pred = clf.predict(xx)\n\n# Plot the function, the prediction and the 90% confidence interval based on\n# the MSE\nfig = plt.figure()\nplt.plot(xx, f(xx), 'g:', label=u'$f(x) = x\,\sin(x)$')\nplt.plot(X, y, 'b.', markersize=10, label=u'Observations')\nplt.plot(xx, y_pred, 'r-', label=u'Prediction')\nplt.plot(xx, y_upper, 'k-')\nplt.plot(xx, y_lower, 'k-')\nplt.fill(np.concatenate([xx, xx[::-1]]),\n         np.concatenate([y_upper, y_lower[::-1]]),\n         alpha=.5, fc='b', ec='None', label='90% prediction interval')\nplt.xlabel('$x$')\nplt.ylabel('$f(x)$')\nplt.ylim(-10, 20)\nplt.legend(loc='upper left')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html
Gradient Boosting regression	A							http://scikit-learn.org/stable/auto_examples/index.html			Demonstrate Gradient Boosting on the Boston housing dataset.<br><br><pre><code>print(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\n\n###############################################################################\n# Load data\nboston = datasets.load_boston()\nX, y = shuffle(boston.data, boston.target, random_state=13)\nX = X.astype(np.float32)\noffset = int(X.shape[0] * 0.9)\nX_train, y_train = X[:offset], y[:offset]\nX_test, y_test = X[offset:], y[offset:]\n\n###############################################################################\n# Fit regression model\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 1,\n          'learning_rate': 0.01, 'loss': 'ls'}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_train, y_train)\nmse = mean_squared_error(y_test, clf.predict(X_test))\nprint("MSE: %.4f" % mse)\n\n###############################################################################\n# Plot training deviance\n\n# compute test set deviance\ntest_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\n    test_score[i] = clf.loss_(y_test, y_pred)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title('Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n         label='Training Set Deviance')\nplt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n         label='Test Set Deviance')\nplt.legend(loc='upper right')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Deviance')\n\n###############################################################################\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, boston.feature_names[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html
Gradient Boosting regularization	A							http://scikit-learn.org/stable/auto_examples/index.html			Illustration of the effect of different regularization strategies for Gradient Boosting. The example is taken from Hastie et al 2009.<br><br><pre><code>print(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\n\n\nX, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\nX = X.astype(np.float32)\n\n# map labels from {-1, 1} to {0, 1}\nlabels, y = np.unique(y, return_inverse=True)\n\nX_train, X_test = X[:2000], X[2000:]\ny_train, y_test = y[:2000], y[2000:]\n\noriginal_params = {'n_estimators': 1000, 'max_leaf_nodes': 4, 'max_depth': None, 'random_state': 2,\n                   'min_samples_split': 5}\n\nplt.figure()\n\nfor label, color, setting in [('No shrinkage', 'orange',\n                               {'learning_rate': 1.0, 'subsample': 1.0}),\n                              ('learning_rate=0.1', 'turquoise',\n                               {'learning_rate': 0.1, 'subsample': 1.0}),\n                              ('subsample=0.5', 'blue',\n                               {'learning_rate': 1.0, 'subsample': 0.5}),\n                              ('learning_rate=0.1, subsample=0.5', 'gray',\n                               {'learning_rate': 0.1, 'subsample': 0.5}),\n                              ('learning_rate=0.1, max_features=2', 'magenta',\n                               {'learning_rate': 0.1, 'max_features': 2})]:\n    params = dict(original_params)\n    params.update(setting)\n\n    clf = ensemble.GradientBoostingClassifier(**params)\n    clf.fit(X_train, y_train)\n\n    # compute test set deviance\n    test_deviance = np.zeros((params['n_estimators'],), dtype=np.float64)\n\n    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n        # clf.loss_ assumes that y_test[i] in {0, 1}\n        test_deviance[i] = clf.loss_(y_test, y_pred)\n\n    plt.plot((np.arange(test_deviance.shape[0]) + 1)[::5], test_deviance[::5],\n            '-', color=color, label=label)\n\nplt.legend(loc='upper left')\nplt.xlabel('Boosting Iterations')\nplt.ylabel('Test Set Deviance')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html
Blind source separation using FastICA	A							http://scikit-learn.org/stable/auto_examples/index.html			An example of estimating sources from noisy data.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nfrom sklearn.decomposition import FastICA, PCA\n\n###############################################################################\n# Generate sample data\nnp.random.seed(0)\nn_samples = 2000\ntime = np.linspace(0, 8, n_samples)\n\ns1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\ns2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\ns3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n\nS = np.c_[s1, s2, s3]\nS += 0.2 * np.random.normal(size=S.shape)  # Add noise\n\nS /= S.std(axis=0)  # Standardize data\n# Mix data\nA = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\nX = np.dot(S, A.T)  # Generate observations\n\n# Compute ICA\nica = FastICA(n_components=3)\nS_ = ica.fit_transform(X)  # Reconstruct signals\nA_ = ica.mixing_  # Get estimated mixing matrix\n\n# We can `prove` that the ICA model applies by reverting the unmixing.\nassert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)\n\n# For comparison, compute PCA\npca = PCA(n_components=3)\nH = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components\n\n###############################################################################\n# Plot results\n\nplt.figure()\n\nmodels = [X, S, S_, H]\nnames = ['Observations (mixed signal)',\n         'True Sources',\n         'ICA recovered signals', \n         'PCA recovered signals']\ncolors = ['red', 'steelblue', 'orange']\n\nfor ii, (model, name) in enumerate(zip(models, names), 1):\n    plt.subplot(4, 1, ii)\n    plt.title(name)\n    for sig, color in zip(model.T, colors):\n        plt.plot(sig, color=color)\n\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_blind_source_separation.html
FastICA on 2D point clouds	A							http://scikit-learn.org/stable/auto_examples/index.html			This example illustrates visually in the feature space a comparison by results using two different component analysis techniques.<br><br><pre><code>print(__doc__)\n\n# Authors: Alexandre Gramfort, Gael Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA, FastICA\n\n###############################################################################\n# Generate sample data\nrng = np.random.RandomState(42)\nS = rng.standard_t(1.5, size=(20000, 2))\nS[:, 0] *= 2.\n\n# Mix data\nA = np.array([[1, 1], [0, 2]])  # Mixing matrix\n\nX = np.dot(S, A.T)  # Generate observations\n\npca = PCA()\nS_pca_ = pca.fit(X).transform(X)\n\nica = FastICA(random_state=rng)\nS_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n\nS_ica_ /= S_ica_.std(axis=0)\n\n\n###############################################################################\n# Plot results\n\ndef plot_samples(S, axis_list=None):\n    plt.scatter(S[:, 0], S[:, 1], s=2, marker='o', zorder=10,\n                color='steelblue', alpha=0.5)\n    if axis_list is not None:\n        colors = ['orange', 'red']\n        for color, axis in zip(colors, axis_list):\n            axis /= axis.std()\n            x_axis, y_axis = axis\n            # Trick to get legend to work\n            plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)\n            plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6,\n                       color=color)\n\n    plt.hlines(0, -3, 3)\n    plt.vlines(0, -3, 3)\n    plt.xlim(-3, 3)\n    plt.ylim(-3, 3)\n    plt.xlabel('x')\n    plt.ylabel('y')\n\nplt.figure()\nplt.subplot(2, 2, 1)\nplot_samples(S / S.std())\nplt.title('True Independent Sources')\n\naxis_list = [pca.components_.T, ica.mixing_]\nplt.subplot(2, 2, 2)\nplot_samples(X / np.std(X), axis_list=axis_list)\nlegend = plt.legend(['PCA', 'ICA'], loc='upper right')\nlegend.set_zorder(100)\n\nplt.title('Observations')\n\nplt.subplot(2, 2, 3)\nplot_samples(S_pca_ / np.std(S_pca_, axis=0))\nplt.title('PCA recovered signals')\n\nplt.subplot(2, 2, 4)\nplot_samples(S_ica_ / np.std(S_ica_))\nplt.title('ICA recovered signals')\n\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html
Image denoising using dictionary learning	A							http://scikit-learn.org/stable/auto_examples/index.html			An example comparing the effect of reconstructing noisy fragments of a raccoon face image using firstly online Dictionary Learning and various transform methods.<br><br><pre><code>print(__doc__)\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\n\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\nfrom sklearn.feature_extraction.image import extract_patches_2d\nfrom sklearn.feature_extraction.image import reconstruct_from_patches_2d\nfrom sklearn.utils.testing import SkipTest\nfrom sklearn.utils.fixes import sp_version\n\nif sp_version < (0, 12):\n    raise SkipTest("Skipping because SciPy version earlier than 0.12.0 and "\n                   "thus does not include the scipy.misc.face() image.")\n\n###############################################################################\ntry:\n    from scipy import misc\n    face = misc.face(gray=True)\nexcept AttributeError:\n    # Old versions of scipy have face in the top level package\n    face = sp.face(gray=True)\n\n# Convert from uint8 representation with values between 0 and 255 to\n# a floating point representation with values between 0 and 1.\nface = face / 255\n\n# downsample for higher speed\nface = face[::2, ::2] + face[1::2, ::2] + face[::2, 1::2] + face[1::2, 1::2]\nface /= 4.0\nheight, width = face.shape\n\n# Distort the right half of the image\nprint('Distorting image...')\ndistorted = face.copy()\ndistorted[:, width // 2:] += 0.075 * np.random.randn(height, width // 2)\n\n# Extract all reference patches from the left half of the image\nprint('Extracting reference patches...')\nt0 = time()\npatch_size = (7, 7)\ndata = extract_patches_2d(distorted[:, :width // 2], patch_size)\ndata = data.reshape(data.shape[0], -1)\ndata -= np.mean(data, axis=0)\ndata /= np.std(data, axis=0)\nprint('done in %.2fs.' % (time() - t0))\n\n###############################################################################\n# Learn the dictionary from reference patches\n\nprint('Learning the dictionary...')\nt0 = time()\ndico = MiniBatchDictionaryLearning(n_components=100, alpha=1, n_iter=500)\nV = dico.fit(data).components_\ndt = time() - t0\nprint('done in %.2fs.' % dt)\n\nplt.figure(figsize=(4.2, 4))\nfor i, comp in enumerate(V[:100]):\n    plt.subplot(10, 10, i + 1)\n    plt.imshow(comp.reshape(patch_size), cmap=plt.cm.gray_r,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\nplt.suptitle('Dictionary learned from face patches\n' +\n             'Train time %.1fs on %d patches' % (dt, len(data)),\n             fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n\n###############################################################################\n# Display the distorted image\n\ndef show_with_diff(image, reference, title):\n    """Helper function to display denoising"""\n    plt.figure(figsize=(5, 3.3))\n    plt.subplot(1, 2, 1)\n    plt.title('Image')\n    plt.imshow(image, vmin=0, vmax=1, cmap=plt.cm.gray,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\n    plt.subplot(1, 2, 2)\n    difference = image - reference\n\n    plt.title('Difference (norm: %.2f)' % np.sqrt(np.sum(difference ** 2)))\n    plt.imshow(difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\n    plt.suptitle(title, size=16)\n    plt.subplots_adjust(0.02, 0.02, 0.98, 0.79, 0.02, 0.2)\n\nshow_with_diff(distorted, face, 'Distorted image')\n\n###############################################################################\n# Extract noisy patches and reconstruct them using the dictionary\n\nprint('Extracting noisy patches... ')\nt0 = time()\ndata = extract_patches_2d(distorted[:, width // 2:], patch_size)\ndata = data.reshape(data.shape[0], -1)\nintercept = np.mean(data, axis=0)\ndata -= intercept\nprint('done in %.2fs.' % (time() - t0))\n\ntransform_algorithms = [\n    ('Orthogonal Matching Pursuit\n1 atom', 'omp',\n     {'transform_n_nonzero_coefs': 1}),\n    ('Orthogonal Matching Pursuit\n2 atoms', 'omp',\n     {'transform_n_nonzero_coefs': 2}),\n    ('Least-angle regression\n5 atoms', 'lars',\n     {'transform_n_nonzero_coefs': 5}),\n    ('Thresholding\n alpha=0.1', 'threshold', {'transform_alpha': .1})]\n\nreconstructions = {}\nfor title, transform_algorithm, kwargs in transform_algorithms:\n    print(title + '...')\n    reconstructions[title] = face.copy()\n    t0 = time()\n    dico.set_params(transform_algorithm=transform_algorithm, **kwargs)\n    code = dico.transform(data)\n    patches = np.dot(code, V)\n\n    if transform_algorithm == 'threshold':\n        patches -= patches.min()\n        patches /= patches.max()\n\n    patches += intercept\n    patches = patches.reshape(len(data), *patch_size)\n    if transform_algorithm == 'threshold':\n        patches -= patches.min()\n        patches /= patches.max()\n    reconstructions[title][:, width // 2:] = reconstruct_from_patches_2d(\n        patches, (height, width // 2))\n    dt = time() - t0\n    print('done in %.2fs.' % dt)\n    show_with_diff(reconstructions[title], face,\n                   title + ' (time: %.1fs)' % dt)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_image_denoising.html
Incremental PCA	A							http://scikit-learn.org/stable/auto_examples/index.html			Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch...<br><br><pre><code>print(__doc__)\n\n# Authors: Kyle Kastner\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA, IncrementalPCA\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nn_components = 2\nipca = IncrementalPCA(n_components=n_components, batch_size=10)\nX_ipca = ipca.fit_transform(X)\n\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X)\n\nfor X_transformed, title in [(X_ipca, "Incremental PCA"), (X_pca, "PCA")]:\n    plt.figure(figsize=(8, 8))\n    for c, i, target_name in zip("rgb", [0, 1, 2], iris.target_names):\n        plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],\n                    c=c, label=target_name)\n\n    if "Incremental" in title:\n        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\n        plt.title(title + " of iris dataset\nMean absolute unsigned error "\n                  "%.6f" % err)\n    else:\n        plt.title(title + " of iris dataset")\n    plt.legend(loc="best")\n    plt.axis([-4, 4, -1.5, 1.5])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_incremental_pca.html
Plot different SVM classifiers in the iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			Comparison of different linear SVM classifiers on a 2D projection of the iris dataset. We only consider the first 2 features of this dataset:<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nsvc = svm.SVC(kernel='linear', C=C).fit(X, y)\nrbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C).fit(X, y)\npoly_svc = svm.SVC(kernel='poly', degree=3, C=C).fit(X, y)\nlin_svc = svm.LinearSVC(C=C).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['SVC with linear kernel',\n          'LinearSVC (linear kernel)',\n          'SVC with RBF kernel',\n          'SVC with polynomial (degree 3) kernel']\n\n\nfor i, clf in enumerate((svc, lin_svc, rbf_svc, poly_svc)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n    plt.xlabel('Sepal length')\n    plt.ylabel('Sepal width')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(titles[i])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html
Plot the decision surface of a decision tree on the iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the decision surface of a decision tree trained on pairs of features of the iris dataset.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Parameters\nn_classes = 3\nplot_colors = "bry"\nplot_step = 0.02\n\n# Load data\niris = load_iris()\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n                                [1, 2], [1, 3], [2, 3]]):\n    # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # Shuffle\n    idx = np.arange(X.shape[0])\n    np.random.seed(13)\n    np.random.shuffle(idx)\n    X = X[idx]\n    y = y[idx]\n\n    # Standardize\n    mean = X.mean(axis=0)\n    std = X.std(axis=0)\n    X = (X - mean) / std\n\n    # Train\n    clf = DecisionTreeClassifier().fit(X, y)\n\n    # Plot the decision boundary\n    plt.subplot(2, 3, pairidx + 1)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n\n    plt.xlabel(iris.feature_names[pair[0]])\n    plt.ylabel(iris.feature_names[pair[1]])\n    plt.axis("tight")\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                    cmap=plt.cm.Paired)\n\n    plt.axis("tight")\n\nplt.suptitle("Decision surface of a decision tree using paired features")\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html
The Iris Dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nplt.figure(2, figsize=(8, 6))\nplt.clf()\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\n\n# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(iris.data)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=Y,\n           cmap=plt.cm.Paired)\nax.set_title("First three PCA directions")\nax.set_xlabel("1st eigenvector")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel("2nd eigenvector")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel("3rd eigenvector")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html
SVM Exercise	A							http://scikit-learn.org/stable/auto_examples/index.html			A tutorial exercise for using different SVM kernels.<br><br><pre><code>print(__doc__)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, svm\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX = X[y != 0, :2]\ny = y[y != 0]\n\nn_sample = len(X)\n\nnp.random.seed(0)\norder = np.random.permutation(n_sample)\nX = X[order]\ny = y[order].astype(np.float)\n\nX_train = X[:.9 * n_sample]\ny_train = y[:.9 * n_sample]\nX_test = X[.9 * n_sample:]\ny_test = y[.9 * n_sample:]\n\n# fit the model\nfor fig_num, kernel in enumerate(('linear', 'rbf', 'poly')):\n    clf = svm.SVC(kernel=kernel, gamma=10)\n    clf.fit(X_train, y_train)\n\n    plt.figure(fig_num)\n    plt.clf()\n    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired)\n\n    # Circle out the test data\n    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors='none', zorder=10)\n\n    plt.axis('tight')\n    x_min = X[:, 0].min()\n    x_max = X[:, 0].max()\n    y_min = X[:, 1].min()\n    y_max = X[:, 1].max()\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\n    plt.title(kernel)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/exercises/plot_iris_exercise.html
Logistic Regression 3-class Classifier	A							http://scikit-learn.org/stable/auto_examples/index.html			Show below is a logistic-regression classifiers decision boundaries on the iris dataset. The datapoints are colored according to their labels.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model, datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\nh = .02  # step size in the mesh\n\nlogreg = linear_model.LogisticRegression(C=1e5)\n\n# we create an instance of Neighbours Classifier and fit the data.\nlogreg.fit(X, Y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html
Isotonic Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			An illustration of the isotonic regression on generated data. The isotonic regression finds a non-decreasing approximation of a function while minimizing the mean squared error on the training data. The benefit of such a model is that it does not assume any form for the target function such as linearity. For comparison a linear regression is also presented.<br><br><pre><code>print(__doc__)\n\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\nn = 100\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))\n\n###############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\nir = IsotonicRegression()\n\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n\n###############################################################################\n# plot result\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(0.5 * np.ones(n))\n\nfig = plt.figure()\nplt.plot(x, y, 'r.', markersize=12)\nplt.plot(x, y_, 'g.-', markersize=12)\nplt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\nplt.gca().add_collection(lc)\nplt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\nplt.title('Isotonic regression')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html
The Johnson-Lindenstrauss bound for embedding with random projections	A							http://scikit-learn.org/stable/auto_examples/index.html			The Johnson-Lindenstrauss lemma states that any high dimensional dataset can be randomly projected into a lower dimensional Euclidean space while controlling the distortion in the pairwise distances.<br><br><pre><code>print(__doc__)\n\nimport sys\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.random_projection import johnson_lindenstrauss_min_dim\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n# Part 1: plot the theoretical dependency between n_components_min and\n# n_samples\n\n# range of admissible distortions\neps_range = np.linspace(0.1, 0.99, 5)\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = np.logspace(1, 9, 9)\n\nplt.figure()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)\n    plt.loglog(n_samples_range, min_n_components, color=color)\n\nplt.legend(["eps = %0.1f" % eps for eps in eps_range], loc="lower right")\nplt.xlabel("Number of observations to eps-embed")\nplt.ylabel("Minimum number of dimensions")\nplt.title("Johnson-Lindenstrauss bounds:\nn_samples vs n_components")\n\n# range of admissible distortions\neps_range = np.linspace(0.01, 0.99, 100)\n\n# range of number of samples (observation) to embed\nn_samples_range = np.logspace(2, 6, 5)\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))\n\nplt.figure()\nfor n_samples, color in zip(n_samples_range, colors):\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)\n    plt.semilogy(eps_range, min_n_components, color=color)\n\nplt.legend(["n_samples = %d" % n for n in n_samples_range], loc="upper right")\nplt.xlabel("Distortion eps")\nplt.ylabel("Minimum number of dimensions")\nplt.title("Johnson-Lindenstrauss bounds:\nn_components vs eps")\n\n# Part 2: perform sparse random projection of some digits images which are\n# quite low dimensional and dense or documents of the 20 newsgroups dataset\n# which is both high dimensional and sparse\n\nif '--twenty-newsgroups' in sys.argv:\n    # Need an internet connection hence not enabled by default\n    data = fetch_20newsgroups_vectorized().data[:500]\nelse:\n    data = load_digits().data[:500]\n\nn_samples, n_features = data.shape\nprint("Embedding %d samples with dim %d using various random projections"\n      % (n_samples, n_features))\n\nn_components_range = np.array([300, 1000, 10000])\ndists = euclidean_distances(data, squared=True).ravel()\n\n# select only non-identical samples pairs\nnonzero = dists != 0\ndists = dists[nonzero]\n\nfor n_components in n_components_range:\n    t0 = time()\n    rp = SparseRandomProjection(n_components=n_components)\n    projected_data = rp.fit_transform(data)\n    print("Projected %d samples from %d to %d in %0.3fs"\n          % (n_samples, n_features, n_components, time() - t0))\n    if hasattr(rp, 'components_'):\n        n_bytes = rp.components_.data.nbytes\n        n_bytes += rp.components_.indices.nbytes\n        print("Random matrix with size: %0.3fMB" % (n_bytes / 1e6))\n\n    projected_dists = euclidean_distances(\n        projected_data, squared=True).ravel()[nonzero]\n\n    plt.figure()\n    plt.hexbin(dists, projected_dists, gridsize=100, cmap=plt.cm.PuBu)\n    plt.xlabel("Pairwise squared distances in original space")\n    plt.ylabel("Pairwise squared distances in projected space")\n    plt.title("Pairwise distances distribution for n_components=%d" %\n              n_components)\n    cb = plt.colorbar()\n    cb.set_label('Sample pairs counts')\n\n    rates = projected_dists / dists\n    print("Mean distances rate: %0.2f (%0.2f)"\n          % (np.mean(rates), np.std(rates)))\n\n    plt.figure()\n    plt.hist(rates, bins=50, normed=True, range=(0., 2.))\n    plt.xlabel("Squared distances rate: projected / original")\n    plt.ylabel("Distribution of samples pairs")\n    plt.title("Histogram of pairwise distance rates for n_components=%d" %\n              n_components)\n\n    # TODO: compute the expected value of eps and add them to the previous plot\n    # as vertical lines / region\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_johnson_lindenstrauss_bound.html
Simple 1D Kernel Density Estimation	A							http://scikit-learn.org/stable/auto_examples/index.html			This example uses the sklearn.neighbors.KernelDensity class to demonstrate the principles of Kernel Density Estimation in one dimension.<br><br><pre><code># Author: Jake Vanderplas <jakevdp@cs.washington.edu>\n#\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.neighbors import KernelDensity\n\n\n#----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\nnp.random.seed(1)\nN = 20\nX = np.concatenate((np.random.normal(0, 1, 0.3 * N),\n                    np.random.normal(5, 1, 0.7 * N)))[:, np.newaxis]\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\nbins = np.linspace(-5, 10, 10)\n\nfig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n# histogram 1\nax[0, 0].hist(X[:, 0], bins=bins, fc='#AAAAFF', normed=True)\nax[0, 0].text(-3.5, 0.31, "Histogram")\n\n# histogram 2\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc='#AAAAFF', normed=True)\nax[0, 1].text(-3.5, 0.31, "Histogram, bins shifted")\n\n# tophat KDE\nkde = KernelDensity(kernel='tophat', bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')\nax[1, 0].text(-3.5, 0.31, "Tophat Kernel Density")\n\n# Gaussian KDE\nkde = KernelDensity(kernel='gaussian', bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc='#AAAAFF')\nax[1, 1].text(-3.5, 0.31, "Gaussian Kernel Density")\n\nfor axi in ax.ravel():\n    axi.plot(X[:, 0], np.zeros(X.shape[0]) - 0.01, '+k')\n    axi.set_xlim(-4, 9)\n    axi.set_ylim(-0.02, 0.34)\n\nfor axi in ax[:, 0]:\n    axi.set_ylabel('Normalized Density')\n\nfor axi in ax[1, :]:\n    axi.set_xlabel('x')\n\n#----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = np.linspace(-6, 6, 1000)[:, None]\nX_src = np.zeros((1, 1))\n\nfig, ax = plt.subplots(2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return '0'\n    elif x == 1:\n        return 'h'\n    elif x == -1:\n        return '-h'\n    else:\n        return '%ih' % x\n\nfor i, kernel in enumerate(['gaussian', 'tophat', 'epanechnikov',\n                            'exponential', 'linear', 'cosine']):\n    axi = ax.ravel()[i]\n    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], np.exp(log_dens), '-k', fc='#AAAAFF')\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title('Available Kernels')\n\n#----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\nnp.random.seed(1)\nX = np.concatenate((np.random.normal(0, 1, 0.3 * N),\n                    np.random.normal(5, 1, 0.7 * N)))[:, np.newaxis]\n\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n\ntrue_dens = (0.3 * norm(0, 1).pdf(X_plot[:, 0])\n             + 0.7 * norm(5, 1).pdf(X_plot[:, 0]))\n\nfig, ax = plt.subplots()\nax.fill(X_plot[:, 0], true_dens, fc='black', alpha=0.2,\n        label='input distribution')\n\nfor kernel in ['gaussian', 'tophat', 'epanechnikov']:\n    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(X_plot[:, 0], np.exp(log_dens), '-',\n            label="kernel = '{0}'".format(kernel))\n\nax.text(6, 0.38, "N={0} points".format(N))\n\nax.legend(loc='upper left')\nax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), '+k')\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_kde_1d.html
Explicit feature map approximation for RBF kernels	A							http://scikit-learn.org/stable/auto_examples/index.html			An example illustrating the approximation of the feature map of an RBF kernel.<br><br><pre><code>print(__doc__)\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n#         Andreas Mueller <amueller@ais.uni-bonn.de>\n# License: BSD 3 clause\n\n# Standard scientific Python imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom time import time\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, pipeline\nfrom sklearn.kernel_approximation import (RBFSampler,\n                                          Nystroem)\nfrom sklearn.decomposition import PCA\n\n# The digits dataset\ndigits = datasets.load_digits(n_class=9)\n\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.data)\ndata = digits.data / 16.\ndata -= data.mean(axis=0)\n\n# We learn the digits on the first half of the digits\ndata_train, targets_train = data[:n_samples / 2], digits.target[:n_samples / 2]\n\n\n# Now predict the value of the digit on the second half:\ndata_test, targets_test = data[n_samples / 2:], digits.target[n_samples / 2:]\n#data_test = scaler.transform(data_test)\n\n# Create a classifier: a support vector classifier\nkernel_svm = svm.SVC(gamma=.2)\nlinear_svm = svm.LinearSVC()\n\n# create pipeline from kernel approximation\n# and linear svm\nfeature_map_fourier = RBFSampler(gamma=.2, random_state=1)\nfeature_map_nystroem = Nystroem(gamma=.2, random_state=1)\nfourier_approx_svm = pipeline.Pipeline([("feature_map", feature_map_fourier),\n                                        ("svm", svm.LinearSVC())])\n\nnystroem_approx_svm = pipeline.Pipeline([("feature_map", feature_map_nystroem),\n                                        ("svm", svm.LinearSVC())])\n\n# fit and predict using linear and kernel svm:\n\nkernel_svm_time = time()\nkernel_svm.fit(data_train, targets_train)\nkernel_svm_score = kernel_svm.score(data_test, targets_test)\nkernel_svm_time = time() - kernel_svm_time\n\nlinear_svm_time = time()\nlinear_svm.fit(data_train, targets_train)\nlinear_svm_score = linear_svm.score(data_test, targets_test)\nlinear_svm_time = time() - linear_svm_time\n\nsample_sizes = 30 * np.arange(1, 10)\nfourier_scores = []\nnystroem_scores = []\nfourier_times = []\nnystroem_times = []\n\nfor D in sample_sizes:\n    fourier_approx_svm.set_params(feature_map__n_components=D)\n    nystroem_approx_svm.set_params(feature_map__n_components=D)\n    start = time()\n    nystroem_approx_svm.fit(data_train, targets_train)\n    nystroem_times.append(time() - start)\n\n    start = time()\n    fourier_approx_svm.fit(data_train, targets_train)\n    fourier_times.append(time() - start)\n\n    fourier_score = fourier_approx_svm.score(data_test, targets_test)\n    nystroem_score = nystroem_approx_svm.score(data_test, targets_test)\n    nystroem_scores.append(nystroem_score)\n    fourier_scores.append(fourier_score)\n\n# plot the results:\nplt.figure(figsize=(8, 8))\naccuracy = plt.subplot(211)\n# second y axis for timeings\ntimescale = plt.subplot(212)\n\naccuracy.plot(sample_sizes, nystroem_scores, label="Nystroem approx. kernel")\ntimescale.plot(sample_sizes, nystroem_times, '--',\n               label='Nystroem approx. kernel')\n\naccuracy.plot(sample_sizes, fourier_scores, label="Fourier approx. kernel")\ntimescale.plot(sample_sizes, fourier_times, '--',\n               label='Fourier approx. kernel')\n\n# horizontal lines for exact rbf and linear kernels:\naccuracy.plot([sample_sizes[0], sample_sizes[-1]],\n              [linear_svm_score, linear_svm_score], label="linear svm")\ntimescale.plot([sample_sizes[0], sample_sizes[-1]],\n               [linear_svm_time, linear_svm_time], '--', label='linear svm')\n\naccuracy.plot([sample_sizes[0], sample_sizes[-1]],\n              [kernel_svm_score, kernel_svm_score], label="rbf svm")\ntimescale.plot([sample_sizes[0], sample_sizes[-1]],\n               [kernel_svm_time, kernel_svm_time], '--', label='rbf svm')\n\n# vertical line for dataset dimensionality = 64\naccuracy.plot([64, 64], [0.7, 1], label="n_features")\n\n# legends and labels\naccuracy.set_title("Classification accuracy")\ntimescale.set_title("Training times")\naccuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\naccuracy.set_xticks(())\naccuracy.set_ylim(np.min(fourier_scores), 1)\ntimescale.set_xlabel("Sampling steps = transformed feature dimension")\naccuracy.set_ylabel("Classification accuracy")\ntimescale.set_ylabel("Training time in seconds")\naccuracy.legend(loc='best')\ntimescale.legend(loc='best')\n\n# visualize the decision surface, projected down to the first\n# two principal components of the dataset\npca = PCA(n_components=8).fit(data_train)\n\nX = pca.transform(data_train)\n\n# Gemerate grid along first two principal components\nmultiples = np.arange(-2, 2, 0.1)\n# steps along first component\nfirst = multiples[:, np.newaxis] * pca.components_[0, :]\n# steps along second component\nsecond = multiples[:, np.newaxis] * pca.components_[1, :]\n# combine\ngrid = first[np.newaxis, :, :] + second[:, np.newaxis, :]\nflat_grid = grid.reshape(-1, data.shape[1])\n\n# title for the plots\ntitles = ['SVC with rbf kernel',\n          'SVC (linear kernel)\n with Fourier rbf feature map\n'\n          'n_components=100',\n          'SVC (linear kernel)\n with Nystroem rbf feature map\n'\n          'n_components=100']\n\nplt.tight_layout()\nplt.figure(figsize=(12, 5))\n\n# predict and plot\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm,\n                         fourier_approx_svm)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(1, 3, i + 1)\n    Z = clf.predict(flat_grid)\n\n    # Put the result into a color plot\n    Z = Z.reshape(grid.shape[:-1])\n    plt.contourf(multiples, multiples, Z, cmap=plt.cm.Paired)\n    plt.axis('off')\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=targets_train, cmap=plt.cm.Paired)\n\n    plt.title(titles[i])\nplt.tight_layout()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_kernel_approximation.html
Kernel PCA	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows that Kernel PCA is able to find a projection of the data that makes data linearly separable.<br><br><pre><code>print(__doc__)\n\n# Authors: Mathieu Blondel\n#          Andreas Mueller\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(0)\n\nX, y = make_circles(n_samples=400, factor=.3, noise=.05)\n\nkpca = KernelPCA(kernel="rbf", fit_inverse_transform=True, gamma=10)\nX_kpca = kpca.fit_transform(X)\nX_back = kpca.inverse_transform(X_kpca)\npca = PCA()\nX_pca = pca.fit_transform(X)\n\n# Plot results\n\nplt.figure()\nplt.subplot(2, 2, 1, aspect='equal')\nplt.title("Original space")\nreds = y == 0\nblues = y == 1\n\nplt.plot(X[reds, 0], X[reds, 1], "ro")\nplt.plot(X[blues, 0], X[blues, 1], "bo")\nplt.xlabel("$x_1$")\nplt.ylabel("$x_2$")\n\nX1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\nX_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n# projection on the first principal component (in the phi space)\nZ_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)\nplt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n\nplt.subplot(2, 2, 2, aspect='equal')\nplt.plot(X_pca[reds, 0], X_pca[reds, 1], "ro")\nplt.plot(X_pca[blues, 0], X_pca[blues, 1], "bo")\nplt.title("Projection by PCA")\nplt.xlabel("1st principal component")\nplt.ylabel("2nd component")\n\nplt.subplot(2, 2, 3, aspect='equal')\nplt.plot(X_kpca[reds, 0], X_kpca[reds, 1], "ro")\nplt.plot(X_kpca[blues, 0], X_kpca[blues, 1], "bo")\nplt.title("Projection by KPCA")\nplt.xlabel("1st principal component in space induced by $\phi$")\nplt.ylabel("2nd component")\n\nplt.subplot(2, 2, 4, aspect='equal')\nplt.plot(X_back[reds, 0], X_back[reds, 1], "ro")\nplt.plot(X_back[blues, 0], X_back[blues, 1], "bo")\nplt.title("Original space after inverse transform")\nplt.xlabel("$x_1$")\nplt.ylabel("$x_2$")\n\nplt.subplots_adjust(0.02, 0.10, 0.98, 0.94, 0.04, 0.35)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html
Comparison of kernel ridge regression and SVR	A							http://scikit-learn.org/stable/auto_examples/index.html			Both kernel ridge regression (KRR) and SVR learn a non-linear function by employing the kernel trick, i.e., they learn a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. They differ in the loss functions (ridge versus epsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in closed-form and is...<br><br><pre><code># Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\n\nfrom __future__ import division\nimport time\n\nimport numpy as np\n\nfrom sklearn.svm import SVR\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.learning_curve import learning_curve\nfrom sklearn.kernel_ridge import KernelRidge\nimport matplotlib.pyplot as plt\n\nrng = np.random.RandomState(0)\n\n#############################################################################\n# Generate sample data\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5))\n\nX_plot = np.linspace(0, 5, 100000)[:, None]\n\n#############################################################################\n# Fit regression model\ntrain_size = 100\nsvr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={"C": [1e0, 1e1, 1e2, 1e3],\n                               "gamma": np.logspace(-2, 2, 5)})\n\nkr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), cv=5,\n                  param_grid={"alpha": [1e0, 0.1, 1e-2, 1e-3],\n                              "gamma": np.logspace(-2, 2, 5)})\n\nt0 = time.time()\nsvr.fit(X[:train_size], y[:train_size])\nsvr_fit = time.time() - t0\nprint("SVR complexity and bandwidth selected and model fitted in %.3f s"\n      % svr_fit)\n\nt0 = time.time()\nkr.fit(X[:train_size], y[:train_size])\nkr_fit = time.time() - t0\nprint("KRR complexity and bandwidth selected and model fitted in %.3f s"\n      % kr_fit)\n\nsv_ratio = svr.best_estimator_.support_.shape[0] / train_size\nprint("Support vector ratio: %.3f" % sv_ratio)\n\nt0 = time.time()\ny_svr = svr.predict(X_plot)\nsvr_predict = time.time() - t0\nprint("SVR prediction for %d inputs in %.3f s"\n      % (X_plot.shape[0], svr_predict))\n\nt0 = time.time()\ny_kr = kr.predict(X_plot)\nkr_predict = time.time() - t0\nprint("KRR prediction for %d inputs in %.3f s"\n      % (X_plot.shape[0], kr_predict))\n\n\n#############################################################################\n# look at the results\nsv_ind = svr.best_estimator_.support_\nplt.scatter(X[sv_ind], y[sv_ind], c='r', s=50, label='SVR support vectors')\nplt.scatter(X[:100], y[:100], c='k', label='data')\nplt.hold('on')\nplt.plot(X_plot, y_svr, c='r',\n         label='SVR (fit: %.3fs, predict: %.3fs)' % (svr_fit, svr_predict))\nplt.plot(X_plot, y_kr, c='g',\n         label='KRR (fit: %.3fs, predict: %.3fs)' % (kr_fit, kr_predict))\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('SVR versus Kernel Ridge')\nplt.legend()\n\n# Visualize training and prediction time\nplt.figure()\n\n# Generate sample data\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0]/5))\nsizes = np.logspace(1, 4, 7)\nfor name, estimator in {"KRR": KernelRidge(kernel='rbf', alpha=0.1,\n                                           gamma=10),\n                        "SVR": SVR(kernel='rbf', C=1e1, gamma=10)}.items():\n    train_time = []\n    test_time = []\n    for train_test_size in sizes:\n        t0 = time.time()\n        estimator.fit(X[:train_test_size], y[:train_test_size])\n        train_time.append(time.time() - t0)\n\n        t0 = time.time()\n        estimator.predict(X_plot[:1000])\n        test_time.append(time.time() - t0)\n\n    plt.plot(sizes, train_time, 'o-', color="r" if name == "SVR" else "g",\n             label="%s (train)" % name)\n    plt.plot(sizes, test_time, 'o--', color="r" if name == "SVR" else "g",\n             label="%s (test)" % name)\n\nplt.xscale("log")\nplt.yscale("log")\nplt.xlabel("Train size")\nplt.ylabel("Time (seconds)")\nplt.title('Execution Time')\nplt.legend(loc="best")\n\n# Visualize learning curves\nplt.figure()\n\nsvr = SVR(kernel='rbf', C=1e1, gamma=0.1)\nkr = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)\ntrain_sizes, train_scores_svr, test_scores_svr = \\n    learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring="mean_squared_error", cv=10)\ntrain_sizes_abs, train_scores_kr, test_scores_kr = \\n    learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring="mean_squared_error", cv=10)\n\nplt.plot(train_sizes, test_scores_svr.mean(1), 'o-', color="r",\n         label="SVR")\nplt.plot(train_sizes, test_scores_kr.mean(1), 'o-', color="g",\n         label="KRR")\nplt.xlabel("Train size")\nplt.ylabel("Mean Squared Error")\nplt.title('Learning curves')\nplt.legend(loc="best")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_kernel_ridge_regression.html
Demonstration of k-means assumptions	A							http://scikit-learn.org/stable/auto_examples/index.html			This example is meant to illustrate situations where k-means will produce unintuitive and possibly unexpected clusters. In the first three plots, the input data does not conform to some implicit assumption that k-means makes and undesirable clusters are produced as a result. In the last plot, k-means returns intuitive clusters despite unevenly sized blobs.<br><br><pre><code>print(__doc__)\n\n# Author: Phil Roth <mr.phil.roth@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nplt.figure(figsize=(12, 12))\n\nn_samples = 1500\nrandom_state = 170\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\n\n# Incorrect number of clusters\ny_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n\nplt.subplot(221)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.title("Incorrect Number of Blobs")\n\n# Anisotropicly distributed data\ntransformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X, transformation)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n\nplt.subplot(222)\nplt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\nplt.title("Anisotropicly Distributed Blobs")\n\n# Different variance\nX_varied, y_varied = make_blobs(n_samples=n_samples,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=random_state)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n\nplt.subplot(223)\nplt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nplt.title("Unequal Variance")\n\n# Unevenly sized blobs\nX_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_filtered)\n\nplt.subplot(224)\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\nplt.title("Unevenly Sized Blobs")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html
A demo of K-Means clustering on the handwritten digits data	A							http://scikit-learn.org/stable/auto_examples/index.html			In this example we compare the various initialization strategies for K-means in terms of runtime and quality of the results.<br><br><pre><code>print(__doc__)\n\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\n\nnp.random.seed(42)\n\ndigits = load_digits()\ndata = scale(digits.data)\n\nn_samples, n_features = data.shape\nn_digits = len(np.unique(digits.target))\nlabels = digits.target\n\nsample_size = 300\n\nprint("n_digits: %d, \t n_samples %d, \t n_features %d"\n      % (n_digits, n_samples, n_features))\n\n\nprint(79 * '_')\nprint('% 9s' % 'init'\n      '    time  inertia    homo   compl  v-meas     ARI AMI  silhouette')\n\n\ndef bench_k_means(estimator, name, data):\n    t0 = time()\n    estimator.fit(data)\n    print('% 9s   %.2fs    %i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n          % (name, (time() - t0), estimator.inertia_,\n             metrics.homogeneity_score(labels, estimator.labels_),\n             metrics.completeness_score(labels, estimator.labels_),\n             metrics.v_measure_score(labels, estimator.labels_),\n             metrics.adjusted_rand_score(labels, estimator.labels_),\n             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n             metrics.silhouette_score(data, estimator.labels_,\n                                      metric='euclidean',\n                                      sample_size=sample_size)))\n\nbench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10),\n              name="k-means++", data=data)\n\nbench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10),\n              name="random", data=data)\n\n# in this case the seeding of the centers is deterministic, hence we run the\n# kmeans algorithm only once with n_init=1\npca = PCA(n_components=n_digits).fit(data)\nbench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n              name="PCA-based",\n              data=data)\nprint(79 * '_')\n\n###############################################################################\n# Visualize the results on PCA-reduced data\n\nreduced_data = PCA(n_components=2).fit_transform(data)\nkmeans = KMeans(init='k-means++', n_clusters=n_digits, n_init=10)\nkmeans.fit(reduced_data)\n\n# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1)\nplt.clf()\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\n\nplt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='w', zorder=10)\nplt.title('K-means clustering on the digits dataset (PCA-reduced data)\n'\n          'Centroids are marked with white cross')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html
Selecting the number of clusters with silhouette analysis on KMeans clustering	A							http://scikit-learn.org/stable/auto_examples/index.html			Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].<br><br><pre><code>from __future__ import print_function\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n# Generating the sample data from make_blobs\n# This particular setting has one distict cluster and 3 clusters placed close\n# together.\nX, y = make_blobs(n_samples=500,\n                  n_features=2,\n                  centers=4,\n                  cluster_std=1,\n                  center_box=(-10.0, 10.0),\n                  shuffle=True,\n                  random_state=1)  # For reproducibility\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print("For n_clusters =", n_clusters,\n          "The average silhouette_score is :", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title("The silhouette plot for the various clusters.")\n    ax1.set_xlabel("The silhouette coefficient values")\n    ax1.set_ylabel("Cluster label")\n\n    # The vertical line for average silhoutte score of all the values\n    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors)\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1],\n                marker='o', c="white", alpha=1, s=200)\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)\n\n    ax2.set_title("The visualization of the clustered data.")\n    ax2.set_xlabel("Feature space for the 1st feature")\n    ax2.set_ylabel("Feature space for the 2nd feature")\n\n    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "\n                  "with n_clusters = %d" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html
Empirical evaluation of the impact of k-means initialization	A							http://scikit-learn.org/stable/auto_examples/index.html			Evaluate the ability of k-means initializations strategies to make the algorithm convergence robust as measured by the relative standard deviation of the inertia of the clustering (i.e. the sum of distances to the nearest cluster center).<br><br><pre><code>print(__doc__)\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import check_random_state\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\n\nrandom_state = np.random.RandomState(0)\n\n# Number of run (with randomly generated dataset) for each strategy so as\n# to be able to compute an estimate of the standard deviation\nn_runs = 5\n\n# k-means models can do several random inits so as to be able to trade\n# CPU time for convergence robustness\nn_init_range = np.array([1, 5, 10, 15, 20])\n\n# Datasets generation parameters\nn_samples_per_center = 100\ngrid_size = 3\nscale = 0.1\nn_clusters = grid_size ** 2\n\n\ndef make_data(random_state, n_samples_per_center, grid_size, scale):\n    random_state = check_random_state(random_state)\n    centers = np.array([[i, j]\n                        for i in range(grid_size)\n                        for j in range(grid_size)])\n    n_clusters_true, n_features = centers.shape\n\n    noise = random_state.normal(\n        scale=scale, size=(n_samples_per_center, centers.shape[1]))\n\n    X = np.concatenate([c + noise for c in centers])\n    y = np.concatenate([[i] * n_samples_per_center\n                        for i in range(n_clusters_true)])\n    return shuffle(X, y, random_state=random_state)\n\n# Part 1: Quantitative evaluation of various init methods\n\nfig = plt.figure()\nplots = []\nlegends = []\n\ncases = [\n    (KMeans, 'k-means++', {}),\n    (KMeans, 'random', {}),\n    (MiniBatchKMeans, 'k-means++', {'max_no_improvement': 3}),\n    (MiniBatchKMeans, 'random', {'max_no_improvement': 3, 'init_size': 500}),\n]\n\nfor factory, init, params in cases:\n    print("Evaluation of %s with %s init" % (factory.__name__, init))\n    inertia = np.empty((len(n_init_range), n_runs))\n\n    for run_id in range(n_runs):\n        X, y = make_data(run_id, n_samples_per_center, grid_size, scale)\n        for i, n_init in enumerate(n_init_range):\n            km = factory(n_clusters=n_clusters, init=init, random_state=run_id,\n                         n_init=n_init, **params).fit(X)\n            inertia[i, run_id] = km.inertia_\n    p = plt.errorbar(n_init_range, inertia.mean(axis=1), inertia.std(axis=1))\n    plots.append(p[0])\n    legends.append("%s with %s init" % (factory.__name__, init))\n\nplt.xlabel('n_init')\nplt.ylabel('inertia')\nplt.legend(plots, legends)\nplt.title("Mean inertia for various k-means init across %d runs" % n_runs)\n\n# Part 2: Qualitative visual inspection of the convergence\n\nX, y = make_data(random_state, n_samples_per_center, grid_size, scale)\nkm = MiniBatchKMeans(n_clusters=n_clusters, init='random', n_init=1,\n                     random_state=random_state).fit(X)\n\nfig = plt.figure()\nfor k in range(n_clusters):\n    my_members = km.labels_ == k\n    color = cm.spectral(float(k) / n_clusters, 1)\n    plt.plot(X[my_members, 0], X[my_members, 1], 'o', marker='.', c=color)\n    cluster_center = km.cluster_centers_[k]\n    plt.plot(cluster_center[0], cluster_center[1], 'o',\n             markerfacecolor=color, markeredgecolor='k', markersize=6)\n    plt.title("Example cluster allocation with a single random init\n"\n              "with MiniBatchKMeans")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_stability_low_dim_dense.html
Label Propagation digits: Demonstrating performance	A							http://scikit-learn.org/stable/auto_examples/index.html			This example demonstrates the power of semisupervised learning by training a Label Spreading model to classify handwritten digits with sets of very few labels.<br><br><pre><code>print(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import label_propagation\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndigits = datasets.load_digits()\nrng = np.random.RandomState(0)\nindices = np.arange(len(digits.data))\nrng.shuffle(indices)\n\nX = digits.data[indices[:330]]\ny = digits.target[indices[:330]]\nimages = digits.images[indices[:330]]\n\nn_total_samples = len(y)\nn_labeled_points = 30\n\nindices = np.arange(n_total_samples)\n\nunlabeled_set = indices[n_labeled_points:]\n\n# shuffle everything around\ny_train = np.copy(y)\ny_train[unlabeled_set] = -1\n\n###############################################################################\n# Learn with LabelSpreading\nlp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=5)\nlp_model.fit(X, y_train)\npredicted_labels = lp_model.transduction_[unlabeled_set]\ntrue_labels = y[unlabeled_set]\n\ncm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)\n\nprint("Label Spreading model: %d labeled & %d unlabeled points (%d total)" %\n      (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))\n\nprint(classification_report(true_labels, predicted_labels))\n\nprint("Confusion matrix")\nprint(cm)\n\n# calculate uncertainty values for each transduced distribution\npred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\n\n# pick the top 10 most uncertain labels\nuncertainty_index = np.argsort(pred_entropies)[-10:]\n\n###############################################################################\n# plot\nf = plt.figure(figsize=(7, 5))\nfor index, image_index in enumerate(uncertainty_index):\n    image = images[image_index]\n\n    sub = f.add_subplot(2, 5, index + 1)\n    sub.imshow(image, cmap=plt.cm.gray_r)\n    plt.xticks([])\n    plt.yticks([])\n    sub.set_title('predict: %i\ntrue: %i' % (\n        lp_model.transduction_[image_index], y[image_index]))\n\nf.suptitle('Learning with small amount of labeled data')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits.html
Label Propagation digits active learning	A							http://scikit-learn.org/stable/auto_examples/index.html			Demonstrates an active learning technique to learn handwritten digits using label propagation.<br><br><pre><code>print(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import label_propagation\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndigits = datasets.load_digits()\nrng = np.random.RandomState(0)\nindices = np.arange(len(digits.data))\nrng.shuffle(indices)\n\nX = digits.data[indices[:330]]\ny = digits.target[indices[:330]]\nimages = digits.images[indices[:330]]\n\nn_total_samples = len(y)\nn_labeled_points = 10\n\nunlabeled_indices = np.arange(n_total_samples)[n_labeled_points:]\nf = plt.figure()\n\nfor i in range(5):\n    y_train = np.copy(y)\n    y_train[unlabeled_indices] = -1\n\n    lp_model = label_propagation.LabelSpreading(gamma=0.25, max_iter=5)\n    lp_model.fit(X, y_train)\n\n    predicted_labels = lp_model.transduction_[unlabeled_indices]\n    true_labels = y[unlabeled_indices]\n\n    cm = confusion_matrix(true_labels, predicted_labels,\n                          labels=lp_model.classes_)\n\n    print('Iteration %i %s' % (i, 70 * '_'))\n    print("Label Spreading model: %d labeled & %d unlabeled (%d total)"\n          % (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))\n\n    print(classification_report(true_labels, predicted_labels))\n\n    print("Confusion matrix")\n    print(cm)\n\n    # compute the entropies of transduced label distributions\n    pred_entropies = stats.distributions.entropy(\n        lp_model.label_distributions_.T)\n\n    # select five digit examples that the classifier is most uncertain about\n    uncertainty_index = uncertainty_index = np.argsort(pred_entropies)[-5:]\n\n    # keep track of indices that we get labels for\n    delete_indices = np.array([])\n\n    f.text(.05, (1 - (i + 1) * .183),\n           "model %d\n\nfit with\n%d labels" % ((i + 1), i * 5 + 10), size=10)\n    for index, image_index in enumerate(uncertainty_index):\n        image = images[image_index]\n\n        sub = f.add_subplot(5, 5, index + 1 + (5 * i))\n        sub.imshow(image, cmap=plt.cm.gray_r)\n        sub.set_title('predict: %i\ntrue: %i' % (\n            lp_model.transduction_[image_index], y[image_index]), size=10)\n        sub.axis('off')\n\n        # labeling 5 points, remote from labeled set\n        delete_index, = np.where(unlabeled_indices == image_index)\n        delete_indices = np.concatenate((delete_indices, delete_index))\n\n    unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\n    n_labeled_points += 5\n\nf.suptitle("Active learning with Label Propagation.\nRows show 5 most "\n           "uncertain labels to learn with the next model.")\nplt.subplots_adjust(0.12, 0.03, 0.9, 0.8, 0.2, 0.45)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits_active_learning.html
Label Propagation learning a complex structure	A							http://scikit-learn.org/stable/auto_examples/index.html			Example of LabelPropagation learning a complex internal structure to demonstrate “manifold learning”. The outer circle should be labeled “red” and the inner circle “blue”. Because both label groups lie inside their own distinct shape, we can see that the labels propagate correctly around the circle.<br><br><pre><code>print(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n#          Andreas Mueller <amueller@ais.uni-bonn.de>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.semi_supervised import label_propagation\nfrom sklearn.datasets import make_circles\n\n# generate ring with inner box\nn_samples = 200\nX, y = make_circles(n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = -np.ones(n_samples)\nlabels[0] = outer\nlabels[-1] = inner\n\n###############################################################################\n# Learn with LabelSpreading\nlabel_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0)\nlabel_spread.fit(X, labels)\n\n###############################################################################\n# Plot output labels\noutput_labels = label_spread.transduction_\nplt.figure(figsize=(8.5, 4))\nplt.subplot(1, 2, 1)\nplot_outer_labeled, = plt.plot(X[labels == outer, 0],\n                               X[labels == outer, 1], 'rs')\nplot_unlabeled, = plt.plot(X[labels == -1, 0], X[labels == -1, 1], 'g.')\nplot_inner_labeled, = plt.plot(X[labels == inner, 0],\n                               X[labels == inner, 1], 'bs')\nplt.legend((plot_outer_labeled, plot_inner_labeled, plot_unlabeled),\n           ('Outer Labeled', 'Inner Labeled', 'Unlabeled'), loc='upper left',\n           numpoints=1, shadow=False)\nplt.title("Raw data (2 classes=red and blue)")\n\nplt.subplot(1, 2, 2)\noutput_label_array = np.asarray(output_labels)\nouter_numbers = np.where(output_label_array == outer)[0]\ninner_numbers = np.where(output_label_array == inner)[0]\nplot_outer, = plt.plot(X[outer_numbers, 0], X[outer_numbers, 1], 'rs')\nplot_inner, = plt.plot(X[inner_numbers, 0], X[inner_numbers, 1], 'bs')\nplt.legend((plot_outer, plot_inner), ('Outer Learned', 'Inner Learned'),\n           loc='upper left', numpoints=1, shadow=False)\nplt.title("Labels learned with Label Spreading (KNN)")\n\nplt.subplots_adjust(left=0.07, bottom=0.07, right=0.93, top=0.92)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_structure.html
Decision boundary of label propagation versus SVM on the Iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			Comparison for decision boundary generated on iris dataset between Label Propagation and SVM.<br><br><pre><code>print(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# Licence: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn import svm\nfrom sklearn.semi_supervised import label_propagation\n\nrng = np.random.RandomState(0)\n\niris = datasets.load_iris()\n\nX = iris.data[:, :2]\ny = iris.target\n\n# step size in the mesh\nh = .02\n\ny_30 = np.copy(y)\ny_30[rng.rand(len(y)) < 0.3] = -1\ny_50 = np.copy(y)\ny_50[rng.rand(len(y)) < 0.5] = -1\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nls30 = (label_propagation.LabelSpreading().fit(X, y_30),\n        y_30)\nls50 = (label_propagation.LabelSpreading().fit(X, y_50),\n        y_50)\nls100 = (label_propagation.LabelSpreading().fit(X, y), y)\nrbf_svc = (svm.SVC(kernel='rbf').fit(X, y), y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = ['Label Spreading 30% data',\n          'Label Spreading 50% data',\n          'Label Spreading 100% data',\n          'SVC with rbf kernel']\n\ncolor_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}\n\nfor i, (clf, y_train) in enumerate((ls30, ls50, ls100, rbf_svc)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.axis('off')\n\n    # Plot also the training points\n    colors = [color_map[y] for y in y_train]\n    plt.scatter(X[:, 0], X[:, 1], c=colors, cmap=plt.cm.Paired)\n\n    plt.title(titles[i])\n\nplt.text(.90, 0, "Unlabeled points are colored white")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_versus_svm_iris.html
Lasso and Elastic Net for Sparse Signals	A							http://scikit-learn.org/stable/auto_examples/index.html			Estimates Lasso and Elastic-Net regression models on a manually generated sparse signal corrupted with an additive noise. Estimated coefficients are compared with the ground-truth.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import r2_score\n\n###############################################################################\n# generate some sparse data to play with\nnp.random.seed(42)\n\nn_samples, n_features = 50, 200\nX = np.random.randn(n_samples, n_features)\ncoef = 3 * np.random.randn(n_features)\ninds = np.arange(n_features)\nnp.random.shuffle(inds)\ncoef[inds[10:]] = 0  # sparsify coef\ny = np.dot(X, coef)\n\n# add noise\ny += 0.01 * np.random.normal((n_samples,))\n\n# Split data in train set and test set\nn_samples = X.shape[0]\nX_train, y_train = X[:n_samples / 2], y[:n_samples / 2]\nX_test, y_test = X[n_samples / 2:], y[n_samples / 2:]\n\n###############################################################################\n# Lasso\nfrom sklearn.linear_model import Lasso\n\nalpha = 0.1\nlasso = Lasso(alpha=alpha)\n\ny_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\nr2_score_lasso = r2_score(y_test, y_pred_lasso)\nprint(lasso)\nprint("r^2 on test data : %f" % r2_score_lasso)\n\n###############################################################################\n# ElasticNet\nfrom sklearn.linear_model import ElasticNet\n\nenet = ElasticNet(alpha=alpha, l1_ratio=0.7)\n\ny_pred_enet = enet.fit(X_train, y_train).predict(X_test)\nr2_score_enet = r2_score(y_test, y_pred_enet)\nprint(enet)\nprint("r^2 on test data : %f" % r2_score_enet)\n\nplt.plot(enet.coef_, label='Elastic net coefficients')\nplt.plot(lasso.coef_, label='Lasso coefficients')\nplt.plot(coef, '--', label='original coefficients')\nplt.legend(loc='best')\nplt.title("Lasso R^2: %f, Elastic Net R^2: %f"\n          % (r2_score_lasso, r2_score_enet))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html
Lasso and Elastic Net	A							http://scikit-learn.org/stable/auto_examples/index.html			Lasso and elastic net (L1 and L2 penalisation) implemented using a coordinate descent.<br><br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import lasso_path, enet_path\nfrom sklearn import datasets\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\nX /= X.std(axis=0)  # Standardize data (easier to set the l1_ratio parameter)\n\n# Compute paths\n\neps = 5e-3  # the smaller it is the longer is the path\n\nprint("Computing regularization path using the lasso...")\nalphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)\n\nprint("Computing regularization path using the positive lasso...")\nalphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(\n    X, y, eps, positive=True, fit_intercept=False)\nprint("Computing regularization path using the elastic net...")\nalphas_enet, coefs_enet, _ = enet_path(\n    X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)\n\nprint("Computing regularization path using the positve elastic net...")\nalphas_positive_enet, coefs_positive_enet, _ = enet_path(\n    X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)\n\n# Display results\n\nplt.figure(1)\nax = plt.gca()\nax.set_color_cycle(2 * ['b', 'r', 'g', 'c', 'k'])\nl1 = plt.plot(-np.log10(alphas_lasso), coefs_lasso.T)\nl2 = plt.plot(-np.log10(alphas_enet), coefs_enet.T, linestyle='--')\n\nplt.xlabel('-Log(alpha)')\nplt.ylabel('coefficients')\nplt.title('Lasso and Elastic-Net Paths')\nplt.legend((l1[-1], l2[-1]), ('Lasso', 'Elastic-Net'), loc='lower left')\nplt.axis('tight')\n\n\nplt.figure(2)\nax = plt.gca()\nax.set_color_cycle(2 * ['b', 'r', 'g', 'c', 'k'])\nl1 = plt.plot(-np.log10(alphas_lasso), coefs_lasso.T)\nl2 = plt.plot(-np.log10(alphas_positive_lasso), coefs_positive_lasso.T,\n              linestyle='--')\n\nplt.xlabel('-Log(alpha)')\nplt.ylabel('coefficients')\nplt.title('Lasso and positive Lasso')\nplt.legend((l1[-1], l2[-1]), ('Lasso', 'positive Lasso'), loc='lower left')\nplt.axis('tight')\n\n\nplt.figure(3)\nax = plt.gca()\nax.set_color_cycle(2 * ['b', 'r', 'g', 'c', 'k'])\nl1 = plt.plot(-np.log10(alphas_enet), coefs_enet.T)\nl2 = plt.plot(-np.log10(alphas_positive_enet), coefs_positive_enet.T,\n              linestyle='--')\n\nplt.xlabel('-Log(alpha)')\nplt.ylabel('coefficients')\nplt.title('Elastic-Net and positive Elastic-Net')\nplt.legend((l1[-1], l2[-1]), ('Elastic-Net', 'positive Elastic-Net'),\n           loc='lower left')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html
Lasso path using LARS	A							http://scikit-learn.org/stable/auto_examples/index.html			Computes Lasso Path along the regularization parameter using the LARS algorithm on the diabetes dataset. Each color represents a different feature of the coefficient vector, and this is displayed as a function of the regularization parameter.<br><br><pre><code>print(__doc__)\n\n# Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\nprint("Computing regularization path using the LARS ...")\nalphas, _, coefs = linear_model.lars_path(X, y, method='lasso', verbose=True)\n\nxx = np.sum(np.abs(coefs.T), axis=1)\nxx /= xx[-1]\n\nplt.plot(xx, coefs.T)\nymin, ymax = plt.ylim()\nplt.vlines(xx, ymin, ymax, linestyle='dashed')\nplt.xlabel('|coef| / max|coef|')\nplt.ylabel('Coefficients')\nplt.title('LASSO Path')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html
Lasso model selection: Cross-Validation / AIC / BIC	A							http://scikit-learn.org/stable/auto_examples/index.html			Use the Akaike information criterion (AIC), the Bayes Information criterion (BIC) and cross-validation to select an optimal value of the regularization parameter alpha of the Lasso estimator.<br><br><pre><code>print(__doc__)\n\n# Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\nfrom sklearn import datasets\n\ndiabetes = datasets.load_diabetes()\nX = diabetes.data\ny = diabetes.target\n\nrng = np.random.RandomState(42)\nX = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features\n\n# normalize data as done by Lars to allow for comparison\nX /= np.sqrt(np.sum(X ** 2, axis=0))\n\n##############################################################################\n# LassoLarsIC: least angle regression with BIC/AIC criterion\n\nmodel_bic = LassoLarsIC(criterion='bic')\nt1 = time.time()\nmodel_bic.fit(X, y)\nt_bic = time.time() - t1\nalpha_bic_ = model_bic.alpha_\n\nmodel_aic = LassoLarsIC(criterion='aic')\nmodel_aic.fit(X, y)\nalpha_aic_ = model_aic.alpha_\n\n\ndef plot_ic_criterion(model, name, color):\n    alpha_ = model.alpha_\n    alphas_ = model.alphas_\n    criterion_ = model.criterion_\n    plt.plot(-np.log10(alphas_), criterion_, '--', color=color,\n             linewidth=3, label='%s criterion' % name)\n    plt.axvline(-np.log10(alpha_), color=color, linewidth=3,\n                label='alpha: %s estimate' % name)\n    plt.xlabel('-log(alpha)')\n    plt.ylabel('criterion')\n\nplt.figure()\nplot_ic_criterion(model_aic, 'AIC', 'b')\nplot_ic_criterion(model_bic, 'BIC', 'r')\nplt.legend()\nplt.title('Information-criterion for model selection (training time %.3fs)'\n          % t_bic)\n\n##############################################################################\n# LassoCV: coordinate descent\n\n# Compute paths\nprint("Computing regularization path using the coordinate descent lasso...")\nt1 = time.time()\nmodel = LassoCV(cv=20).fit(X, y)\nt_lasso_cv = time.time() - t1\n\n# Display results\nm_log_alphas = -np.log10(model.alphas_)\n\nplt.figure()\nymin, ymax = 2300, 3800\nplt.plot(m_log_alphas, model.mse_path_, ':')\nplt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n         label='Average across the folds', linewidth=2)\nplt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n            label='alpha: CV estimate')\n\nplt.legend()\n\nplt.xlabel('-log(alpha)')\nplt.ylabel('Mean square error')\nplt.title('Mean square error on each fold: coordinate descent '\n          '(train time: %.2fs)' % t_lasso_cv)\nplt.axis('tight')\nplt.ylim(ymin, ymax)\n\n##############################################################################\n# LassoLarsCV: least angle regression\n\n# Compute paths\nprint("Computing regularization path using the Lars lasso...")\nt1 = time.time()\nmodel = LassoLarsCV(cv=20).fit(X, y)\nt_lasso_lars_cv = time.time() - t1\n\n# Display results\nm_log_alphas = -np.log10(model.cv_alphas_)\n\nplt.figure()\nplt.plot(m_log_alphas, model.cv_mse_path_, ':')\nplt.plot(m_log_alphas, model.cv_mse_path_.mean(axis=-1), 'k',\n         label='Average across the folds', linewidth=2)\nplt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n            label='alpha CV')\nplt.legend()\n\nplt.xlabel('-log(alpha)')\nplt.ylabel('Mean square error')\nplt.title('Mean square error on each fold: Lars (train time: %.2fs)'\n          % t_lasso_lars_cv)\nplt.axis('tight')\nplt.ylim(ymin, ymax)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_model_selection.html
Normal and Shrinkage Linear Discriminant Analysis for classification	A							http://scikit-learn.org/stable/auto_examples/index.html			Shows how shrinkage improves classification.<br><br><pre><code>from __future__ import division\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nn_train = 20  # samples for training\nn_test = 200  # samples for testing\nn_averages = 50  # how often to repeat classification\nn_features_max = 75  # maximum number of features\nstep = 4  # step size for the calculation\n\n\ndef generate_data(n_samples, n_features):\n    """Generate random blob-ish data with noisy features.\n\n    This returns an array of input data with shape `(n_samples, n_features)`\n    and an array of `n_samples` target labels.\n\n    Only one feature contains discriminative information, the other features\n    contain only noise.\n    """\n    X, y = make_blobs(n_samples=n_samples, n_features=1, centers=[[-2], [2]])\n\n    # add non-discriminative features\n    if n_features > 1:\n        X = np.hstack([X, np.random.randn(n_samples, n_features - 1)])\n    return X, y\n\nacc_clf1, acc_clf2 = [], []\nn_features_range = range(1, n_features_max + 1, step)\nfor n_features in n_features_range:\n    score_clf1, score_clf2 = 0, 0\n    for _ in range(n_averages):\n        X, y = generate_data(n_train, n_features)\n\n        clf1 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto').fit(X, y)\n        clf2 = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None).fit(X, y)\n\n        X, y = generate_data(n_test, n_features)\n        score_clf1 += clf1.score(X, y)\n        score_clf2 += clf2.score(X, y)\n\n    acc_clf1.append(score_clf1 / n_averages)\n    acc_clf2.append(score_clf2 / n_averages)\n\nfeatures_samples_ratio = np.array(n_features_range) / n_train\n\nplt.plot(features_samples_ratio, acc_clf1, linewidth=2,\n         label="Linear Discriminant Analysis with shrinkage", color='r')\nplt.plot(features_samples_ratio, acc_clf2, linewidth=2,\n         label="Linear Discriminant Analysis", color='g')\n\nplt.xlabel('n_features / n_samples')\nplt.ylabel('Classification accuracy')\n\nplt.legend(loc=1, prop={'size': 12})\nplt.suptitle('Linear Discriminant Analysis vs. \\nshrinkage Linear Discriminant Analysis (1 discriminative feature)')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_lda.html
Linear and Quadratic Discriminant Analysis with confidence ellipsoid	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the confidence ellipsoids of each class and decision boundary<br><br><pre><code>print(__doc__)\n\nfrom scipy import linalg\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import colors\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n###############################################################################\n# colormap\ncmap = colors.LinearSegmentedColormap(\n    'red_blue_classes',\n    {'red': [(0, 1, 1), (1, 0.7, 0.7)],\n     'green': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n     'blue': [(0, 0.7, 0.7), (1, 1, 1)]})\nplt.cm.register_cmap(cmap=cmap)\n\n\n###############################################################################\n# generate datasets\ndef dataset_fixed_cov():\n    '''Generate 2 Gaussians samples with the same covariance matrix'''\n    n, dim = 300, 2\n    np.random.seed(0)\n    C = np.array([[0., -0.23], [0.83, .23]])\n    X = np.r_[np.dot(np.random.randn(n, dim), C),\n              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]\n    y = np.hstack((np.zeros(n), np.ones(n)))\n    return X, y\n\n\ndef dataset_cov():\n    '''Generate 2 Gaussians samples with different covariance matrices'''\n    n, dim = 300, 2\n    np.random.seed(0)\n    C = np.array([[0., -1.], [2.5, .7]]) * 2.\n    X = np.r_[np.dot(np.random.randn(n, dim), C),\n              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]\n    y = np.hstack((np.zeros(n), np.ones(n)))\n    return X, y\n\n\n###############################################################################\n# plot functions\ndef plot_data(lda, X, y, y_pred, fig_index):\n    splot = plt.subplot(2, 2, fig_index)\n    if fig_index == 1:\n        plt.title('Linear Discriminant Analysis')\n        plt.ylabel('Data with fixed covariance')\n    elif fig_index == 2:\n        plt.title('Quadratic Discriminant Analysis')\n    elif fig_index == 3:\n        plt.ylabel('Data with varying covariances')\n\n    tp = (y == y_pred)  # True Positive\n    tp0, tp1 = tp[y == 0], tp[y == 1]\n    X0, X1 = X[y == 0], X[y == 1]\n    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n\n    # class 0: dots\n    plt.plot(X0_tp[:, 0], X0_tp[:, 1], 'o', color='red')\n    plt.plot(X0_fp[:, 0], X0_fp[:, 1], '.', color='#990000')  # dark red\n\n    # class 1: dots\n    plt.plot(X1_tp[:, 0], X1_tp[:, 1], 'o', color='blue')\n    plt.plot(X1_fp[:, 0], X1_fp[:, 1], '.', color='#000099')  # dark blue\n\n    # class 0 and 1 : areas\n    nx, ny = 200, 100\n    x_min, x_max = plt.xlim()\n    y_min, y_max = plt.ylim()\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n                         np.linspace(y_min, y_max, ny))\n    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n    plt.pcolormesh(xx, yy, Z, cmap='red_blue_classes',\n                   norm=colors.Normalize(0., 1.))\n    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors='k')\n\n    # means\n    plt.plot(lda.means_[0][0], lda.means_[0][1],\n             'o', color='black', markersize=10)\n    plt.plot(lda.means_[1][0], lda.means_[1][1],\n             'o', color='black', markersize=10)\n\n    return splot\n\n\ndef plot_ellipse(splot, mean, cov, color):\n    v, w = linalg.eigh(cov)\n    u = w[0] / linalg.norm(w[0])\n    angle = np.arctan(u[1] / u[0])\n    angle = 180 * angle / np.pi  # convert to degrees\n    # filled Gaussian at 2 standard deviation\n    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n                              180 + angle, color=color)\n    ell.set_clip_box(splot.bbox)\n    ell.set_alpha(0.5)\n    splot.add_artist(ell)\n    splot.set_xticks(())\n    splot.set_yticks(())\n\n\ndef plot_lda_cov(lda, splot):\n    plot_ellipse(splot, lda.means_[0], lda.covariance_, 'red')\n    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')\n\n\ndef plot_qda_cov(qda, splot):\n    plot_ellipse(splot, qda.means_[0], qda.covariances_[0], 'red')\n    plot_ellipse(splot, qda.means_[1], qda.covariances_[1], 'blue')\n\n###############################################################################\nfor i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n    # Linear Discriminant Analysis\n    lda = LinearDiscriminantAnalysis(solver="svd", store_covariance=True)\n    y_pred = lda.fit(X, y).predict(X)\n    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)\n    plot_lda_cov(lda, splot)\n    plt.axis('tight')\n\n    # Quadratic Discriminant Analysis\n    qda = QuadraticDiscriminantAnalysis(store_covariances=True)\n    y_pred = qda.fit(X, y).predict(X)\n    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)\n    plot_qda_cov(qda, splot)\n    plt.axis('tight')\nplt.suptitle('Linear Discriminant Analysis vs Quadratic Discriminant Analysis')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html
Plotting Learning Curves	A							http://scikit-learn.org/stable/auto_examples/index.html			On the left side the learning curve of a naive Bayes classifier is shown for the digits dataset. Note that the training score and the cross-validation score are both not very good at the end. However, the shape of the curve can be found in more complex datasets very often: the training score is very high at the beginning and decreases and the cross-validation score is very low at the beginning...<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import cross_validation\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.learning_curve import learning_curve\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    """\n    Generate a simple plot of the test and traning learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the "fit" and "predict" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : integer, cross-validation generator, optional\n        If an integer is passed, it is the number of folds (defaults to 3).\n        Specific cross-validation objects can be passed, see\n        sklearn.cross_validation module for the list of possible objects\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    """\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel("Training examples")\n    plt.ylabel("Score")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color="r")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color="g")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color="r",\n             label="Training score")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color="g",\n             label="Cross-validation score")\n\n    plt.legend(loc="best")\n    return plt\n\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n\ntitle = "Learning Curves (Naive Bayes)"\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=100,\n                                   test_size=0.2, random_state=0)\n\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n\ntitle = "Learning Curves (SVM, RBF kernel, $\gamma=0.001$)"\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = cross_validation.ShuffleSplit(digits.data.shape[0], n_iter=10,\n                                   test_size=0.2, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html
Manifold learning on handwritten digits: Locally Linear Embedding, Isomap...	A							http://scikit-learn.org/stable/auto_examples/index.html			An illustration of various embeddings on the digits dataset.<br><br><pre><code># Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2011\n\nprint(__doc__)\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nfrom sklearn import (manifold, datasets, decomposition, ensemble,\n                     discriminant_analysis, random_projection)\n\ndigits = datasets.load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\nn_neighbors = 30\n\n\n#----------------------------------------------------------------------\n# Scale and visualize the embedding vectors\ndef plot_embedding(X, title=None):\n    x_min, x_max = np.min(X, 0), np.max(X, 0)\n    X = (X - x_min) / (x_max - x_min)\n\n    plt.figure()\n    ax = plt.subplot(111)\n    for i in range(X.shape[0]):\n        plt.text(X[i, 0], X[i, 1], str(digits.target[i]),\n                 color=plt.cm.Set1(y[i] / 10.),\n                 fontdict={'weight': 'bold', 'size': 9})\n\n    if hasattr(offsetbox, 'AnnotationBbox'):\n        # only print thumbnails with matplotlib > 1.0\n        shown_images = np.array([[1., 1.]])  # just something big\n        for i in range(digits.data.shape[0]):\n            dist = np.sum((X[i] - shown_images) ** 2, 1)\n            if np.min(dist) < 4e-3:\n                # don't show points that are too close\n                continue\n            shown_images = np.r_[shown_images, [X[i]]]\n            imagebox = offsetbox.AnnotationBbox(\n                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n                X[i])\n            ax.add_artist(imagebox)\n    plt.xticks([]), plt.yticks([])\n    if title is not None:\n        plt.title(title)\n\n\n#----------------------------------------------------------------------\n# Plot images of the digits\nn_img_per_row = 20\nimg = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\nfor i in range(n_img_per_row):\n    ix = 10 * i + 1\n    for j in range(n_img_per_row):\n        iy = 10 * j + 1\n        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n\nplt.imshow(img, cmap=plt.cm.binary)\nplt.xticks([])\nplt.yticks([])\nplt.title('A selection from the 64-dimensional digits dataset')\n\n\n#----------------------------------------------------------------------\n# Random 2D projection using a random unitary matrix\nprint("Computing random projection")\nrp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\nX_projected = rp.fit_transform(X)\nplot_embedding(X_projected, "Random Projection of the digits")\n\n\n#----------------------------------------------------------------------\n# Projection on to the first 2 principal components\n\nprint("Computing PCA projection")\nt0 = time()\nX_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\nplot_embedding(X_pca,\n               "Principal Components projection of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# Projection on to the first 2 linear discriminant components\n\nprint("Computing Linear Discriminant Analysis projection")\nX2 = X.copy()\nX2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible\nt0 = time()\nX_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2).fit_transform(X2, y)\nplot_embedding(X_lda,\n               "Linear Discriminant projection of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# Isomap projection of the digits dataset\nprint("Computing Isomap embedding")\nt0 = time()\nX_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\nprint("Done.")\nplot_embedding(X_iso,\n               "Isomap projection of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# Locally linear embedding of the digits dataset\nprint("Computing LLE embedding")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method='standard')\nt0 = time()\nX_lle = clf.fit_transform(X)\nprint("Done. Reconstruction error: %g" % clf.reconstruction_error_)\nplot_embedding(X_lle,\n               "Locally Linear Embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# Modified Locally linear embedding of the digits dataset\nprint("Computing modified LLE embedding")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method='modified')\nt0 = time()\nX_mlle = clf.fit_transform(X)\nprint("Done. Reconstruction error: %g" % clf.reconstruction_error_)\nplot_embedding(X_mlle,\n               "Modified Locally Linear Embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# HLLE embedding of the digits dataset\nprint("Computing Hessian LLE embedding")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method='hessian')\nt0 = time()\nX_hlle = clf.fit_transform(X)\nprint("Done. Reconstruction error: %g" % clf.reconstruction_error_)\nplot_embedding(X_hlle,\n               "Hessian Locally Linear Embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n\n#----------------------------------------------------------------------\n# LTSA embedding of the digits dataset\nprint("Computing LTSA embedding")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method='ltsa')\nt0 = time()\nX_ltsa = clf.fit_transform(X)\nprint("Done. Reconstruction error: %g" % clf.reconstruction_error_)\nplot_embedding(X_ltsa,\n               "Local Tangent Space Alignment of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# MDS  embedding of the digits dataset\nprint("Computing MDS embedding")\nclf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\nt0 = time()\nX_mds = clf.fit_transform(X)\nprint("Done. Stress: %f" % clf.stress_)\nplot_embedding(X_mds,\n               "MDS embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# Random Trees embedding of the digits dataset\nprint("Computing Totally Random Trees embedding")\nhasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,\n                                       max_depth=5)\nt0 = time()\nX_transformed = hasher.fit_transform(X)\npca = decomposition.TruncatedSVD(n_components=2)\nX_reduced = pca.fit_transform(X_transformed)\n\nplot_embedding(X_reduced,\n               "Random forest embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# Spectral embedding of the digits dataset\nprint("Computing Spectral embedding")\nembedder = manifold.SpectralEmbedding(n_components=2, random_state=0,\n                                      eigen_solver="arpack")\nt0 = time()\nX_se = embedder.fit_transform(X)\n\nplot_embedding(X_se,\n               "Spectral embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\n#----------------------------------------------------------------------\n# t-SNE embedding of the digits dataset\nprint("Computing t-SNE embedding")\ntsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\nt0 = time()\nX_tsne = tsne.fit_transform(X)\n\nplot_embedding(X_tsne,\n               "t-SNE embedding of the digits (time %.2fs)" %\n               (time() - t0))\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html
Logit function	A							http://scikit-learn.org/stable/auto_examples/index.html			Show in the plot is how the logistic regression would, in this synthetic dataset, classify values as either 0 or 1, i.e. class one or two, using the logit-curve.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gael Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\n# this is our test set, it's just a straight line with some\n# Gaussian noise\nxmin, xmax = -5, 5\nn_samples = 100\nnp.random.seed(0)\nX = np.random.normal(size=n_samples)\ny = (X > 0).astype(np.float)\nX[X > 0] *= 4\nX += .3 * np.random.normal(size=n_samples)\n\nX = X[:, np.newaxis]\n# run the classifier\nclf = linear_model.LogisticRegression(C=1e5)\nclf.fit(X, y)\n\n# and plot the result\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.scatter(X.ravel(), y, color='black', zorder=20)\nX_test = np.linspace(-5, 10, 300)\n\n\ndef model(x):\n    return 1 / (1 + np.exp(-x))\nloss = model(X_test * clf.coef_ + clf.intercept_).ravel()\nplt.plot(X_test, loss, color='blue', linewidth=3)\n\nols = linear_model.LinearRegression()\nols.fit(X, y)\nplt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\nplt.axhline(.5, color='.5')\n\nplt.ylabel('y')\nplt.xlabel('X')\nplt.xticks(())\nplt.yticks(())\nplt.ylim(-.25, 1.25)\nplt.xlim(-4, 10)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html
L1 Penalty and Sparsity in Logistic Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			Comparison of the sparsity (percentage of zero coefficients) of solutions when L1 and L2 penalty are used for different values of C. We can see that large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions.<br><br><pre><code>print(__doc__)\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Andreas Mueller <amueller@ais.uni-bonn.de>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\ndigits = datasets.load_digits()\n\nX, y = digits.data, digits.target\nX = StandardScaler().fit_transform(X)\n\n# classify small against large digits\ny = (y > 4).astype(np.int)\n\n\n# Set regularization parameter\nfor i, C in enumerate((100, 1, 0.01)):\n    # turn down tolerance for short training time\n    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n    clf_l1_LR.fit(X, y)\n    clf_l2_LR.fit(X, y)\n\n    coef_l1_LR = clf_l1_LR.coef_.ravel()\n    coef_l2_LR = clf_l2_LR.coef_.ravel()\n\n    # coef_l1_LR contains zeros due to the\n    # L1 sparsity inducing norm\n\n    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n\n    print("C=%.2f" % C)\n    print("Sparsity with L1 penalty: %.2f%%" % sparsity_l1_LR)\n    print("score with L1 penalty: %.4f" % clf_l1_LR.score(X, y))\n    print("Sparsity with L2 penalty: %.2f%%" % sparsity_l2_LR)\n    print("score with L2 penalty: %.4f" % clf_l2_LR.score(X, y))\n\n    l1_plot = plt.subplot(3, 2, 2 * i + 1)\n    l2_plot = plt.subplot(3, 2, 2 * (i + 1))\n    if i == 0:\n        l1_plot.set_title("L1 penalty")\n        l2_plot.set_title("L2 penalty")\n\n    l1_plot.imshow(np.abs(coef_l1_LR.reshape(8, 8)), interpolation='nearest',\n                   cmap='binary', vmax=1, vmin=0)\n    l2_plot.imshow(np.abs(coef_l2_LR.reshape(8, 8)), interpolation='nearest',\n                   cmap='binary', vmax=1, vmin=0)\n    plt.text(-8, 3, "C = %.2f" % C)\n\n    l1_plot.set_xticks(())\n    l1_plot.set_yticks(())\n    l2_plot.set_xticks(())\n    l2_plot.set_yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html
Path with L1- Logistic Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			Computes path on IRIS dataset.<br><br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\nfrom sklearn.svm import l1_min_c\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX = X[y != 2]\ny = y[y != 2]\n\nX -= np.mean(X, 0)\n\n###############################################################################\n# Demo path functions\n\ncs = l1_min_c(X, y, loss='log') * np.logspace(0, 3)\n\n\nprint("Computing regularization path ...")\nstart = datetime.now()\nclf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\ncoefs_ = []\nfor c in cs:\n    clf.set_params(C=c)\n    clf.fit(X, y)\n    coefs_.append(clf.coef_.ravel().copy())\nprint("This took ", datetime.now() - start)\n\ncoefs_ = np.array(coefs_)\nplt.plot(np.log10(cs), coefs_)\nymin, ymax = plt.ylim()\nplt.xlabel('log(C)')\nplt.ylabel('Coefficients')\nplt.title('Logistic Regression Path')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html
Ledoit-Wolf vs OAS estimation	A							http://scikit-learn.org/stable/auto_examples/index.html			The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import toeplitz, cholesky\n\nfrom sklearn.covariance import LedoitWolf, OAS\n\nnp.random.seed(0)\n###############################################################################\nn_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = toeplitz(r ** np.arange(n_features))\ncoloring_matrix = cholesky(real_cov)\n\nn_samples_range = np.arange(6, 31, 1)\nrepeat = 100\nlw_mse = np.zeros((n_samples_range.size, repeat))\noa_mse = np.zeros((n_samples_range.size, repeat))\nlw_shrinkage = np.zeros((n_samples_range.size, repeat))\noa_shrinkage = np.zeros((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = np.dot(\n            np.random.normal(size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = LedoitWolf(store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = OAS(store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\nplt.subplot(2, 1, 1)\nplt.errorbar(n_samples_range, lw_mse.mean(1), yerr=lw_mse.std(1),\n             label='Ledoit-Wolf', color='g')\nplt.errorbar(n_samples_range, oa_mse.mean(1), yerr=oa_mse.std(1),\n             label='OAS', color='r')\nplt.ylabel("Squared error")\nplt.legend(loc="upper right")\nplt.title("Comparison of covariance estimators")\nplt.xlim(5, 31)\n\n# plot shrinkage coefficient\nplt.subplot(2, 1, 2)\nplt.errorbar(n_samples_range, lw_shrinkage.mean(1), yerr=lw_shrinkage.std(1),\n             label='Ledoit-Wolf', color='g')\nplt.errorbar(n_samples_range, oa_shrinkage.mean(1), yerr=oa_shrinkage.std(1),\n             label='OAS', color='r')\nplt.xlabel("n_samples")\nplt.ylabel("Shrinkage")\nplt.legend(loc="lower right")\nplt.ylim(plt.ylim()[0], 1. + (plt.ylim()[1] - plt.ylim()[0]) / 10.)\nplt.xlim(5, 31)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_lw_vs_oas.html
Robust covariance estimation and Mahalanobis distances relevance	A							http://scikit-learn.org/stable/auto_examples/index.html			An example to show covariance estimation with the Mahalanobis distances on Gaussian distributed data.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\nn_samples = 125\nn_outliers = 25\nn_features = 2\n\n# generate data\ngen_cov = np.eye(n_features)\ngen_cov[0, 0] = 2.\nX = np.dot(np.random.randn(n_samples, n_features), gen_cov)\n# add some outliers\noutliers_cov = np.eye(n_features)\noutliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.\nX[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)\n\n# fit a Minimum Covariance Determinant (MCD) robust estimator to data\nrobust_cov = MinCovDet().fit(X)\n\n# compare estimators learnt from the full data set with true parameters\nemp_cov = EmpiricalCovariance().fit(X)\n\n###############################################################################\n# Display results\nfig = plt.figure()\nplt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)\n\n# Show data set\nsubfig1 = plt.subplot(3, 1, 1)\ninlier_plot = subfig1.scatter(X[:, 0], X[:, 1],\n                              color='black', label='inliers')\noutlier_plot = subfig1.scatter(X[:, 0][-n_outliers:], X[:, 1][-n_outliers:],\n                               color='red', label='outliers')\nsubfig1.set_xlim(subfig1.get_xlim()[0], 11.)\nsubfig1.set_title("Mahalanobis distances of a contaminated data set:")\n\n# Show contours of the distance functions\nxx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))\nzz = np.c_[xx.ravel(), yy.ravel()]\n\nmahal_emp_cov = emp_cov.mahalanobis(zz)\nmahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\nemp_cov_contour = subfig1.contour(xx, yy, np.sqrt(mahal_emp_cov),\n                                  cmap=plt.cm.PuBu_r,\n                                  linestyles='dashed')\n\nmahal_robust_cov = robust_cov.mahalanobis(zz)\nmahal_robust_cov = mahal_robust_cov.reshape(xx.shape)\nrobust_contour = subfig1.contour(xx, yy, np.sqrt(mahal_robust_cov),\n                                 cmap=plt.cm.YlOrBr_r, linestyles='dotted')\n\nsubfig1.legend([emp_cov_contour.collections[1], robust_contour.collections[1],\n                inlier_plot, outlier_plot],\n               ['MLE dist', 'robust dist', 'inliers', 'outliers'],\n               loc="upper right", borderaxespad=0)\nplt.xticks(())\nplt.yticks(())\n\n# Plot the scores for each point\nemp_mahal = emp_cov.mahalanobis(X - np.mean(X, 0)) ** (0.33)\nsubfig2 = plt.subplot(2, 2, 3)\nsubfig2.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=.25)\nsubfig2.plot(1.26 * np.ones(n_samples - n_outliers),\n             emp_mahal[:-n_outliers], '+k', markeredgewidth=1)\nsubfig2.plot(2.26 * np.ones(n_outliers),\n             emp_mahal[-n_outliers:], '+k', markeredgewidth=1)\nsubfig2.axes.set_xticklabels(('inliers', 'outliers'), size=15)\nsubfig2.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)\nsubfig2.set_title("1. from non-robust estimates\n(Maximum Likelihood)")\nplt.yticks(())\n\nrobust_mahal = robust_cov.mahalanobis(X - robust_cov.location_) ** (0.33)\nsubfig3 = plt.subplot(2, 2, 4)\nsubfig3.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]],\n                widths=.25)\nsubfig3.plot(1.26 * np.ones(n_samples - n_outliers),\n             robust_mahal[:-n_outliers], '+k', markeredgewidth=1)\nsubfig3.plot(2.26 * np.ones(n_outliers),\n             robust_mahal[-n_outliers:], '+k', markeredgewidth=1)\nsubfig3.axes.set_xticklabels(('inliers', 'outliers'), size=15)\nsubfig3.set_ylabel(r"$\sqrt[3]{\rm{(Mahal. dist.)}}$", size=16)\nsubfig3.set_title("2. from robust estimates\n(Minimum Covariance Determinant)")\nplt.yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_mahalanobis_distances.html
Manifold Learning methods on a severed sphere	A							http://scikit-learn.org/stable/auto_examples/index.html			An application of the different Manifold learning techniques on a spherical data-set. Here one can see the use of dimensionality reduction in order to gain some intuition regarding the manifold learning methods. Regarding the dataset, the poles are cut from the sphere, as well as a thin slice down its side. This enables the manifold learning techniques to ‘spread it open’ whilst projecting it...<br><br><pre><code># Author: Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold\nfrom sklearn.utils import check_random_state\n\n# Next line to silence pyflakes.\nAxes3D\n\n# Variables for manifold learning.\nn_neighbors = 10\nn_samples = 1000\n\n# Create our sphere.\nrandom_state = check_random_state(0)\np = random_state.rand(n_samples) * (2 * np.pi - 0.55)\nt = random_state.rand(n_samples) * np.pi\n\n# Sever the poles from the sphere.\nindices = ((t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8))))\ncolors = p[indices]\nx, y, z = np.sin(t[indices]) * np.cos(p[indices]), \\n    np.sin(t[indices]) * np.sin(p[indices]), \\n    np.cos(t[indices])\n\n# Plot our dataset.\nfig = plt.figure(figsize=(15, 8))\nplt.suptitle("Manifold Learning with %i points, %i neighbors"\n             % (1000, n_neighbors), fontsize=14)\n\nax = fig.add_subplot(251, projection='3d')\nax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)\ntry:\n    # compatibility matplotlib < 1.0\n    ax.view_init(40, -10)\nexcept:\n    pass\n\nsphere_data = np.array([x, y, z]).T\n\n# Perform Locally Linear Embedding Manifold learning\nmethods = ['standard', 'ltsa', 'hessian', 'modified']\nlabels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']\n\nfor i, method in enumerate(methods):\n    t0 = time()\n    trans_data = manifold\\n        .LocallyLinearEmbedding(n_neighbors, 2,\n                                method=method).fit_transform(sphere_data).T\n    t1 = time()\n    print("%s: %.2g sec" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n    plt.title("%s (%.2g sec)" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis('tight')\n\n# Perform Isomap Manifold learning.\nt0 = time()\ntrans_data = manifold.Isomap(n_neighbors, n_components=2)\\n    .fit_transform(sphere_data).T\nt1 = time()\nprint("%s: %.2g sec" % ('ISO', t1 - t0))\n\nax = fig.add_subplot(257)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title("%s (%.2g sec)" % ('Isomap', t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n# Perform Multi-dimensional scaling.\nt0 = time()\nmds = manifold.MDS(2, max_iter=100, n_init=1)\ntrans_data = mds.fit_transform(sphere_data).T\nt1 = time()\nprint("MDS: %.2g sec" % (t1 - t0))\n\nax = fig.add_subplot(258)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title("MDS (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n# Perform Spectral Embedding.\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=2,\n                                n_neighbors=n_neighbors)\ntrans_data = se.fit_transform(sphere_data).T\nt1 = time()\nprint("Spectral Embedding: %.2g sec" % (t1 - t0))\n\nax = fig.add_subplot(259)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title("Spectral Embedding (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\n# Perform t-distributed stochastic neighbor embedding.\nt0 = time()\ntsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\ntrans_data = tsne.fit_transform(sphere_data).T\nt1 = time()\nprint("t-SNE: %.2g sec" % (t1 - t0))\n\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title("t-SNE (%.2g sec)" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html
Multi-dimensional scaling	A							http://scikit-learn.org/stable/auto_examples/index.html			An illustration of the metric and non-metric MDS on generated noisy data.<br><br><pre><code># Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# Licence: BSD\n\nprint(__doc__)\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\nfrom sklearn.decomposition import PCA\n\nn_samples = 20\nseed = np.random.RandomState(seed=3)\nX_true = seed.randint(0, 20, 2 * n_samples).astype(np.float)\nX_true = X_true.reshape((n_samples, 2))\n# Center the data\nX_true -= X_true.mean()\n\nsimilarities = euclidean_distances(X_true)\n\n# Add noise to the similarities\nnoise = np.random.rand(n_samples, n_samples)\nnoise = noise + noise.T\nnoise[np.arange(noise.shape[0]), np.arange(noise.shape[0])] = 0\nsimilarities += noise\n\nmds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,\n                   dissimilarity="precomputed", n_jobs=1)\npos = mds.fit(similarities).embedding_\n\nnmds = manifold.MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,\n                    dissimilarity="precomputed", random_state=seed, n_jobs=1,\n                    n_init=1)\nnpos = nmds.fit_transform(similarities, init=pos)\n\n# Rescale the data\npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum())\nnpos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum())\n\n# Rotate the data\nclf = PCA(n_components=2)\nX_true = clf.fit_transform(X_true)\n\npos = clf.fit_transform(pos)\n\nnpos = clf.fit_transform(npos)\n\nfig = plt.figure(1)\nax = plt.axes([0., 0., 1., 1.])\n\nplt.scatter(X_true[:, 0], X_true[:, 1], c='r', s=20)\nplt.scatter(pos[:, 0], pos[:, 1], s=20, c='g')\nplt.scatter(npos[:, 0], npos[:, 1], s=20, c='b')\nplt.legend(('True position', 'MDS', 'NMDS'), loc='best')\n\nsimilarities = similarities.max() / similarities * 100\nsimilarities[np.isinf(similarities)] = 0\n\n# Plot the edges\nstart_idx, end_idx = np.where(pos)\n#a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [[X_true[i, :], X_true[j, :]]\n            for i in range(len(pos)) for j in range(len(pos))]\nvalues = np.abs(similarities)\nlc = LineCollection(segments,\n                    zorder=0, cmap=plt.cm.hot_r,\n                    norm=plt.Normalize(0, values.max()))\nlc.set_array(similarities.flatten())\nlc.set_linewidths(0.5 * np.ones(len(segments)))\nax.add_collection(lc)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_mds.html
A demo of the mean-shift clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			Reference:<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nfrom sklearn.datasets.samples_generator import make_blobs\n\n###############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)\n\n###############################################################################\n# Compute clustering with MeanShift\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\n\nlabels_unique = np.unique(labels)\nn_clusters_ = len(labels_unique)\n\nprint("number of estimated clusters : %d" % n_clusters_)\n\n###############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.figure(1)\nplt.clf()\n\ncolors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\nfor k, col in zip(range(n_clusters_), colors):\n    my_members = labels == k\n    cluster_center = cluster_centers[k]\n    plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n             markeredgecolor='k', markersize=14)\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html
Comparison of the K-Means and MiniBatchKMeans clustering algorithms	A							http://scikit-learn.org/stable/auto_examples/index.html			We want to compare the performance of the MiniBatchKMeans and KMeans: the MiniBatchKMeans is faster, but gives slightly different results (see Mini Batch K-Means).<br><br><pre><code>print(__doc__)\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nfrom sklearn.metrics.pairwise import pairwise_distances_argmin\nfrom sklearn.datasets.samples_generator import make_blobs\n\n##############################################################################\n# Generate sample data\nnp.random.seed(0)\n\nbatch_size = 45\ncenters = [[1, 1], [-1, -1], [1, -1]]\nn_clusters = len(centers)\nX, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\n\n##############################################################################\n# Compute clustering with Means\n\nk_means = KMeans(init='k-means++', n_clusters=3, n_init=10)\nt0 = time.time()\nk_means.fit(X)\nt_batch = time.time() - t0\nk_means_labels = k_means.labels_\nk_means_cluster_centers = k_means.cluster_centers_\nk_means_labels_unique = np.unique(k_means_labels)\n\n##############################################################################\n# Compute clustering with MiniBatchKMeans\n\nmbk = MiniBatchKMeans(init='k-means++', n_clusters=3, batch_size=batch_size,\n                      n_init=10, max_no_improvement=10, verbose=0)\nt0 = time.time()\nmbk.fit(X)\nt_mini_batch = time.time() - t0\nmbk_means_labels = mbk.labels_\nmbk_means_cluster_centers = mbk.cluster_centers_\nmbk_means_labels_unique = np.unique(mbk_means_labels)\n\n##############################################################################\n# Plot result\n\nfig = plt.figure(figsize=(8, 3))\nfig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\ncolors = ['#4EACC5', '#FF9C34', '#4E9A06']\n\n# We want to have the same colors for the same cluster from the\n# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per\n# closest one.\n\norder = pairwise_distances_argmin(k_means_cluster_centers,\n                                  mbk_means_cluster_centers)\n\n# KMeans\nax = fig.add_subplot(1, 3, 1)\nfor k, col in zip(range(n_clusters), colors):\n    my_members = k_means_labels == k\n    cluster_center = k_means_cluster_centers[k]\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n            markerfacecolor=col, marker='.')\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n            markeredgecolor='k', markersize=6)\nax.set_title('KMeans')\nax.set_xticks(())\nax.set_yticks(())\nplt.text(-3.5, 1.8,  'train time: %.2fs\ninertia: %f' % (\n    t_batch, k_means.inertia_))\n\n# MiniBatchKMeans\nax = fig.add_subplot(1, 3, 2)\nfor k, col in zip(range(n_clusters), colors):\n    my_members = mbk_means_labels == order[k]\n    cluster_center = mbk_means_cluster_centers[order[k]]\n    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n            markerfacecolor=col, marker='.')\n    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n            markeredgecolor='k', markersize=6)\nax.set_title('MiniBatchKMeans')\nax.set_xticks(())\nax.set_yticks(())\nplt.text(-3.5, 1.8, 'train time: %.2fs\ninertia: %f' %\n         (t_mini_batch, mbk.inertia_))\n\n# Initialise the different array to all False\ndifferent = (mbk_means_labels == 4)\nax = fig.add_subplot(1, 3, 3)\n\nfor l in range(n_clusters):\n    different += ((k_means_labels == k) != (mbk_means_labels == order[k]))\n\nidentic = np.logical_not(different)\nax.plot(X[identic, 0], X[identic, 1], 'w',\n        markerfacecolor='#bbbbbb', marker='.')\nax.plot(X[different, 0], X[different, 1], 'w',\n        markerfacecolor='m', marker='.')\nax.set_title('Difference')\nax.set_xticks(())\nax.set_yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_mini_batch_kmeans.html
Model Complexity Influence	A							http://scikit-learn.org/stable/auto_examples/index.html			Demonstrate how model complexity influences both prediction accuracy and computational performance.<br><br><pre><code>print(__doc__)\n\n# Author: Eustache Diemert <eustache@diemert.fr>\n# License: BSD 3 clause\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.parasite_axes import host_subplot\nfrom mpl_toolkits.axisartist.axislines import Axes\nfrom scipy.sparse.csr import csr_matrix\n\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm.classes import NuSVR\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\nfrom sklearn.linear_model.stochastic_gradient import SGDClassifier\nfrom sklearn.metrics import hamming_loss\n\n###############################################################################\n# Routines\n\n\n# initialize random generator\nnp.random.seed(0)\n\n\ndef generate_data(case, sparse=False):\n    """Generate regression/classification data."""\n    bunch = None\n    if case == 'regression':\n        bunch = datasets.load_boston()\n    elif case == 'classification':\n        bunch = datasets.fetch_20newsgroups_vectorized(subset='all')\n    X, y = shuffle(bunch.data, bunch.target)\n    offset = int(X.shape[0] * 0.8)\n    X_train, y_train = X[:offset], y[:offset]\n    X_test, y_test = X[offset:], y[offset:]\n    if sparse:\n        X_train = csr_matrix(X_train)\n        X_test = csr_matrix(X_test)\n    else:\n        X_train = np.array(X_train)\n        X_test = np.array(X_test)\n    y_test = np.array(y_test)\n    y_train = np.array(y_train)\n    data = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train,\n            'y_test': y_test}\n    return data\n\n\ndef benchmark_influence(conf):\n    """\n    Benchmark influence of :changing_param: on both MSE and latency.\n    """\n    prediction_times = []\n    prediction_powers = []\n    complexities = []\n    for param_value in conf['changing_param_values']:\n        conf['tuned_params'][conf['changing_param']] = param_value\n        estimator = conf['estimator'](**conf['tuned_params'])\n        print("Benchmarking %s" % estimator)\n        estimator.fit(conf['data']['X_train'], conf['data']['y_train'])\n        conf['postfit_hook'](estimator)\n        complexity = conf['complexity_computer'](estimator)\n        complexities.append(complexity)\n        start_time = time.time()\n        for _ in range(conf['n_samples']):\n            y_pred = estimator.predict(conf['data']['X_test'])\n        elapsed_time = (time.time() - start_time) / float(conf['n_samples'])\n        prediction_times.append(elapsed_time)\n        pred_score = conf['prediction_performance_computer'](\n            conf['data']['y_test'], y_pred)\n        prediction_powers.append(pred_score)\n        print("Complexity: %d | %s: %.4f | Pred. Time: %fs\n" % (\n            complexity, conf['prediction_performance_label'], pred_score,\n            elapsed_time))\n    return prediction_powers, prediction_times, complexities\n\n\ndef plot_influence(conf, mse_values, prediction_times, complexities):\n    """\n    Plot influence of model complexity on both accuracy and latency.\n    """\n    plt.figure(figsize=(12, 6))\n    host = host_subplot(111, axes_class=Axes)\n    plt.subplots_adjust(right=0.75)\n    par1 = host.twinx()\n    host.set_xlabel('Model Complexity (%s)' % conf['complexity_label'])\n    y1_label = conf['prediction_performance_label']\n    y2_label = "Time (s)"\n    host.set_ylabel(y1_label)\n    par1.set_ylabel(y2_label)\n    p1, = host.plot(complexities, mse_values, 'b-', label="prediction error")\n    p2, = par1.plot(complexities, prediction_times, 'r-',\n                    label="latency")\n    host.legend(loc='upper right')\n    host.axis["left"].label.set_color(p1.get_color())\n    par1.axis["right"].label.set_color(p2.get_color())\n    plt.title('Influence of Model Complexity - %s' % conf['estimator'].__name__)\n    plt.show()\n\n\ndef _count_nonzero_coefficients(estimator):\n    a = estimator.coef_.toarray()\n    return np.count_nonzero(a)\n\n###############################################################################\n# main code\nregression_data = generate_data('regression')\nclassification_data = generate_data('classification', sparse=True)\nconfigurations = [\n    {'estimator': SGDClassifier,\n     'tuned_params': {'penalty': 'elasticnet', 'alpha': 0.001, 'loss':\n                      'modified_huber', 'fit_intercept': True},\n     'changing_param': 'l1_ratio',\n     'changing_param_values': [0.25, 0.5, 0.75, 0.9],\n     'complexity_label': 'non_zero coefficients',\n     'complexity_computer': _count_nonzero_coefficients,\n     'prediction_performance_computer': hamming_loss,\n     'prediction_performance_label': 'Hamming Loss (Misclassification Ratio)',\n     'postfit_hook': lambda x: x.sparsify(),\n     'data': classification_data,\n     'n_samples': 30},\n    {'estimator': NuSVR,\n     'tuned_params': {'C': 1e3, 'gamma': 2 ** -15},\n     'changing_param': 'nu',\n     'changing_param_values': [0.1, 0.25, 0.5, 0.75, 0.9],\n     'complexity_label': 'n_support_vectors',\n     'complexity_computer': lambda x: len(x.support_vectors_),\n     'data': regression_data,\n     'postfit_hook': lambda x: x,\n     'prediction_performance_computer': mean_squared_error,\n     'prediction_performance_label': 'MSE',\n     'n_samples': 30},\n    {'estimator': GradientBoostingRegressor,\n     'tuned_params': {'loss': 'ls'},\n     'changing_param': 'n_estimators',\n     'changing_param_values': [10, 50, 100, 200, 500],\n     'complexity_label': 'n_trees',\n     'complexity_computer': lambda x: x.n_estimators,\n     'data': regression_data,\n     'postfit_hook': lambda x: x,\n     'prediction_performance_computer': mean_squared_error,\n     'prediction_performance_label': 'MSE',\n     'n_samples': 30},\n]\nfor conf in configurations:\n    prediction_performances, prediction_times, complexities = \\n        benchmark_influence(conf)\n    plot_influence(conf, prediction_performances, prediction_times,\n                   complexities)</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_model_complexity_influence.html
Joint feature selection with multi-task Lasso	A							http://scikit-learn.org/stable/auto_examples/index.html			The multi-task lasso allows to fit multiple regression problems jointly enforcing the selected features to be the same across tasks. This example simulates sequential measurements, each task is a time instant, and the relevant features vary in amplitude over time while being the same. The multi-task lasso imposes that features that are selected at one time point are select for all time point....<br><br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import MultiTaskLasso, Lasso\n\nrng = np.random.RandomState(42)\n\n# Generate some 2D coefficients with sine waves with random frequency and phase\nn_samples, n_features, n_tasks = 100, 30, 40\nn_relevant_features = 5\ncoef = np.zeros((n_tasks, n_features))\ntimes = np.linspace(0, 2 * np.pi, n_tasks)\nfor k in range(n_relevant_features):\n    coef[:, k] = np.sin((1. + rng.randn(1)) * times + 3 * rng.randn(1))\n\nX = rng.randn(n_samples, n_features)\nY = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)\n\ncoef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T])\ncoef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X, Y).coef_\n\n###############################################################################\n# Plot support and time series\nfig = plt.figure(figsize=(8, 5))\nplt.subplot(1, 2, 1)\nplt.spy(coef_lasso_)\nplt.xlabel('Feature')\nplt.ylabel('Time (or Task)')\nplt.text(10, 5, 'Lasso')\nplt.subplot(1, 2, 2)\nplt.spy(coef_multi_task_lasso_)\nplt.xlabel('Feature')\nplt.ylabel('Time (or Task)')\nplt.text(10, 5, 'MultiTaskLasso')\nfig.suptitle('Coefficient non-zero location')\n\nfeature_to_plot = 0\nplt.figure()\nplt.plot(coef[:, feature_to_plot], 'k', label='Ground truth')\nplt.plot(coef_lasso_[:, feature_to_plot], 'g', label='Lasso')\nplt.plot(coef_multi_task_lasso_[:, feature_to_plot],\n         'r', label='MultiTaskLasso')\nplt.legend(loc='upper center')\nplt.axis('tight')\nplt.ylim([-1.1, 1.1])\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_multi_task_lasso_support.html
Multilabel classification	A							http://scikit-learn.org/stable/auto_examples/index.html			This example simulates a multi-label document classification problem. The dataset is generated randomly based on the following process:<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import CCA\n\n\ndef plot_hyperplane(clf, min_x, max_x, linestyle, label):\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n    plt.plot(xx, yy, linestyle, label=label)\n\n\ndef plot_subfigure(X, Y, subplot, title, transform):\n    if transform == "pca":\n        X = PCA(n_components=2).fit_transform(X)\n    elif transform == "cca":\n        X = CCA(n_components=2).fit(X, Y).transform(X)\n    else:\n        raise ValueError\n\n    min_x = np.min(X[:, 0])\n    max_x = np.max(X[:, 0])\n\n    min_y = np.min(X[:, 1])\n    max_y = np.max(X[:, 1])\n\n    classif = OneVsRestClassifier(SVC(kernel='linear'))\n    classif.fit(X, Y)\n\n    plt.subplot(2, 2, subplot)\n    plt.title(title)\n\n    zero_class = np.where(Y[:, 0])\n    one_class = np.where(Y[:, 1])\n    plt.scatter(X[:, 0], X[:, 1], s=40, c='gray')\n    plt.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors='b',\n               facecolors='none', linewidths=2, label='Class 1')\n    plt.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors='orange',\n               facecolors='none', linewidths=2, label='Class 2')\n\n    plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',\n                    'Boundary\nfor class 1')\n    plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',\n                    'Boundary\nfor class 2')\n    plt.xticks(())\n    plt.yticks(())\n\n    plt.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n    plt.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n    if subplot == 2:\n        plt.xlabel('First principal component')\n        plt.ylabel('Second principal component')\n        plt.legend(loc="upper left")\n\n\nplt.figure(figsize=(8, 6))\n\nX, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                      allow_unlabeled=True,\n                                      random_state=1)\n\nplot_subfigure(X, Y, 1, "With unlabeled samples + CCA", "cca")\nplot_subfigure(X, Y, 2, "With unlabeled samples + PCA", "pca")\n\nX, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                      allow_unlabeled=False,\n                                      random_state=1)\n\nplot_subfigure(X, Y, 3, "Without unlabeled samples + CCA", "cca")\nplot_subfigure(X, Y, 4, "Without unlabeled samples + PCA", "pca")\n\nplt.subplots_adjust(.04, .02, .97, .94, .09, .2)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_multilabel.html
Face completion with a multi-output estimators	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows the use of multi-output estimator to complete images. The goal is to predict the lower half of a face given its upper half.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.utils.validation import check_random_state\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RidgeCV\n\n# Load the faces datasets\ndata = fetch_olivetti_faces()\ntargets = data.target\n\ndata = data.images.reshape((len(data.images), -1))\ntrain = data[targets < 30]\ntest = data[targets >= 30]  # Test on independent people\n\n# Test on a subset of people\nn_faces = 5\nrng = check_random_state(4)\nface_ids = rng.randint(test.shape[0], size=(n_faces, ))\ntest = test[face_ids, :]\n\nn_pixels = data.shape[1]\nX_train = train[:, :np.ceil(0.5 * n_pixels)]  # Upper half of the faces\ny_train = train[:, np.floor(0.5 * n_pixels):]  # Lower half of the faces\nX_test = test[:, :np.ceil(0.5 * n_pixels)]\ny_test = test[:, np.floor(0.5 * n_pixels):]\n\n# Fit estimators\nESTIMATORS = {\n    "Extra trees": ExtraTreesRegressor(n_estimators=10, max_features=32,\n                                       random_state=0),\n    "K-nn": KNeighborsRegressor(),\n    "Linear regression": LinearRegression(),\n    "Ridge": RidgeCV(),\n}\n\ny_test_predict = dict()\nfor name, estimator in ESTIMATORS.items():\n    estimator.fit(X_train, y_train)\n    y_test_predict[name] = estimator.predict(X_test)\n\n# Plot the completed faces\nimage_shape = (64, 64)\n\nn_cols = 1 + len(ESTIMATORS)\nplt.figure(figsize=(2. * n_cols, 2.26 * n_faces))\nplt.suptitle("Face completion with multi-output estimators", size=16)\n\nfor i in range(n_faces):\n    true_face = np.hstack((X_test[i], y_test[i]))\n\n    if i:\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1)\n    else:\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1,\n                          title="true faces")\n\n\n    sub.axis("off")\n    sub.imshow(true_face.reshape(image_shape),\n               cmap=plt.cm.gray,\n               interpolation="nearest")\n\n    for j, est in enumerate(sorted(ESTIMATORS)):\n        completed_face = np.hstack((X_test[i], y_test_predict[est][i]))\n\n        if i:\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j)\n\n        else:\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j,\n                              title=est)\n\n        sub.axis("off")\n        sub.imshow(completed_face.reshape(image_shape),\n                   cmap=plt.cm.gray,\n                   interpolation="nearest")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html
Nearest Centroid Classification	A							http://scikit-learn.org/stable/auto_examples/index.html			Sample usage of Nearest Centroid classification. It will plot the decision boundaries for each class.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.neighbors import NearestCentroid\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\nfor shrinkage in [None, 0.1]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = NearestCentroid(shrink_threshold=shrinkage)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    print(shrinkage, np.mean(y == y_pred))\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.title("3-Class classification (shrink_threshold=%r)"\n              % shrinkage)\n    plt.axis('tight')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_nearest_centroid.html
Linear Regression Example	A							http://scikit-learn.org/stable/auto_examples/index.html			This example uses the only the first feature of the diabetes dataset, in order to illustrate a two-dimensional plot of this regression technique. The straight line can be seen in the plot, showing how linear regression attempts to draw a straight line that will best minimize the residual sum of squares between the observed responses in the dataset, and the responses predicted by the linear...<br><br><pre><code>print(__doc__)\n\n\n# Code source: Jaques Grobler\n# License: BSD 3 clause\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\n\n# Load the diabetes dataset\ndiabetes = datasets.load_diabetes()\n\n\n# Use only one feature\ndiabetes_X = diabetes.data[:, np.newaxis, 2]\n\n# Split the data into training/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training/testing sets\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# The coefficients\nprint('Coefficients: \n', regr.coef_)\n# The mean square error\nprint("Residual sum of squares: %.2f"\n      % np.mean((regr.predict(diabetes_X_test) - diabetes_y_test) ** 2))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(diabetes_X_test, diabetes_y_test))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, regr.predict(diabetes_X_test), color='blue',\n         linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html
Sparsity Example: Fitting only features 1  and 2	A							http://scikit-learn.org/stable/auto_examples/index.html			Features 1 and 2 of the diabetes-dataset are fitted and plotted below. It illustrates that although feature 2 has a strong coefficient on the full model, it does not give us much regarding y when compared to just feature 1<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn import datasets, linear_model\n\ndiabetes = datasets.load_diabetes()\nindices = (0, 1)\n\nX_train = diabetes.data[:-20, indices]\nX_test = diabetes.data[-20:, indices]\ny_train = diabetes.target[:-20]\ny_test = diabetes.target[-20:]\n\nols = linear_model.LinearRegression()\nols.fit(X_train, y_train)\n\n\n###############################################################################\n# Plot the figure\ndef plot_figs(fig_num, elev, azim, X_train, clf):\n    fig = plt.figure(fig_num, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, elev=elev, azim=azim)\n\n    ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c='k', marker='+')\n    ax.plot_surface(np.array([[-.1, -.1], [.15, .15]]),\n                    np.array([[-.1, .15], [-.1, .15]]),\n                    clf.predict(np.array([[-.1, -.1, .15, .15],\n                                          [-.1, .15, -.1, .15]]).T\n                                ).reshape((2, 2)),\n                    alpha=.5)\n    ax.set_xlabel('X_1')\n    ax.set_ylabel('X_2')\n    ax.set_zlabel('Y')\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n\n#Generate the three different figures from different views\nelev = 43.5\nazim = -110\nplot_figs(1, elev, azim, X_train, ols)\n\nelev = -.5\nazim = 0\nplot_figs(2, elev, azim, X_train, ols)\n\nelev = -.5\nazim = 90\nplot_figs(3, elev, azim, X_train, ols)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_3d.html
Ordinary Least Squares and Ridge Regression Variance	A							http://scikit-learn.org/stable/auto_examples/index.html			Due to the few points in each dimension and the straight line that linear regression uses to follow these points as well as it can, noise on the observations will cause great variance as shown in the first plot. Every line’s slope can vary quite a bit for each prediction due to the noise induced in the observations.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\nX_train = np.c_[.5, 1].T\ny_train = [.5, 1]\nX_test = np.c_[0, 2].T\n\nnp.random.seed(0)\n\nclassifiers = dict(ols=linear_model.LinearRegression(),\n                   ridge=linear_model.Ridge(alpha=.1))\n\nfignum = 1\nfor name, clf in classifiers.items():\n    fig = plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    plt.title(name)\n    ax = plt.axes([.12, .12, .8, .8])\n\n    for _ in range(6):\n        this_X = .1 * np.random.normal(size=(2, 1)) + X_train\n        clf.fit(this_X, y_train)\n\n        ax.plot(X_test, clf.predict(X_test), color='.5')\n        ax.scatter(this_X, y_train, s=3, c='.5', marker='o', zorder=10)\n\n    clf.fit(X_train, y_train)\n    ax.plot(X_test, clf.predict(X_test), linewidth=2, color='blue')\n    ax.scatter(X_train, y_train, s=30, c='r', marker='+', zorder=10)\n\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_ylim((0, 1.6))\n    ax.set_xlabel('X')\n    ax.set_ylabel('y')\n    ax.set_xlim(0, 2)\n    fignum += 1\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html
Orthogonal Matching Pursuit	A							http://scikit-learn.org/stable/auto_examples/index.html			Using orthogonal matching pursuit for recovering a sparse signal from a noisy measurement encoded with a dictionary<br><br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\nfrom sklearn.linear_model import OrthogonalMatchingPursuitCV\nfrom sklearn.datasets import make_sparse_coded_signal\n\nn_components, n_features = 512, 100\nn_nonzero_coefs = 17\n\n# generate the data\n###################\n\n# y = Xw\n# |x|_0 = n_nonzero_coefs\n\ny, X, w = make_sparse_coded_signal(n_samples=1,\n                                   n_components=n_components,\n                                   n_features=n_features,\n                                   n_nonzero_coefs=n_nonzero_coefs,\n                                   random_state=0)\n\nidx, = w.nonzero()\n\n# distort the clean signal\n##########################\ny_noisy = y + 0.05 * np.random.randn(len(y))\n\n# plot the sparse signal\n########################\nplt.figure(figsize=(7, 7))\nplt.subplot(4, 1, 1)\nplt.xlim(0, 512)\nplt.title("Sparse signal")\nplt.stem(idx, w[idx])\n\n# plot the noise-free reconstruction\n####################################\n\nomp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\nomp.fit(X, y)\ncoef = omp.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 2)\nplt.xlim(0, 512)\nplt.title("Recovered signal from noise-free measurements")\nplt.stem(idx_r, coef[idx_r])\n\n# plot the noisy reconstruction\n###############################\nomp.fit(X, y_noisy)\ncoef = omp.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 3)\nplt.xlim(0, 512)\nplt.title("Recovered signal from noisy measurements")\nplt.stem(idx_r, coef[idx_r])\n\n# plot the noisy reconstruction with number of non-zeros set by CV\n##################################################################\nomp_cv = OrthogonalMatchingPursuitCV()\nomp_cv.fit(X, y_noisy)\ncoef = omp_cv.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 4)\nplt.xlim(0, 512)\nplt.title("Recovered signal from noisy measurements with CV")\nplt.stem(idx_r, coef[idx_r])\n\nplt.subplots_adjust(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)\nplt.suptitle('Sparse signal recovery with Orthogonal Matching Pursuit',\n             fontsize=16)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_omp.html
One-class SVM with non-linear kernel (RBF)	A							http://scikit-learn.org/stable/auto_examples/index.html			An example using a one-class SVM for novelty detection.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn import svm\n\nxx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n# Generate train data\nX = 0.3 * np.random.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * np.random.randn(20, 2)\nX_test = np.r_[X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n\n# fit the model\nclf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_train = y_pred_train[y_pred_train == -1].size\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\n# plot the line, the points, and the nearest vectors to the plane\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title("Novelty Detection")\nplt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)\na = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='red')\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='orange')\n\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\nplt.axis('tight')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend([a.collections[0], b1, b2, c],\n           ["learned frontier", "training observations",\n            "new regular observations", "new abnormal observations"],\n           loc="upper left",\n           prop=matplotlib.font_manager.FontProperties(size=11))\nplt.xlabel(\n    "error train: %d/200 ; errors novel regular: %d/40 ; "\n    "errors novel abnormal: %d/40"\n    % (n_error_train, n_error_test, n_error_outliers))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html
Out-of-core classification of text documents	A							http://scikit-learn.org/stable/auto_examples/index.html			This is an example showing how scikit-learn can be used for classification using an out-of-core approach: learning from data that doesn’t fit into main memory. We make use of an online classifier, i.e., one that supports the partial_fit method, that will be fed with batches of examples. To guarantee that the features space remains the same over time we leverage a HashingVectorizer that will...<br><br><pre><code># Authors: Eustache Diemert <eustache@diemert.fr>\n#          @FedericoV <https://github.com/FedericoV/>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom glob import glob\nimport itertools\nimport os.path\nimport re\nimport tarfile\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nfrom sklearn.externals.six.moves import html_parser\nfrom sklearn.externals.six.moves import urllib\nfrom sklearn.datasets import get_data_home\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.naive_bayes import MultinomialNB\n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return '__file__' in globals()\n\n\n###############################################################################\n# Reuters Dataset related routines\n###############################################################################\n\n\nclass ReutersParser(html_parser.HTMLParser):\n    """Utility class to parse a SGML file and yield documents one at a time."""\n\n    def __init__(self, encoding='latin-1'):\n        html_parser.HTMLParser.__init__(self)\n        self._reset()\n        self.encoding = encoding\n\n    def handle_starttag(self, tag, attrs):\n        method = 'start_' + tag\n        getattr(self, method, lambda x: None)(attrs)\n\n    def handle_endtag(self, tag):\n        method = 'end_' + tag\n        getattr(self, method, lambda: None)()\n\n    def _reset(self):\n        self.in_title = 0\n        self.in_body = 0\n        self.in_topics = 0\n        self.in_topic_d = 0\n        self.title = ""\n        self.body = ""\n        self.topics = []\n        self.topic_d = ""\n\n    def parse(self, fd):\n        self.docs = []\n        for chunk in fd:\n            self.feed(chunk.decode(self.encoding))\n            for doc in self.docs:\n                yield doc\n            self.docs = []\n        self.close()\n\n    def handle_data(self, data):\n        if self.in_body:\n            self.body += data\n        elif self.in_title:\n            self.title += data\n        elif self.in_topic_d:\n            self.topic_d += data\n\n    def start_reuters(self, attributes):\n        pass\n\n    def end_reuters(self):\n        self.body = re.sub(r'\s+', r' ', self.body)\n        self.docs.append({'title': self.title,\n                          'body': self.body,\n                          'topics': self.topics})\n        self._reset()\n\n    def start_title(self, attributes):\n        self.in_title = 1\n\n    def end_title(self):\n        self.in_title = 0\n\n    def start_body(self, attributes):\n        self.in_body = 1\n\n    def end_body(self):\n        self.in_body = 0\n\n    def start_topics(self, attributes):\n        self.in_topics = 1\n\n    def end_topics(self):\n        self.in_topics = 0\n\n    def start_d(self, attributes):\n        self.in_topic_d = 1\n\n    def end_d(self):\n        self.in_topic_d = 0\n        self.topics.append(self.topic_d)\n        self.topic_d = ""\n\n\ndef stream_reuters_documents(data_path=None):\n    """Iterate over documents of the Reuters dataset.\n\n    The Reuters archive will automatically be downloaded and uncompressed if\n    the `data_path` directory does not exist.\n\n    Documents are represented as dictionaries with 'body' (str),\n    'title' (str), 'topics' (list(str)) keys.\n\n    """\n\n    DOWNLOAD_URL = ('http://archive.ics.uci.edu/ml/machine-learning-databases/'\n                    'reuters21578-mld/reuters21578.tar.gz')\n    ARCHIVE_FILENAME = 'reuters21578.tar.gz'\n\n    if data_path is None:\n        data_path = os.path.join(get_data_home(), "reuters")\n    if not os.path.exists(data_path):\n        """Download the dataset."""\n        print("downloading dataset (once and for all) into %s" %\n              data_path)\n        os.mkdir(data_path)\n\n        def progress(blocknum, bs, size):\n            total_sz_mb = '%.2f MB' % (size / 1e6)\n            current_sz_mb = '%.2f MB' % ((blocknum * bs) / 1e6)\n            if _not_in_sphinx():\n                print('\rdownloaded %s / %s' % (current_sz_mb, total_sz_mb),\n                      end='')\n\n        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n        urllib.request.urlretrieve(DOWNLOAD_URL, filename=archive_path,\n                                   reporthook=progress)\n        if _not_in_sphinx():\n            print('\r', end='')\n        print("untarring Reuters dataset...")\n        tarfile.open(archive_path, 'r:gz').extractall(data_path)\n        print("done.")\n\n    parser = ReutersParser()\n    for filename in glob(os.path.join(data_path, "*.sgm")):\n        for doc in parser.parse(open(filename, 'rb')):\n            yield doc\n\n\n###############################################################################\n# Main\n###############################################################################\n# Create the vectorizer and limit the number of features to a reasonable\n# maximum\nvectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,\n                               non_negative=True)\n\n\n# Iterator over parsed Reuters SGML files.\ndata_stream = stream_reuters_documents()\n\n# We learn a binary classification between the "acq" class and all the others.\n# "acq" was chosen as it is more or less evenly distributed in the Reuters\n# files. For other datasets, one should take care of creating a test set with\n# a realistic portion of positive instances.\nall_classes = np.array([0, 1])\npositive_class = 'acq'\n\n# Here are some classifiers that support the `partial_fit` method\npartial_fit_classifiers = {\n    'SGD': SGDClassifier(),\n    'Perceptron': Perceptron(),\n    'NB Multinomial': MultinomialNB(alpha=0.01),\n    'Passive-Aggressive': PassiveAggressiveClassifier(),\n}\n\n\ndef get_minibatch(doc_iter, size, pos_class=positive_class):\n    """Extract a minibatch of examples, return a tuple X_text, y.\n\n    Note: size is before excluding invalid docs with no topics assigned.\n\n    """\n    data = [(u'{title}\n\n{body}'.format(**doc), pos_class in doc['topics'])\n            for doc in itertools.islice(doc_iter, size)\n            if doc['topics']]\n    if not len(data):\n        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n    X_text, y = zip(*data)\n    return X_text, np.asarray(y, dtype=int)\n\n\ndef iter_minibatches(doc_iter, minibatch_size):\n    """Generator of minibatches."""\n    X_text, y = get_minibatch(doc_iter, minibatch_size)\n    while len(X_text):\n        yield X_text, y\n        X_text, y = get_minibatch(doc_iter, minibatch_size)\n\n\n# test data statistics\ntest_stats = {'n_test': 0, 'n_test_pos': 0}\n\n# First we hold out a number of examples to estimate accuracy\nn_test_documents = 1000\ntick = time.time()\nX_test_text, y_test = get_minibatch(data_stream, 1000)\nparsing_time = time.time() - tick\ntick = time.time()\nX_test = vectorizer.transform(X_test_text)\nvectorizing_time = time.time() - tick\ntest_stats['n_test'] += len(y_test)\ntest_stats['n_test_pos'] += sum(y_test)\nprint("Test set is %d documents (%d positive)" % (len(y_test), sum(y_test)))\n\n\ndef progress(cls_name, stats):\n    """Report progress information, return a string."""\n    duration = time.time() - stats['t0']\n    s = "%20s classifier : \t" % cls_name\n    s += "%(n_train)6d train docs (%(n_train_pos)6d positive) " % stats\n    s += "%(n_test)6d test docs (%(n_test_pos)6d positive) " % test_stats\n    s += "accuracy: %(accuracy).3f " % stats\n    s += "in %.2fs (%5d docs/s)" % (duration, stats['n_train'] / duration)\n    return s\n\n\ncls_stats = {}\n\nfor cls_name in partial_fit_classifiers:\n    stats = {'n_train': 0, 'n_train_pos': 0,\n             'accuracy': 0.0, 'accuracy_history': [(0, 0)], 't0': time.time(),\n             'runtime_history': [(0, 0)], 'total_fit_time': 0.0}\n    cls_stats[cls_name] = stats\n\nget_minibatch(data_stream, n_test_documents)\n# Discard test set\n\n# We will feed the classifier with mini-batches of 1000 documents; this means\n# we have at most 1000 docs in memory at any time.  The smaller the document\n# batch, the bigger the relative overhead of the partial fit methods.\nminibatch_size = 1000\n\n# Create the data_stream that parses Reuters SGML files and iterates on\n# documents as a stream.\nminibatch_iterators = iter_minibatches(data_stream, minibatch_size)\ntotal_vect_time = 0.0\n\n# Main loop : iterate on mini-batchs of examples\nfor i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n\n    tick = time.time()\n    X_train = vectorizer.transform(X_train_text)\n    total_vect_time += time.time() - tick\n\n    for cls_name, cls in partial_fit_classifiers.items():\n        tick = time.time()\n        # update estimator with examples in the current mini-batch\n        cls.partial_fit(X_train, y_train, classes=all_classes)\n\n        # accumulate test accuracy stats\n        cls_stats[cls_name]['total_fit_time'] += time.time() - tick\n        cls_stats[cls_name]['n_train'] += X_train.shape[0]\n        cls_stats[cls_name]['n_train_pos'] += sum(y_train)\n        tick = time.time()\n        cls_stats[cls_name]['accuracy'] = cls.score(X_test, y_test)\n        cls_stats[cls_name]['prediction_time'] = time.time() - tick\n        acc_history = (cls_stats[cls_name]['accuracy'],\n                       cls_stats[cls_name]['n_train'])\n        cls_stats[cls_name]['accuracy_history'].append(acc_history)\n        run_history = (cls_stats[cls_name]['accuracy'],\n                       total_vect_time + cls_stats[cls_name]['total_fit_time'])\n        cls_stats[cls_name]['runtime_history'].append(run_history)\n\n        if i % 3 == 0:\n            print(progress(cls_name, cls_stats[cls_name]))\n    if i % 3 == 0:\n        print('\n')\n\n\n###############################################################################\n# Plot results\n###############################################################################\n\n\ndef plot_accuracy(x, y, x_legend):\n    """Plot accuracy as a function of x."""\n    x = np.array(x)\n    y = np.array(y)\n    plt.title('Classification accuracy as a function of %s' % x_legend)\n    plt.xlabel('%s' % x_legend)\n    plt.ylabel('Accuracy')\n    plt.grid(True)\n    plt.plot(x, y)\n\nrcParams['legend.fontsize'] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\nplt.figure()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats['accuracy_history'])\n    plot_accuracy(n_examples, accuracy, "training examples (#)")\n    ax = plt.gca()\n    ax.set_ylim((0.8, 1))\nplt.legend(cls_names, loc='best')\n\nplt.figure()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats['runtime_history'])\n    plot_accuracy(runtime, accuracy, 'runtime (s)')\n    ax = plt.gca()\n    ax.set_ylim((0.8, 1))\nplt.legend(cls_names, loc='best')\n\n# Plot fitting times\nplt.figure()\nfig = plt.gcf()\ncls_runtime = []\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats['total_fit_time'])\n\ncls_runtime.append(total_vect_time)\ncls_names.append('Vectorization')\nbar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n\nax = plt.subplot(111)\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n                     color=bar_colors)\n\nax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel('runtime (s)')\nax.set_title('Training Times')\n\n\ndef autolabel(rectangles):\n    """attach some text vi autolabel on rectangles."""\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width() / 2.,\n                1.05 * height, '%.4f' % height,\n                ha='center', va='bottom')\n\nautolabel(rectangles)\nplt.show()\n\n# Plot prediction times\nplt.figure()\n#fig = plt.gcf()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats['prediction_time'])\ncls_runtime.append(parsing_time)\ncls_names.append('Read/Parse\n+Feat.Extr.')\ncls_runtime.append(vectorizing_time)\ncls_names.append('Hashing\n+Vect.')\nbar_colors = rcParams['axes.color_cycle'][:len(cls_names)]\n\nax = plt.subplot(111)\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n                     color=bar_colors)\n\nax.set_xticks(np.linspace(0.25, len(cls_names) - 0.75, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\nplt.setp(plt.xticks()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel('runtime (s)')\nax.set_title('Prediction Times (%d instances)' % n_test_documents)\nautolabel(rectangles)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html
Outlier detection with several methods.	A							http://scikit-learn.org/stable/auto_examples/index.html			When the amount of contamination is known, this example illustrates two different ways of performing Novelty and Outlier Detection:<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom scipy import stats\n\nfrom sklearn import svm\nfrom sklearn.covariance import EllipticEnvelope\n\n# Example settings\nn_samples = 200\noutliers_fraction = 0.25\nclusters_separation = [0, 1, 2]\n\n# define two outlier detection tools to be compared\nclassifiers = {\n    "One-Class SVM": svm.OneClassSVM(nu=0.95 * outliers_fraction + 0.05,\n                                     kernel="rbf", gamma=0.1),\n    "robust covariance estimator": EllipticEnvelope(contamination=.1)}\n\n# Compare given classifiers under given settings\nxx, yy = np.meshgrid(np.linspace(-7, 7, 500), np.linspace(-7, 7, 500))\nn_inliers = int((1. - outliers_fraction) * n_samples)\nn_outliers = int(outliers_fraction * n_samples)\nground_truth = np.ones(n_samples, dtype=int)\nground_truth[-n_outliers:] = 0\n\n# Fit the problem with varying cluster separation\nfor i, offset in enumerate(clusters_separation):\n    np.random.seed(42)\n    # Data generation\n    X1 = 0.3 * np.random.randn(0.5 * n_inliers, 2) - offset\n    X2 = 0.3 * np.random.randn(0.5 * n_inliers, 2) + offset\n    X = np.r_[X1, X2]\n    # Add outliers\n    X = np.r_[X, np.random.uniform(low=-6, high=6, size=(n_outliers, 2))]\n\n    # Fit the model with the One-Class SVM\n    plt.figure(figsize=(10, 5))\n    for i, (clf_name, clf) in enumerate(classifiers.items()):\n        # fit the data and tag outliers\n        clf.fit(X)\n        y_pred = clf.decision_function(X).ravel()\n        threshold = stats.scoreatpercentile(y_pred,\n                                            100 * outliers_fraction)\n        y_pred = y_pred > threshold\n        n_errors = (y_pred != ground_truth).sum()\n        # plot the levels lines and the points\n        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        Z = Z.reshape(xx.shape)\n        subplot = plt.subplot(1, 2, i + 1)\n        subplot.set_title("Outlier detection")\n        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n                         cmap=plt.cm.Blues_r)\n        a = subplot.contour(xx, yy, Z, levels=[threshold],\n                            linewidths=2, colors='red')\n        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n                         colors='orange')\n        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white')\n        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black')\n        subplot.axis('tight')\n        subplot.legend(\n            [a.collections[0], b, c],\n            ['learned decision function', 'true inliers', 'true outliers'],\n            prop=matplotlib.font_manager.FontProperties(size=11))\n        subplot.set_xlabel("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))\n        subplot.set_xlim((-7, 7))\n        subplot.set_ylim((-7, 7))\n    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html
Outlier detection on a real data set	A							http://scikit-learn.org/stable/auto_examples/index.html			This example illustrates the need for robust covariance estimation on a real data set. It is useful both for outlier detection and for a better understanding of the data structure.<br><br><pre><code>print(__doc__)\n\n# Author: Virgile Fritsch <virgile.fritsch@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.svm import OneClassSVM\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn.datasets import load_boston\n\n# Get data\nX1 = load_boston()['data'][:, [8, 10]]  # two clusters\nX2 = load_boston()['data'][:, [5, 12]]  # "banana"-shaped\n\n# Define "classifiers" to be used\nclassifiers = {\n    "Empirical Covariance": EllipticEnvelope(support_fraction=1.,\n                                             contamination=0.261),\n    "Robust Covariance (Minimum Covariance Determinant)":\n    EllipticEnvelope(contamination=0.261),\n    "OCSVM": OneClassSVM(nu=0.261, gamma=0.05)}\ncolors = ['m', 'g', 'b']\nlegend1 = {}\nlegend2 = {}\n\n# Learn a frontier for outlier detection with several classifiers\nxx1, yy1 = np.meshgrid(np.linspace(-8, 28, 500), np.linspace(3, 40, 500))\nxx2, yy2 = np.meshgrid(np.linspace(3, 10, 500), np.linspace(-5, 45, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    plt.figure(1)\n    clf.fit(X1)\n    Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])\n    Z1 = Z1.reshape(xx1.shape)\n    legend1[clf_name] = plt.contour(\n        xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])\n    plt.figure(2)\n    clf.fit(X2)\n    Z2 = clf.decision_function(np.c_[xx2.ravel(), yy2.ravel()])\n    Z2 = Z2.reshape(xx2.shape)\n    legend2[clf_name] = plt.contour(\n        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i])\n\nlegend1_values_list = list( legend1.values() )\nlegend1_keys_list = list( legend1.keys() )\n\n# Plot the results (= shape of the data points cloud)\nplt.figure(1)  # two clusters\nplt.title("Outlier detection on a real data set (boston housing)")\nplt.scatter(X1[:, 0], X1[:, 1], color='black')\nbbox_args = dict(boxstyle="round", fc="0.8")\narrow_args = dict(arrowstyle="->")\nplt.annotate("several confounded points", xy=(24, 19),\n             xycoords="data", textcoords="data",\n             xytext=(13, 10), bbox=bbox_args, arrowprops=arrow_args)\nplt.xlim((xx1.min(), xx1.max()))\nplt.ylim((yy1.min(), yy1.max()))\nplt.legend((legend1_values_list[0].collections[0],\n            legend1_values_list[1].collections[0],\n            legend1_values_list[2].collections[0]),\n           (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),\n           loc="upper center",\n           prop=matplotlib.font_manager.FontProperties(size=12))\nplt.ylabel("accessibility to radial highways")\nplt.xlabel("pupil-teacher ratio by town")\n\nlegend2_values_list = list( legend2.values() )\nlegend2_keys_list = list( legend2.keys() )\n\nplt.figure(2)  # "banana" shape\nplt.title("Outlier detection on a real data set (boston housing)")\nplt.scatter(X2[:, 0], X2[:, 1], color='black')\nplt.xlim((xx2.min(), xx2.max()))\nplt.ylim((yy2.min(), yy2.max()))\nplt.legend((legend2_values_list[0].collections[0],\n            legend2_values_list[1].collections[0],\n            legend2_values_list[2].collections[0]),\n           (legend2_values_list[0], legend2_values_list[1], legend2_values_list[2]),\n           loc="upper center",\n           prop=matplotlib.font_manager.FontProperties(size=12))\nplt.ylabel("% lower status of the population")\nplt.xlabel("average number of rooms per dwelling")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_outlier_detection_housing.html
Partial Dependence Plots	A							http://scikit-learn.org/stable/auto_examples/index.html			Partial dependence plots show the dependence between the target function [2] and a set of ‘target’ features, marginalizing over the values of all other features (the complement features). Due to the limits of human perception the size of the target feature set must be small (usually, one or two) thus the target features are usually chosen among the most important features (see...<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.ensemble.partial_dependence import partial_dependence\nfrom sklearn.datasets.california_housing import fetch_california_housing\n\n\ndef main():\n    cal_housing = fetch_california_housing()\n\n    # split 80/20 train-test\n    X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n                                                        cal_housing.target,\n                                                        test_size=0.2,\n                                                        random_state=1)\n    names = cal_housing.feature_names\n\n    print('_' * 80)\n    print("Training GBRT...")\n    clf = GradientBoostingRegressor(n_estimators=100, max_depth=4,\n                                    learning_rate=0.1, loss='huber',\n                                    random_state=1)\n    clf.fit(X_train, y_train)\n    print("done.")\n\n    print('_' * 80)\n    print('Convenience plot with ``partial_dependence_plots``')\n    print\n\n    features = [0, 5, 1, 2, (5, 1)]\n    fig, axs = plot_partial_dependence(clf, X_train, features,\n                                       feature_names=names,\n                                       n_jobs=3, grid_resolution=50)\n    fig.suptitle('Partial dependence of house value on nonlocation features\n'\n                 'for the California housing dataset')\n    plt.subplots_adjust(top=0.9)  # tight_layout causes overlap with suptitle\n\n    print('_' * 80)\n    print('Custom 3d plot via ``partial_dependence``')\n    print\n    fig = plt.figure()\n\n    target_feature = (1, 5)\n    pdp, (x_axis, y_axis) = partial_dependence(clf, target_feature,\n                                               X=X_train, grid_resolution=50)\n    XX, YY = np.meshgrid(x_axis, y_axis)\n    Z = pdp.T.reshape(XX.shape).T\n    ax = Axes3D(fig)\n    surf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1, cmap=plt.cm.BuPu)\n    ax.set_xlabel(names[target_feature[0]])\n    ax.set_ylabel(names[target_feature[1]])\n    ax.set_zlabel('Partial dependence')\n    #  pretty init view\n    ax.view_init(elev=22, azim=122)\n    plt.colorbar(surf)\n    plt.suptitle('Partial dependence of house value on median age and '\n                 'average occupancy')\n    plt.subplots_adjust(top=0.9)\n\n    plt.show()\n\n\n# Needed on Windows because plot_partial_dependence uses multiprocessing\nif __name__ == '__main__':\n    main()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_partial_dependence.html
Principal components analysis (PCA)	A							http://scikit-learn.org/stable/auto_examples/index.html			These figures aid in illustrating how a point cloud can be very flat in one direction–which is where PCA comes in to choose a direction that is not flat.<br><br><pre><code>print(__doc__)\n\n# Authors: Gael Varoquaux\n#          Jaques Grobler\n#          Kevin Hughes\n# License: BSD 3 clause\n\nfrom sklearn.decomposition import PCA\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\n###############################################################################\n# Create the data\n\ne = np.exp(1)\nnp.random.seed(4)\n\n\ndef pdf(x):\n    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x)\n                  + stats.norm(scale=4 / e).pdf(x))\n\ny = np.random.normal(scale=0.5, size=(30000))\nx = np.random.normal(scale=0.5, size=(30000))\nz = np.random.normal(scale=0.1, size=len(x))\n\ndensity = pdf(x) * pdf(y)\npdf_z = pdf(5 * z)\n\ndensity *= pdf_z\n\na = x + y\nb = 2 * y\nc = a - b + z\n\nnorm = np.sqrt(a.var() + b.var())\na /= norm\nb /= norm\n\n\n###############################################################################\n# Plot the figures\ndef plot_figs(fig_num, elev, azim):\n    fig = plt.figure(fig_num, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elev, azim=azim)\n\n    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker='+', alpha=.4)\n    Y = np.c_[a, b, c]\n\n    # Using SciPy's SVD, this would be:\n    # _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)\n\n    pca = PCA(n_components=3)\n    pca.fit(Y)\n    pca_score = pca.explained_variance_ratio_\n    V = pca.components_\n\n    x_pca_axis, y_pca_axis, z_pca_axis = V.T * pca_score / pca_score.min()\n\n    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V.T\n    x_pca_plane = np.r_[x_pca_axis[:2], - x_pca_axis[1::-1]]\n    y_pca_plane = np.r_[y_pca_axis[:2], - y_pca_axis[1::-1]]\n    z_pca_plane = np.r_[z_pca_axis[:2], - z_pca_axis[1::-1]]\n    x_pca_plane.shape = (2, 2)\n    y_pca_plane.shape = (2, 2)\n    z_pca_plane.shape = (2, 2)\n    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n\n\nelev = -40\nazim = -80\nplot_figs(1, elev, azim)\n\nelev = 30\nazim = 20\nplot_figs(2, elev, azim)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_3d.html
PCA example with Iris Data-set	A							http://scikit-learn.org/stable/auto_examples/index.html			Principal Component Analysis applied to the Iris dataset.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfrom sklearn import decomposition\nfrom sklearn import datasets\n\nnp.random.seed(5)\n\ncenters = [[1, 1], [-1, -1], [1, -1]]\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nfig = plt.figure(1, figsize=(4, 3))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\npca = decomposition.PCA(n_components=3)\npca.fit(X)\nX = pca.transform(X)\n\nfor name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n    ax.text3D(X[y == label, 0].mean(),\n              X[y == label, 1].mean() + 1.5,\n              X[y == label, 2].mean(), name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n# Reorder the labels to have colors matching the cluster results\ny = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.spectral)\n\nx_surf = [X[:, 0].min(), X[:, 0].max(),\n          X[:, 0].min(), X[:, 0].max()]\ny_surf = [X[:, 0].max(), X[:, 0].max(),\n          X[:, 0].min(), X[:, 0].min()]\nx_surf = np.array(x_surf)\ny_surf = np.array(y_surf)\nv0 = pca.transform(pca.components_[[0]])\nv0 /= v0[-1]\nv1 = pca.transform(pca.components_[[1]])\nv1 /= v1[-1]\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html
Model selection with Probabilistic PCA and Factor Analysis (FA)	A							http://scikit-learn.org/stable/auto_examples/index.html			Probabilistic PCA and Factor Analysis are probabilistic models. The consequence is that the likelihood of new data can be used for model selection and covariance estimation. Here we compare PCA and FA with cross-validation on low rank data corrupted with homoscedastic noise (noise variance is the same for each feature) or heteroscedastic noise (noise variance is the different for each feature)....<br><br><pre><code>print(__doc__)\n\n# Authors: Alexandre Gramfort\n#          Denis A. Engemann\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\n\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.covariance import ShrunkCovariance, LedoitWolf\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.grid_search import GridSearchCV\n\n###############################################################################\n# Create the data\n\nn_samples, n_features, rank = 1000, 50, 10\nsigma = 1.\nrng = np.random.RandomState(42)\nU, _, _ = linalg.svd(rng.randn(n_features, n_features))\nX = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\n\n# Adding homoscedastic noise\nX_homo = X + sigma * rng.randn(n_samples, n_features)\n\n# Adding heteroscedastic noise\nsigmas = sigma * rng.rand(n_features) + sigma / 2.\nX_hetero = X + rng.randn(n_samples, n_features) * sigmas\n\n###############################################################################\n# Fit the models\n\nn_components = np.arange(0, n_features, 5)  # options for n_components\n\n\ndef compute_scores(X):\n    pca = PCA()\n    fa = FactorAnalysis()\n\n    pca_scores, fa_scores = [], []\n    for n in n_components:\n        pca.n_components = n\n        fa.n_components = n\n        pca_scores.append(np.mean(cross_val_score(pca, X)))\n        fa_scores.append(np.mean(cross_val_score(fa, X)))\n\n    return pca_scores, fa_scores\n\n\ndef shrunk_cov_score(X):\n    shrinkages = np.logspace(-2, 0, 30)\n    cv = GridSearchCV(ShrunkCovariance(), {'shrinkage': shrinkages})\n    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))\n\n\ndef lw_score(X):\n    return np.mean(cross_val_score(LedoitWolf(), X))\n\n\nfor X, title in [(X_homo, 'Homoscedastic Noise'),\n                 (X_hetero, 'Heteroscedastic Noise')]:\n    pca_scores, fa_scores = compute_scores(X)\n    n_components_pca = n_components[np.argmax(pca_scores)]\n    n_components_fa = n_components[np.argmax(fa_scores)]\n\n    pca = PCA(n_components='mle')\n    pca.fit(X)\n    n_components_pca_mle = pca.n_components_\n\n    print("best n_components by PCA CV = %d" % n_components_pca)\n    print("best n_components by FactorAnalysis CV = %d" % n_components_fa)\n    print("best n_components by PCA MLE = %d" % n_components_pca_mle)\n\n    plt.figure()\n    plt.plot(n_components, pca_scores, 'b', label='PCA scores')\n    plt.plot(n_components, fa_scores, 'r', label='FA scores')\n    plt.axvline(rank, color='g', label='TRUTH: %d' % rank, linestyle='-')\n    plt.axvline(n_components_pca, color='b',\n                label='PCA CV: %d' % n_components_pca, linestyle='--')\n    plt.axvline(n_components_fa, color='r',\n                label='FactorAnalysis CV: %d' % n_components_fa, linestyle='--')\n    plt.axvline(n_components_pca_mle, color='k',\n                label='PCA MLE: %d' % n_components_pca_mle, linestyle='--')\n\n    # compare with other covariance estimators\n    plt.axhline(shrunk_cov_score(X), color='violet',\n                label='Shrunk Covariance MLE', linestyle='-.')\n    plt.axhline(lw_score(X), color='orange',\n                label='LedoitWolf MLE' % n_components_pca_mle, linestyle='-.')\n\n    plt.xlabel('nb of components')\n    plt.ylabel('CV scores')\n    plt.legend(loc='lower right')\n    plt.title(title)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_fa_model_selection.html
Comparison of LDA and PCA 2D projection of Iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.<br><br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)\n\n# Percentage of variance explained for each components\nprint('explained variance ratio (first two components): %s'\n      % str(pca.explained_variance_ratio_))\n\nplt.figure()\nfor c, i, target_name in zip("rgb", [0, 1, 2], target_names):\n    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], c=c, label=target_name)\nplt.legend()\nplt.title('PCA of IRIS dataset')\n\nplt.figure()\nfor c, i, target_name in zip("rgb", [0, 1, 2], target_names):\n    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], c=c, label=target_name)\nplt.legend()\nplt.title('LDA of IRIS dataset')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html
Test with permutations the significance of a classification score	A							http://scikit-learn.org/stable/auto_examples/index.html			In order to test if a classification score is significative a technique in repeating the classification procedure after randomizing, permuting, the labels. The p-value is then given by the percentage of runs for which the score obtained is greater than the classification score obtained in the first place.<br><br><pre><code># Author:  Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import StratifiedKFold, permutation_test_score\nfrom sklearn import datasets\n\n\n##############################################################################\n# Loading a dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nn_classes = np.unique(y).size\n\n# Some noisy data not correlated\nrandom = np.random.RandomState(seed=0)\nE = random.normal(size=(len(X), 2200))\n\n# Add noisy data to the informative features for make the task harder\nX = np.c_[X, E]\n\nsvm = SVC(kernel='linear')\ncv = StratifiedKFold(y, 2)\n\nscore, permutation_scores, pvalue = permutation_test_score(\n    svm, X, y, scoring="accuracy", cv=cv, n_permutations=100, n_jobs=1)\n\nprint("Classification score %s (pvalue : %s)" % (score, pvalue))\n\n###############################################################################\n# View histogram of permutation scores\nplt.hist(permutation_scores, 20, label='Permutation scores')\nylim = plt.ylim()\n# BUG: vlines(..., linestyle='--') fails on older versions of matplotlib\n#plt.vlines(score, ylim[0], ylim[1], linestyle='--',\n#          color='g', linewidth=3, label='Classification Score'\n#          ' (pvalue %s)' % pvalue)\n#plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle='--',\n#          color='k', linewidth=3, label='Luck')\nplt.plot(2 * [score], ylim, '--g', linewidth=3,\n         label='Classification Score'\n         ' (pvalue %s)' % pvalue)\nplt.plot(2 * [1. / n_classes], ylim, '--k', linewidth=3, label='Luck')\n\nplt.ylim(ylim)\nplt.legend()\nplt.xlabel('Score')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_permutation_test_for_classification.html
Polynomial interpolation	A							http://scikit-learn.org/stable/auto_examples/index.html			This example demonstrates how to approximate a function with a polynomial of degree n_degree by using ridge regression. Concretely, from n_samples 1d points, it suffices to build the Vandermonde matrix, which is n_samples x n_degree+1 and has the following form:<br><br><pre><code>print(__doc__)\n\n# Author: Mathieu Blondel\n#         Jake Vanderplas\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\ndef f(x):\n    """ function to approximate by polynomial interpolation"""\n    return x * np.sin(x)\n\n\n# generate points used to plot\nx_plot = np.linspace(0, 10, 100)\n\n# generate points and keep a subset of them\nx = np.linspace(0, 10, 100)\nrng = np.random.RandomState(0)\nrng.shuffle(x)\nx = np.sort(x[:20])\ny = f(x)\n\n# create matrix versions of these arrays\nX = x[:, np.newaxis]\nX_plot = x_plot[:, np.newaxis]\n\nplt.plot(x_plot, f(x_plot), label="ground truth")\nplt.scatter(x, y, label="training points")\n\nfor degree in [3, 4, 5]:\n    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n    model.fit(X, y)\n    y_plot = model.predict(X_plot)\n    plt.plot(x_plot, y_plot, label="degree %d" % degree)\n\nplt.legend(loc='lower left')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html
Precision-Recall	A							http://scikit-learn.org/stable/auto_examples/index.html			Example of Precision-Recall metric to evaluate classifier output quality.<br><br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binarize the output\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n                                                    random_state=random_state)\n\n# Run classifier\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\n\n# Compute Precision-Recall and plot curve\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n                                                        y_score[:, i])\n    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])\n\n# Compute micro-average ROC curve and ROC area\nprecision["micro"], recall["micro"], _ = precision_recall_curve(y_test.ravel(),\n    y_score.ravel())\naverage_precision["micro"] = average_precision_score(y_test, y_score,\n                                                     average="micro")\n\n# Plot Precision-Recall curve\nplt.clf()\nplt.plot(recall[0], precision[0], label='Precision-Recall curve')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall example: AUC={0:0.2f}'.format(average_precision[0]))\nplt.legend(loc="lower left")\nplt.show()\n\n# Plot Precision-Recall curve for each class\nplt.clf()\nplt.plot(recall["micro"], precision["micro"],\n         label='micro-average Precision-recall curve (area = {0:0.2f})'\n               ''.format(average_precision["micro"]))\nfor i in range(n_classes):\n    plt.plot(recall[i], precision[i],\n             label='Precision-recall curve of class {0} (area = {1:0.2f})'\n                   ''.format(i, average_precision[i]))\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Extension of Precision-Recall curve to multi-class')\nplt.legend(loc="lower right")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html
Prediction Latency	A							http://scikit-learn.org/stable/auto_examples/index.html			This is an example showing the prediction latency of various scikit-learn estimators.<br><br><pre><code># Authors: Eustache Diemert <eustache@diemert.fr>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\nfrom collections import defaultdict\n\nimport time\nimport gc\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import scoreatpercentile\nfrom sklearn.datasets.samples_generator import make_regression\nfrom sklearn.ensemble.forest import RandomForestRegressor\nfrom sklearn.linear_model.ridge import Ridge\nfrom sklearn.linear_model.stochastic_gradient import SGDRegressor\nfrom sklearn.svm.classes import SVR\n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return '__file__' in globals()\n\n\ndef atomic_benchmark_estimator(estimator, X_test, verbose=False):\n    """Measure runtime prediction of each instance."""\n    n_instances = X_test.shape[0]\n    runtimes = np.zeros(n_instances, dtype=np.float)\n    for i in range(n_instances):\n        instance = X_test[[i], :]\n        start = time.time()\n        estimator.predict(instance)\n        runtimes[i] = time.time() - start\n    if verbose:\n        print("atomic_benchmark runtimes:", min(runtimes), scoreatpercentile(\n            runtimes, 50), max(runtimes))\n    return runtimes\n\n\ndef bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):\n    """Measure runtime prediction of the whole input."""\n    n_instances = X_test.shape[0]\n    runtimes = np.zeros(n_bulk_repeats, dtype=np.float)\n    for i in range(n_bulk_repeats):\n        start = time.time()\n        estimator.predict(X_test)\n        runtimes[i] = time.time() - start\n    runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))\n    if verbose:\n        print("bulk_benchmark runtimes:", min(runtimes), scoreatpercentile(\n            runtimes, 50), max(runtimes))\n    return runtimes\n\n\ndef benchmark_estimator(estimator, X_test, n_bulk_repeats=30, verbose=False):\n    """\n    Measure runtimes of prediction in both atomic and bulk mode.\n\n    Parameters\n    ----------\n    estimator : already trained estimator supporting `predict()`\n    X_test : test input\n    n_bulk_repeats : how many times to repeat when evaluating bulk mode\n\n    Returns\n    -------\n    atomic_runtimes, bulk_runtimes : a pair of `np.array` which contain the\n    runtimes in seconds.\n\n    """\n    atomic_runtimes = atomic_benchmark_estimator(estimator, X_test, verbose)\n    bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats,\n                                             verbose)\n    return atomic_runtimes, bulk_runtimes\n\n\ndef generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):\n    """Generate a regression dataset with the given parameters."""\n    if verbose:\n        print("generating dataset...")\n    X, y, coef = make_regression(n_samples=n_train + n_test,\n                                 n_features=n_features, noise=noise, coef=True)\n    X_train = X[:n_train]\n    y_train = y[:n_train]\n    X_test = X[n_train:]\n    y_test = y[n_train:]\n    idx = np.arange(n_train)\n    np.random.seed(13)\n    np.random.shuffle(idx)\n    X_train = X_train[idx]\n    y_train = y_train[idx]\n\n    std = X_train.std(axis=0)\n    mean = X_train.mean(axis=0)\n    X_train = (X_train - mean) / std\n    X_test = (X_test - mean) / std\n\n    std = y_train.std(axis=0)\n    mean = y_train.mean(axis=0)\n    y_train = (y_train - mean) / std\n    y_test = (y_test - mean) / std\n\n    gc.collect()\n    if verbose:\n        print("ok")\n    return X_train, y_train, X_test, y_test\n\n\ndef boxplot_runtimes(runtimes, pred_type, configuration):\n    """\n    Plot a new `Figure` with boxplots of prediction runtimes.\n\n    Parameters\n    ----------\n    runtimes : list of `np.array` of latencies in micro-seconds\n    cls_names : list of estimator class names that generated the runtimes\n    pred_type : 'bulk' or 'atomic'\n\n    """\n\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    bp = plt.boxplot(runtimes, )\n\n    cls_infos = ['%s\n(%d %s)' % (estimator_conf['name'],\n                                  estimator_conf['complexity_computer'](\n                                      estimator_conf['instance']),\n                                  estimator_conf['complexity_label']) for\n                 estimator_conf in configuration['estimators']]\n    plt.setp(ax1, xticklabels=cls_infos)\n    plt.setp(bp['boxes'], color='black')\n    plt.setp(bp['whiskers'], color='black')\n    plt.setp(bp['fliers'], color='red', marker='+')\n\n    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n                   alpha=0.5)\n\n    ax1.set_axisbelow(True)\n    ax1.set_title('Prediction Time per Instance - %s, %d feats.' % (\n        pred_type.capitalize(),\n        configuration['n_features']))\n    ax1.set_ylabel('Prediction Time (us)')\n\n    plt.show()\n\n\ndef benchmark(configuration):\n    """Run the whole benchmark."""\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration['n_train'], configuration['n_test'],\n        configuration['n_features'])\n\n    stats = {}\n    for estimator_conf in configuration['estimators']:\n        print("Benchmarking", estimator_conf['instance'])\n        estimator_conf['instance'].fit(X_train, y_train)\n        gc.collect()\n        a, b = benchmark_estimator(estimator_conf['instance'], X_test)\n        stats[estimator_conf['name']] = {'atomic': a, 'bulk': b}\n\n    cls_names = [estimator_conf['name'] for estimator_conf in configuration[\n        'estimators']]\n    runtimes = [1e6 * stats[clf_name]['atomic'] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, 'atomic', configuration)\n    runtimes = [1e6 * stats[clf_name]['bulk'] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, 'bulk (%d)' % configuration['n_test'],\n                     configuration)\n\n\ndef n_feature_influence(estimators, n_train, n_test, n_features, percentile):\n    """\n    Estimate influence of the number of features on prediction time.\n\n    Parameters\n    ----------\n\n    estimators : dict of (name (str), estimator) to benchmark\n    n_train : nber of training instances (int)\n    n_test : nber of testing instances (int)\n    n_features : list of feature-space dimensionality to test (int)\n    percentile : percentile at which to measure the speed (int [0-100])\n\n    Returns:\n    --------\n\n    percentiles : dict(estimator_name,\n                       dict(n_features, percentile_perf_in_us))\n\n    """\n    percentiles = defaultdict(defaultdict)\n    for n in n_features:\n        print("benchmarking with %d features" % n)\n        X_train, y_train, X_test, y_test = generate_dataset(n_train, n_test, n)\n        for cls_name, estimator in estimators.items():\n            estimator.fit(X_train, y_train)\n            gc.collect()\n            runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)\n            percentiles[cls_name][n] = 1e6 * scoreatpercentile(runtimes,\n                                                               percentile)\n    return percentiles\n\n\ndef plot_n_features_influence(percentiles, percentile):\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    colors = ['r', 'g', 'b']\n    for i, cls_name in enumerate(percentiles.keys()):\n        x = np.array(sorted([n for n in percentiles[cls_name].keys()]))\n        y = np.array([percentiles[cls_name][n] for n in x])\n        plt.plot(x, y, color=colors[i], )\n    ax1.yaxis.grid(True, linestyle='-', which='major', color='lightgrey',\n                   alpha=0.5)\n    ax1.set_axisbelow(True)\n    ax1.set_title('Evolution of Prediction Time with #Features')\n    ax1.set_xlabel('#Features')\n    ax1.set_ylabel('Prediction Time at %d%%-ile (us)' % percentile)\n    plt.show()\n\n\ndef benchmark_throughputs(configuration, duration_secs=0.1):\n    """benchmark throughput for different estimators."""\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration['n_train'], configuration['n_test'],\n        configuration['n_features'])\n    throughputs = dict()\n    for estimator_config in configuration['estimators']:\n        estimator_config['instance'].fit(X_train, y_train)\n        start_time = time.time()\n        n_predictions = 0\n        while (time.time() - start_time) < duration_secs:\n            estimator_config['instance'].predict(X_test[[0]])\n            n_predictions += 1\n        throughputs[estimator_config['name']] = n_predictions / duration_secs\n    return throughputs\n\n\ndef plot_benchmark_throughput(throughputs, configuration):\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors = ['r', 'g', 'b']\n    cls_infos = ['%s\n(%d %s)' % (estimator_conf['name'],\n                                  estimator_conf['complexity_computer'](\n                                      estimator_conf['instance']),\n                                  estimator_conf['complexity_label']) for\n                 estimator_conf in configuration['estimators']]\n    cls_values = [throughputs[estimator_conf['name']] for estimator_conf in\n                  configuration['estimators']]\n    plt.bar(range(len(throughputs)), cls_values, width=0.5, color=colors)\n    ax.set_xticks(np.linspace(0.25, len(throughputs) - 0.75, len(throughputs)))\n    ax.set_xticklabels(cls_infos, fontsize=10)\n    ymax = max(cls_values) * 1.2\n    ax.set_ylim((0, ymax))\n    ax.set_ylabel('Throughput (predictions/sec)')\n    ax.set_title('Prediction Throughput for different estimators (%d '\n                 'features)' % configuration['n_features'])\n    plt.show()\n\n\n###############################################################################\n# main code\n\nstart_time = time.time()\n\n# benchmark bulk/atomic prediction speed for various regressors\nconfiguration = {\n    'n_train': int(1e3),\n    'n_test': int(1e2),\n    'n_features': int(1e2),\n    'estimators': [\n        {'name': 'Linear Model',\n         'instance': SGDRegressor(penalty='elasticnet', alpha=0.01,\n                                  l1_ratio=0.25, fit_intercept=True),\n         'complexity_label': 'non-zero coefficients',\n         'complexity_computer': lambda clf: np.count_nonzero(clf.coef_)},\n        {'name': 'RandomForest',\n         'instance': RandomForestRegressor(),\n         'complexity_label': 'estimators',\n         'complexity_computer': lambda clf: clf.n_estimators},\n        {'name': 'SVR',\n         'instance': SVR(kernel='rbf'),\n         'complexity_label': 'support vectors',\n         'complexity_computer': lambda clf: len(clf.support_vectors_)},\n    ]\n}\nbenchmark(configuration)\n\n# benchmark n_features influence on prediction speed\npercentile = 90\npercentiles = n_feature_influence({'ridge': Ridge()},\n                                  configuration['n_train'],\n                                  configuration['n_test'],\n                                  [100, 250, 500], percentile)\nplot_n_features_influence(percentiles, percentile)\n\n# benchmark throughput\nthroughputs = benchmark_throughputs(configuration)\nplot_benchmark_throughput(throughputs, configuration)\n\nstop_time = time.time()\nprint("example run in %.2fs" % (stop_time - start_time))</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html
Plot randomly generated classification dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot several randomly generated 2D classification datasets. This example illustrates the datasets.make_classification datasets.make_blobs and datasets.make_gaussian_quantiles functions.<br><br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_gaussian_quantiles\n\nplt.figure(figsize=(8, 8))\nplt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n\nplt.subplot(321)\nplt.title("One informative feature, one cluster per class", fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,\n                             n_clusters_per_class=1)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.subplot(322)\nplt.title("Two informative features, one cluster per class", fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.subplot(323)\nplt.title("Two informative features, two clusters per class", fontsize='small')\nX2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)\nplt.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2)\n\n\nplt.subplot(324)\nplt.title("Multi-class, two informative features, one cluster",\n          fontsize='small')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1, n_classes=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.subplot(325)\nplt.title("Three blobs", fontsize='small')\nX1, Y1 = make_blobs(n_features=2, centers=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.subplot(326)\nplt.title("Gaussian divided into three quantiles", fontsize='small')\nX1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/datasets/plot_random_dataset.html
Hashing feature transformation using Totally Random Trees	A							http://scikit-learn.org/stable/auto_examples/index.html			RandomTreesEmbedding provides a way to map data to a very high-dimensional, sparse representation, which might be beneficial for classification. The mapping is completely unsupervised and very efficient.<br><br><pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_circles\nfrom sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.naive_bayes import BernoulliNB\n\n# make a synthetic dataset\nX, y = make_circles(factor=0.5, random_state=0, noise=0.05)\n\n# use RandomTreesEmbedding to transform data\nhasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3)\nX_transformed = hasher.fit_transform(X)\n\n# Visualize result using PCA\npca = TruncatedSVD(n_components=2)\nX_reduced = pca.fit_transform(X_transformed)\n\n# Learn a Naive Bayes classifier on the transformed data\nnb = BernoulliNB()\nnb.fit(X_transformed, y)\n\n\n# Learn an ExtraTreesClassifier for comparison\ntrees = ExtraTreesClassifier(max_depth=3, n_estimators=10, random_state=0)\ntrees.fit(X, y)\n\n\n# scatter plot of original and reduced data\nfig = plt.figure(figsize=(9, 8))\n\nax = plt.subplot(221)\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nax.set_title("Original Data (2d)")\nax.set_xticks(())\nax.set_yticks(())\n\nax = plt.subplot(222)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50)\nax.set_title("PCA reduction (2d) of transformed data (%dd)" %\n             X_transformed.shape[1])\nax.set_xticks(())\nax.set_yticks(())\n\n# Plot the decision in original space. For that, we will assign a color to each\n# point in the mesh [x_min, m_max] x [y_min, y_max].\nh = .01\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# transform grid using RandomTreesEmbedding\ntransformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()])\ny_grid_pred = nb.predict_proba(transformed_grid)[:, 1]\n\nax = plt.subplot(223)\nax.set_title("Naive Bayes on Transformed data")\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nax.set_ylim(-1.4, 1.4)\nax.set_xlim(-1.4, 1.4)\nax.set_xticks(())\nax.set_yticks(())\n\n# transform grid using ExtraTreesClassifier\ny_grid_pred = trees.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\nax = plt.subplot(224)\nax.set_title("ExtraTrees predictions")\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\nax.scatter(X[:, 0], X[:, 1], c=y, s=50)\nax.set_ylim(-1.4, 1.4)\nax.set_xlim(-1.4, 1.4)\nax.set_xticks(())\nax.set_yticks(())\n\nplt.tight_layout()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_embedding.html
Plot randomly generated multilabel dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			This illustrates the datasets.make_multilabel_classification dataset generator. Each sample consists of counts of two features (up to 50 in total), which are differently distributed in each of two classes.<br><br><pre><code>from __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_multilabel_classification as make_ml_clf\n\nprint(__doc__)\n\nCOLORS = np.array(['!',\n                   '#FF3333',  # red\n                   '#0198E1',  # blue\n                   '#BF5FFF',  # purple\n                   '#FCD116',  # yellow\n                   '#FF7216',  # orange\n                   '#4DBD33',  # green\n                   '#87421F'   # brown\n                   ])\n\n# Use same random seed for multiple calls to make_multilabel_classification to\n# ensure same distributions\nRANDOM_SEED = np.random.randint(2 ** 10)\n\n\ndef plot_2d(ax, n_labels=1, n_classes=3, length=50):\n    X, Y, p_c, p_w_c = make_ml_clf(n_samples=150, n_features=2,\n                                   n_classes=n_classes, n_labels=n_labels,\n                                   length=length, allow_unlabeled=False,\n                                   return_distributions=True,\n                                   random_state=RANDOM_SEED)\n\n    ax.scatter(X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]\n                                                    ).sum(axis=1)),\n               marker='.')\n    ax.scatter(p_w_c[0] * length, p_w_c[1] * length,\n               marker='*', linewidth=.5, edgecolor='black',\n               s=20 + 1500 * p_c ** 2,\n               color=COLORS.take([1, 2, 4]))\n    ax.set_xlabel('Feature 0 count')\n    return p_c, p_w_c\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, sharex='row', sharey='row', figsize=(8, 4))\nplt.subplots_adjust(bottom=.15)\n\np_c, p_w_c = plot_2d(ax1, n_labels=1)\nax1.set_title('n_labels=1, length=50')\nax1.set_ylabel('Feature 1 count')\n\nplot_2d(ax2, n_labels=3)\nax2.set_title('n_labels=3, length=50')\nax2.set_xlim(left=0, auto=True)\nax2.set_ylim(bottom=0, auto=True)\n\nplt.show()\n\nprint('The data was generated from (random_state=%d):' % RANDOM_SEED)\nprint('Class', 'P(C)', 'P(w0|C)', 'P(w1|C)', sep='\t')\nfor k, p, p_w in zip(['red', 'blue', 'yellow'], p_c, p_w_c.T):\n    print('%s\t%0.2f\t%0.2f\t%0.2f' % (k, p, p_w[0], p_w[1]))</code></pre>	http://scikit-learn.org/stable/auto_examples/datasets/plot_random_multilabel_dataset.html
Robust linear model estimation using RANSAC	A							http://scikit-learn.org/stable/auto_examples/index.html			In this example we see how to robustly fit a linear model to faulty data using the RANSAC algorithm.<br><br><pre><code>import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import linear_model, datasets\n\n\nn_samples = 1000\nn_outliers = 50\n\n\nX, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n                                      n_informative=1, noise=10,\n                                      coef=True, random_state=0)\n\n# Add outlier data\nnp.random.seed(0)\nX[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\ny[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n\n# Fit line using all data\nmodel = linear_model.LinearRegression()\nmodel.fit(X, y)\n\n# Robustly fit linear model with RANSAC algorithm\nmodel_ransac = linear_model.RANSACRegressor(linear_model.LinearRegression())\nmodel_ransac.fit(X, y)\ninlier_mask = model_ransac.inlier_mask_\noutlier_mask = np.logical_not(inlier_mask)\n\n# Predict data of estimated models\nline_X = np.arange(-5, 5)\nline_y = model.predict(line_X[:, np.newaxis])\nline_y_ransac = model_ransac.predict(line_X[:, np.newaxis])\n\n# Compare estimated coefficients\nprint("Estimated coefficients (true, normal, RANSAC):")\nprint(coef, model.coef_, model_ransac.estimator_.coef_)\n\nplt.plot(X[inlier_mask], y[inlier_mask], '.g', label='Inliers')\nplt.plot(X[outlier_mask], y[outlier_mask], '.r', label='Outliers')\nplt.plot(line_X, line_y, '-k', label='Linear regressor')\nplt.plot(line_X, line_y_ransac, '-b', label='RANSAC regressor')\nplt.legend(loc='lower right')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ransac.html
RBF SVM parameters	A							http://scikit-learn.org/stable/auto_examples/index.html			This example illustrates the effect of the parameters gamma and C of the Radial Basis Function (RBF) kernel SVM.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.grid_search import GridSearchCV\n\n\n# Utility function to move the midpoint of a colormap to be around\n# the values of interest.\n\nclass MidpointNormalize(Normalize):\n\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        self.midpoint = midpoint\n        Normalize.__init__(self, vmin, vmax, clip)\n\n    def __call__(self, value, clip=None):\n        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n        return np.ma.masked_array(np.interp(value, x, y))\n\n##############################################################################\n# Load and prepare data set\n#\n# dataset for grid search\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Dataset for decision function visualization: we only keep the first two\n# features in X and sub-sample the dataset to keep only 2 classes and\n# make it a binary classification problem.\n\nX_2d = X[:, :2]\nX_2d = X_2d[y > 0]\ny_2d = y[y > 0]\ny_2d -= 1\n\n# It is usually a good idea to scale the data for SVM training.\n# We are cheating a bit in this example in scaling all of the data,\n# instead of fitting the transformation on the training set and\n# just applying it on the test set.\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_2d = scaler.fit_transform(X_2d)\n\n##############################################################################\n# Train classifiers\n#\n# For an initial search, a logarithmic grid with basis\n# 10 is often helpful. Using a basis of 2, a finer\n# tuning can be achieved but at a much higher cost.\n\nC_range = np.logspace(-2, 10, 13)\ngamma_range = np.logspace(-9, 3, 13)\nparam_grid = dict(gamma=gamma_range, C=C_range)\ncv = StratifiedShuffleSplit(y, n_iter=5, test_size=0.2, random_state=42)\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\ngrid.fit(X, y)\n\nprint("The best parameters are %s with a score of %0.2f"\n      % (grid.best_params_, grid.best_score_))\n\n# Now we need to fit a classifier for all parameters in the 2d version\n# (we use a smaller set of parameters here because it takes a while to train)\n\nC_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = SVC(C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))\n\n##############################################################################\n# visualization\n#\n# draw visualization of parameter effects\n\nplt.figure(figsize=(8, 6))\nxx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\nfor (k, (C, gamma, clf)) in enumerate(classifiers):\n    # evaluate decision function in a grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # visualize decision function for these parameters\n    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n    plt.title("gamma=10^%d, C=10^%d" % (np.log10(gamma), np.log10(C)),\n              size='medium')\n\n    # visualize parameter's effect on decision function\n    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.axis('tight')\n\n# plot the scores of the grid\n# grid_scores_ contains parameter settings and scores\n# We extract just the scores\nscores = [x[1] for x in grid.grid_scores_]\nscores = np.array(scores).reshape(len(C_range), len(gamma_range))\n\n# Draw heatmap of the validation accuracy as a function of gamma and C\n#\n# The score are encoded as colors with the hot colormap which varies from dark\n# red to bright yellow. As the most interesting scores are all located in the\n# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\n# as to make it easier to visualize the small variations of score values in the\n# interesting range while not brutally collapsing all the low score values to\n# the same color.\n\nplt.figure(figsize=(8, 6))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\nplt.xlabel('gamma')\nplt.ylabel('C')\nplt.colorbar()\nplt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\nplt.yticks(np.arange(len(C_range)), C_range)\nplt.title('Validation accuracy')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html
Restricted Boltzmann Machine features for digit classification	A							http://scikit-learn.org/stable/auto_examples/index.html			For greyscale image data where pixel values can be interpreted as degrees of blackness on a white background, like handwritten digit recognition, the Bernoulli Restricted Boltzmann machine model (BernoulliRBM) can perform effective non-linear feature extraction.<br><br><pre><code>from __future__ import print_function\n\nprint(__doc__)\n\n# Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.ndimage import convolve\nfrom sklearn import linear_model, datasets, metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.neural_network import BernoulliRBM\nfrom sklearn.pipeline import Pipeline\n\n\n###############################################################################\n# Setting up\n\ndef nudge_dataset(X, Y):\n    """\n    This produces a dataset 5 times bigger than the original one,\n    by moving the 8x8 images in X around by 1px to left, right, down, up\n    """\n    direction_vectors = [\n        [[0, 1, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [1, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 1],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 1, 0]]]\n\n    shift = lambda x, w: convolve(x.reshape((8, 8)), mode='constant',\n                                  weights=w).ravel()\n    X = np.concatenate([X] +\n                       [np.apply_along_axis(shift, 1, X, vector)\n                        for vector in direction_vectors])\n    Y = np.concatenate([Y for _ in range(5)], axis=0)\n    return X, Y\n\n# Load Data\ndigits = datasets.load_digits()\nX = np.asarray(digits.data, 'float32')\nX, Y = nudge_dataset(X, digits.target)\nX = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n                                                    test_size=0.2,\n                                                    random_state=0)\n\n# Models we will use\nlogistic = linear_model.LogisticRegression()\nrbm = BernoulliRBM(random_state=0, verbose=True)\n\nclassifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])\n\n###############################################################################\n# Training\n\n# Hyper-parameters. These were set by cross-validation,\n# using a GridSearchCV. Here we are not performing cross-validation to\n# save time.\nrbm.learning_rate = 0.06\nrbm.n_iter = 20\n# More components tend to give better prediction performance, but larger\n# fitting time\nrbm.n_components = 100\nlogistic.C = 6000.0\n\n# Training RBM-Logistic Pipeline\nclassifier.fit(X_train, Y_train)\n\n# Training Logistic regression\nlogistic_classifier = linear_model.LogisticRegression(C=100.0)\nlogistic_classifier.fit(X_train, Y_train)\n\n###############################################################################\n# Evaluation\n\nprint()\nprint("Logistic regression using RBM features:\n%s\n" % (\n    metrics.classification_report(\n        Y_test,\n        classifier.predict(X_test))))\n\nprint("Logistic regression using raw pixel features:\n%s\n" % (\n    metrics.classification_report(\n        Y_test,\n        logistic_classifier.predict(X_test))))\n\n###############################################################################\n# Plotting\n\nplt.figure(figsize=(4.2, 4))\nfor i, comp in enumerate(rbm.components_):\n    plt.subplot(10, 10, i + 1)\n    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,\n               interpolation='nearest')\n    plt.xticks(())\n    plt.yticks(())\nplt.suptitle('100 components extracted by RBM', fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html
Nearest Neighbors regression	A							http://scikit-learn.org/stable/auto_examples/index.html			Demonstrate the resolution of a regression problem using a k-Nearest Neighbor and the interpolation of the target using both barycenter and constant weights.<br><br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#         Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#\n# License: BSD 3 clause (C) INRIA\n\n\n###############################################################################\n# Generate sample data\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\n\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\nT = np.linspace(0, 5, 500)[:, np.newaxis]\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 1 * (0.5 - np.random.rand(8))\n\n###############################################################################\n# Fit regression model\nn_neighbors = 5\n\nfor i, weights in enumerate(['uniform', 'distance']):\n    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n    y_ = knn.fit(X, y).predict(T)\n\n    plt.subplot(2, 1, i + 1)\n    plt.scatter(X, y, c='k', label='data')\n    plt.plot(T, y_, c='g', label='prediction')\n    plt.axis('tight')\n    plt.legend()\n    plt.title("KNeighborsRegressor (k = %i, weights = '%s')" % (n_neighbors,\n                                                                weights))\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html
Recursive feature elimination	A							http://scikit-learn.org/stable/auto_examples/index.html			A recursive feature elimination example showing the relevance of pixels in a digit classification task.<br><br><pre><code>print(__doc__)\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt\n\n# Load the digits dataset\ndigits = load_digits()\nX = digits.images.reshape((len(digits.images), -1))\ny = digits.target\n\n# Create the RFE object and rank each pixel\nsvc = SVC(kernel="linear", C=1)\nrfe = RFE(estimator=svc, n_features_to_select=1, step=1)\nrfe.fit(X, y)\nranking = rfe.ranking_.reshape(digits.images[0].shape)\n\n# Plot pixel ranking\nplt.matshow(ranking)\nplt.colorbar()\nplt.title("Ranking of pixels with RFE")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_digits.html
Recursive feature elimination with cross-validation	A							http://scikit-learn.org/stable/auto_examples/index.html			A recursive feature elimination example with automatic tuning of the number of features selected with cross-validation.<br><br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.datasets import make_classification\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000, n_features=25, n_informative=3,\n                           n_redundant=2, n_repeated=0, n_classes=8,\n                           n_clusters_per_class=1, random_state=0)\n\n# Create the RFE object and compute a cross-validated score.\nsvc = SVC(kernel="linear")\n# The "accuracy" scoring is proportional to the number of correct\n# classifications\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y, 2),\n              scoring='accuracy')\nrfecv.fit(X, y)\n\nprint("Optimal number of features : %d" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel("Number of features selected")\nplt.ylabel("Cross validation score (nb of correct classifications)")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_rfe_with_cross_validation.html
Plot Ridge coefficients as a function of the regularization	A							http://scikit-learn.org/stable/auto_examples/index.html			Shows the effect of collinearity in the coefficients of an estimator.<br><br><pre><code># Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\n# X is the 10x10 Hilbert matrix\nX = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])\ny = np.ones(10)\n\n###############################################################################\n# Compute paths\n\nn_alphas = 200\nalphas = np.logspace(-10, -2, n_alphas)\nclf = linear_model.Ridge(fit_intercept=False)\n\ncoefs = []\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X, y)\n    coefs.append(clf.coef_)\n\n###############################################################################\n# Display results\n\nax = plt.gca()\nax.set_color_cycle(['b', 'r', 'g', 'c', 'k', 'y', 'm'])\n\nax.plot(alphas, coefs)\nax.set_xscale('log')\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Ridge coefficients as a function of the regularization')\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html
Robust linear estimator fitting	A							http://scikit-learn.org/stable/auto_examples/index.html			Here a sine function is fit with a polynomial of order 3, for values close to zero.<br><br><pre><code>from matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn import linear_model, metrics\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nnp.random.seed(42)\n\nX = np.random.normal(size=400)\ny = np.sin(X)\n# Make sure that it X is 2D\nX = X[:, np.newaxis]\n\nX_test = np.random.normal(size=200)\ny_test = np.sin(X_test)\nX_test = X_test[:, np.newaxis]\n\ny_errors = y.copy()\ny_errors[::3] = 3\n\nX_errors = X.copy()\nX_errors[::3] = 3\n\ny_errors_large = y.copy()\ny_errors_large[::3] = 10\n\nX_errors_large = X.copy()\nX_errors_large[::3] = 10\n\nestimators = [('OLS', linear_model.LinearRegression()),\n              ('Theil-Sen', linear_model.TheilSenRegressor(random_state=42)),\n              ('RANSAC', linear_model.RANSACRegressor(random_state=42)), ]\n\nx_plot = np.linspace(X.min(), X.max())\n\nfor title, this_X, this_y in [\n        ('Modeling errors only', X, y),\n        ('Corrupt X, small deviants', X_errors, y),\n        ('Corrupt y, small deviants', X, y_errors),\n        ('Corrupt X, large deviants', X_errors_large, y),\n        ('Corrupt y, large deviants', X, y_errors_large)]:\n    plt.figure(figsize=(5, 4))\n    plt.plot(this_X[:, 0], this_y, 'k+')\n\n    for name, estimator in estimators:\n        model = make_pipeline(PolynomialFeatures(3), estimator)\n        model.fit(this_X, this_y)\n        mse = metrics.mean_squared_error(model.predict(X_test), y_test)\n        y_plot = model.predict(x_plot[:, np.newaxis])\n        plt.plot(x_plot, y_plot,\n                 label='%s: error = %.3f' % (name, mse))\n\n    plt.legend(loc='best', frameon=False,\n               title='Error: mean absolute deviation\n to non corrupt data')\n    plt.xlim(-4, 10.2)\n    plt.ylim(-2, 10.2)\n    plt.title(title)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_robust_fit.html
Robust Scaling on Toy Data	A							http://scikit-learn.org/stable/auto_examples/index.html			Making sure that each Feature has approximately the same scale can be a crucial preprocessing step. However, when data contains outliers, StandardScaler can often be mislead. In such cases, it is better to use a scaler that is robust against outliers.<br><br><pre><code>from __future__ import print_function\nprint(__doc__)\n\n\n# Code source: Thomas Unterthiner\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# Create training and test data\nnp.random.seed(42)\nn_datapoints = 100\nCov = [[0.9, 0.0], [0.0, 20.0]]\nmu1 = [100.0, -3.0]\nmu2 = [101.0, -3.0]\nX1 = np.random.multivariate_normal(mean=mu1, cov=Cov, size=n_datapoints)\nX2 = np.random.multivariate_normal(mean=mu2, cov=Cov, size=n_datapoints)\nY_train = np.hstack([[-1]*n_datapoints, [1]*n_datapoints])\nX_train = np.vstack([X1, X2])\n\nX1 = np.random.multivariate_normal(mean=mu1, cov=Cov, size=n_datapoints)\nX2 = np.random.multivariate_normal(mean=mu2, cov=Cov, size=n_datapoints)\nY_test = np.hstack([[-1]*n_datapoints, [1]*n_datapoints])\nX_test = np.vstack([X1, X2])\n\nX_train[0, 0] = -1000  # a fairly large outlier\n\n\n# Scale data\nstandard_scaler = StandardScaler()\nXtr_s = standard_scaler.fit_transform(X_train)\nXte_s = standard_scaler.transform(X_test)\n\nrobust_scaler = RobustScaler()\nXtr_r = robust_scaler.fit_transform(X_train)\nXte_r = robust_scaler.fit_transform(X_test)\n\n\n# Plot data\nfig, ax = plt.subplots(1, 3, figsize=(12, 4))\nax[0].scatter(X_train[:, 0], X_train[:, 1],\n              color=np.where(Y_train > 0, 'r', 'b'))\nax[1].scatter(Xtr_s[:, 0], Xtr_s[:, 1], color=np.where(Y_train > 0, 'r', 'b'))\nax[2].scatter(Xtr_r[:, 0], Xtr_r[:, 1], color=np.where(Y_train > 0, 'r', 'b'))\nax[0].set_title("Unscaled data")\nax[1].set_title("After standard scaling (zoomed in)")\nax[2].set_title("After robust scaling (zoomed in)")\n# for the scaled data, we zoom in to the data center (outlier can't be seen!)\nfor a in ax[1:]:\n    a.set_xlim(-3, 3)\n    a.set_ylim(-3, 3)\nplt.tight_layout()\nplt.show()\n\n\n# Classify using k-NN\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(Xtr_s, Y_train)\nacc_s = knn.score(Xte_s, Y_test)\nprint("Testset accuracy using standard scaler: %.3f" % acc_s)\nknn.fit(Xtr_r, Y_train)\nacc_r = knn.score(Xte_r, Y_test)\nprint("Testset accuracy using robust scaler:   %.3f" % acc_r)</code></pre>	http://scikit-learn.org/stable/auto_examples/preprocessing/plot_robust_scaling.html
Robust vs Empirical covariance estimate	A							http://scikit-learn.org/stable/auto_examples/index.html			The usual covariance maximum likelihood estimate is very sensitive to the presence of outliers in the data set. In such a case, it would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to “erroneous” observations in the data set.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\n# example settings\nn_samples = 80\nn_features = 5\nrepeat = 10\n\nrange_n_outliers = np.concatenate(\n    (np.linspace(0, n_samples / 8, 5),\n     np.linspace(n_samples / 8, n_samples / 2, 5)[1:-1]))\n\n# definition of arrays to store results\nerr_loc_mcd = np.zeros((range_n_outliers.size, repeat))\nerr_cov_mcd = np.zeros((range_n_outliers.size, repeat))\nerr_loc_emp_full = np.zeros((range_n_outliers.size, repeat))\nerr_cov_emp_full = np.zeros((range_n_outliers.size, repeat))\nerr_loc_emp_pure = np.zeros((range_n_outliers.size, repeat))\nerr_cov_emp_pure = np.zeros((range_n_outliers.size, repeat))\n\n# computation\nfor i, n_outliers in enumerate(range_n_outliers):\n    for j in range(repeat):\n\n        rng = np.random.RandomState(i * j)\n\n        # generate data\n        X = rng.randn(n_samples, n_features)\n        # add some outliers\n        outliers_index = rng.permutation(n_samples)[:n_outliers]\n        outliers_offset = 10. * \\n            (np.random.randint(2, size=(n_outliers, n_features)) - 0.5)\n        X[outliers_index] += outliers_offset\n        inliers_mask = np.ones(n_samples).astype(bool)\n        inliers_mask[outliers_index] = False\n\n        # fit a Minimum Covariance Determinant (MCD) robust estimator to data\n        mcd = MinCovDet().fit(X)\n        # compare raw robust estimates with the true location and covariance\n        err_loc_mcd[i, j] = np.sum(mcd.location_ ** 2)\n        err_cov_mcd[i, j] = mcd.error_norm(np.eye(n_features))\n\n        # compare estimators learned from the full data set with true\n        # parameters\n        err_loc_emp_full[i, j] = np.sum(X.mean(0) ** 2)\n        err_cov_emp_full[i, j] = EmpiricalCovariance().fit(X).error_norm(\n            np.eye(n_features))\n\n        # compare with an empirical covariance learned from a pure data set\n        # (i.e. "perfect" mcd)\n        pure_X = X[inliers_mask]\n        pure_location = pure_X.mean(0)\n        pure_emp_cov = EmpiricalCovariance().fit(pure_X)\n        err_loc_emp_pure[i, j] = np.sum(pure_location ** 2)\n        err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))\n\n# Display results\nfont_prop = matplotlib.font_manager.FontProperties(size=11)\nplt.subplot(2, 1, 1)\nplt.errorbar(range_n_outliers, err_loc_mcd.mean(1),\n             yerr=err_loc_mcd.std(1) / np.sqrt(repeat),\n             label="Robust location", color='m')\nplt.errorbar(range_n_outliers, err_loc_emp_full.mean(1),\n             yerr=err_loc_emp_full.std(1) / np.sqrt(repeat),\n             label="Full data set mean", color='green')\nplt.errorbar(range_n_outliers, err_loc_emp_pure.mean(1),\n             yerr=err_loc_emp_pure.std(1) / np.sqrt(repeat),\n             label="Pure data set mean", color='black')\nplt.title("Influence of outliers on the location estimation")\nplt.ylabel(r"Error ($||\mu - \hat{\mu}||_2^2$)")\nplt.legend(loc="upper left", prop=font_prop)\n\nplt.subplot(2, 1, 2)\nx_size = range_n_outliers.size\nplt.errorbar(range_n_outliers, err_cov_mcd.mean(1),\n             yerr=err_cov_mcd.std(1),\n             label="Robust covariance (mcd)", color='m')\nplt.errorbar(range_n_outliers[:(x_size / 5 + 1)],\n             err_cov_emp_full.mean(1)[:(x_size / 5 + 1)],\n             yerr=err_cov_emp_full.std(1)[:(x_size / 5 + 1)],\n             label="Full data set empirical covariance", color='green')\nplt.plot(range_n_outliers[(x_size / 5):(x_size / 2 - 1)],\n         err_cov_emp_full.mean(1)[(x_size / 5):(x_size / 2 - 1)], color='green',\n         ls='--')\nplt.errorbar(range_n_outliers, err_cov_emp_pure.mean(1),\n             yerr=err_cov_emp_pure.std(1),\n             label="Pure data set empirical covariance", color='black')\nplt.title("Influence of outliers on the covariance estimation")\nplt.xlabel("Amount of contamination (%)")\nplt.ylabel("RMSE")\nplt.legend(loc="upper center", prop=font_prop)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_robust_vs_empirical_covariance.html
Receiver Operating Characteristic (ROC)	A							http://scikit-learn.org/stable/auto_examples/index.html			Example of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\n\n# Import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binarize the output\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n# Add noisy features to make the problem harder\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# shuffle and split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n                                                    random_state=0)\n\n# Learn to predict each class against the other\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc["micro"] = auc(fpr["micro"], tpr["micro"])\n\n\n##############################################################################\n# Plot of a ROC curve for a specific class\nplt.figure()\nplt.plot(fpr[2], tpr[2], label='ROC curve (area = %0.2f)' % roc_auc[2])\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc="lower right")\nplt.show()\n\n\n##############################################################################\n# Plot ROC curves for the multiclass problem\n\n# Compute macro-average ROC curve and ROC area\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr["macro"] = all_fpr\ntpr["macro"] = mean_tpr\nroc_auc["macro"] = auc(fpr["macro"], tpr["macro"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr["micro"], tpr["micro"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc["micro"]),\n         linewidth=2)\n\nplt.plot(fpr["macro"], tpr["macro"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc["macro"]),\n         linewidth=2)\n\nfor i in range(n_classes):\n    plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                   ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc="lower right")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html
Receiver Operating Characteristic (ROC) with cross validation	A							http://scikit-learn.org/stable/auto_examples/index.html			Example of Receiver Operating Characteristic (ROC) metric to evaluate classifier output quality using cross-validation.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nfrom scipy import interp\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.cross_validation import StratifiedKFold\n\n###############################################################################\n# Data IO and generation\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX, y = X[y != 2], y[y != 2]\nn_samples, n_features = X.shape\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n###############################################################################\n# Classification and ROC analysis\n\n# Run classifier with cross-validation and plot ROC curves\ncv = StratifiedKFold(y, n_folds=6)\nclassifier = svm.SVC(kernel='linear', probability=True,\n                     random_state=random_state)\n\nmean_tpr = 0.0\nmean_fpr = np.linspace(0, 1, 100)\nall_tpr = []\n\nfor i, (train, test) in enumerate(cv):\n    probas_ = classifier.fit(X[train], y[train]).predict_proba(X[test])\n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])\n    mean_tpr += interp(mean_fpr, fpr, tpr)\n    mean_tpr[0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n\nplt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Luck')\n\nmean_tpr /= len(cv)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, 'k--',\n         label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc="lower right")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html
Spectral clustering for image segmentation	A							http://scikit-learn.org/stable/auto_examples/index.html			In this example, an image with connected circles is generated and spectral clustering is used to separate the circles.<br><br><pre><code>print(__doc__)\n\n# Authors:  Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>\n#           Gael Varoquaux <gael.varoquaux@normalesup.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import spectral_clustering\n\n###############################################################################\nl = 100\nx, y = np.indices((l, l))\n\ncenter1 = (28, 24)\ncenter2 = (40, 50)\ncenter3 = (67, 58)\ncenter4 = (24, 70)\n\nradius1, radius2, radius3, radius4 = 16, 14, 15, 14\n\ncircle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2\ncircle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2\ncircle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 < radius3 ** 2\ncircle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 < radius4 ** 2\n\n###############################################################################\n# 4 circles\nimg = circle1 + circle2 + circle3 + circle4\nmask = img.astype(bool)\nimg = img.astype(float)\n\nimg += 1 + 0.2 * np.random.randn(*img.shape)\n\n# Convert the image into a graph with the value of the gradient on the\n# edges.\ngraph = image.img_to_graph(img, mask=mask)\n\n# Take a decreasing function of the gradient: we take it weakly\n# dependent from the gradient the segmentation is close to a voronoi\ngraph.data = np.exp(-graph.data / graph.data.std())\n\n# Force the solver to be arpack, since amg is numerically\n# unstable on this example\nlabels = spectral_clustering(graph, n_clusters=4, eigen_solver='arpack')\nlabel_im = -np.ones(mask.shape)\nlabel_im[mask] = labels\n\nplt.matshow(img)\nplt.matshow(label_im)\n\n###############################################################################\n# 2 circles\nimg = circle1 + circle2\nmask = img.astype(bool)\nimg = img.astype(float)\n\nimg += 1 + 0.2 * np.random.randn(*img.shape)\n\ngraph = image.img_to_graph(img, mask=mask)\ngraph.data = np.exp(-graph.data / graph.data.std())\n\nlabels = spectral_clustering(graph, n_clusters=2, eigen_solver='arpack')\nlabel_im = -np.ones(mask.shape)\nlabel_im[mask] = labels\n\nplt.matshow(img)\nplt.matshow(label_im)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_segmentation_toy.html
Feature selection using SelectFromModel and LassoCV	A							http://scikit-learn.org/stable/auto_examples/index.html			Use SelectFromModel meta-transformer along with Lasso to select the best couple of features from the Boston dataset.<br><br><pre><code># Author: Manoj Kumar <mks542@nyu.edu>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\n\n# Load the boston dataset.\nboston = load_boston()\nX, y = boston['data'], boston['target']\n\n# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\nclf = LassoCV()\n\n# Set a minimum threshold of 0.25\nsfm = SelectFromModel(clf, threshold=0.25)\nsfm.fit(X, y)\nn_features = sfm.transform(X).shape[1]\n\n# Reset the threshold till the number of features equals two.\n# Note that the attribute can be set directly instead of repeatedly\n# fitting the metatransformer.\nwhile n_features > 2:\n    sfm.threshold += 0.1\n    X_transform = sfm.transform(X)\n    n_features = X_transform.shape[1]\n\n# Plot the selected two features from X.\nplt.title(\n    "Features selected from Boston using SelectFromModel with "\n    "threshold %0.3f." % sfm.threshold)\nfeature1 = X_transform[:, 0]\nfeature2 = X_transform[:, 1] \nplt.plot(feature1, feature2, 'r.')\nplt.xlabel("Feature number 1")\nplt.ylabel("Feature number 2")\nplt.ylim([np.min(feature2), np.max(feature2)])\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/feature_selection/plot_select_from_model_boston.html
SVM: Maximum margin separating hyperplane	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the maximum margin separating hyperplane within a two-class separable dataset using a Support Vector Machine classifier with linear kernel.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# we create 40 separable points\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# fit the model\nclf = svm.SVC(kernel='linear')\nclf.fit(X, Y)\n\n# get the separating hyperplane\nw = clf.coef_[0]\na = -w[0] / w[1]\nxx = np.linspace(-5, 5)\nyy = a * xx - (clf.intercept_[0]) / w[1]\n\n# plot the parallels to the separating hyperplane that pass through the\n# support vectors\nb = clf.support_vectors_[0]\nyy_down = a * xx + (b[1] - a * b[0])\nb = clf.support_vectors_[-1]\nyy_up = a * xx + (b[1] - a * b[0])\n\n# plot the line, the points, and the nearest vectors to the plane\nplt.plot(xx, yy, 'k-')\nplt.plot(xx, yy_down, 'k--')\nplt.plot(xx, yy_up, 'k--')\n\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n            s=80, facecolors='none')\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane.html
SVM: Separating hyperplane for unbalanced classes	A							http://scikit-learn.org/stable/auto_examples/index.html			Find the optimal separating hyperplane using an SVC for classes that are unbalanced.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n#from sklearn.linear_model import SGDClassifier\n\n# we create 40 separable points\nrng = np.random.RandomState(0)\nn_samples_1 = 1000\nn_samples_2 = 100\nX = np.r_[1.5 * rng.randn(n_samples_1, 2),\n          0.5 * rng.randn(n_samples_2, 2) + [2, 2]]\ny = [0] * (n_samples_1) + [1] * (n_samples_2)\n\n# fit the model and get the separating hyperplane\nclf = svm.SVC(kernel='linear', C=1.0)\nclf.fit(X, y)\n\nw = clf.coef_[0]\na = -w[0] / w[1]\nxx = np.linspace(-5, 5)\nyy = a * xx - clf.intercept_[0] / w[1]\n\n\n# get the separating hyperplane using weighted classes\nwclf = svm.SVC(kernel='linear', class_weight={1: 10})\nwclf.fit(X, y)\n\nww = wclf.coef_[0]\nwa = -ww[0] / ww[1]\nwyy = wa * xx - wclf.intercept_[0] / ww[1]\n\n# plot separating hyperplanes and samples\nh0 = plt.plot(xx, yy, 'k-', label='no weights')\nh1 = plt.plot(xx, wyy, 'k--', label='with weights')\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\nplt.legend()\n\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html
Comparing various online solvers	A							http://scikit-learn.org/stable/auto_examples/index.html			An example showing how different online solvers perform on the hand-written digits dataset.<br><br><pre><code># Author: Rob Zinkov <rob at zinkov dot com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import SGDClassifier, Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nheldout = [0.95, 0.90, 0.75, 0.50, 0.01]\nrounds = 20\ndigits = datasets.load_digits()\nX, y = digits.data, digits.target\n\nclassifiers = [\n    ("SGD", SGDClassifier()),\n    ("ASGD", SGDClassifier(average=True)),\n    ("Perceptron", Perceptron()),\n    ("Passive-Aggressive I", PassiveAggressiveClassifier(loss='hinge',\n                                                         C=1.0)),\n    ("Passive-Aggressive II", PassiveAggressiveClassifier(loss='squared_hinge',\n                                                          C=1.0)),\n    ("SAG", LogisticRegression(solver='sag', tol=1e-1, C=1.e4 / X.shape[0]))\n]\n\nxx = 1. - np.array(heldout)\n\nfor name, clf in classifiers:\n    print("training %s" % name)\n    rng = np.random.RandomState(42)\n    yy = []\n    for i in heldout:\n        yy_ = []\n        for r in range(rounds):\n            X_train, X_test, y_train, y_test = \\n                train_test_split(X, y, test_size=i, random_state=rng)\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            yy_.append(1 - np.mean(y_pred == y_test))\n        yy.append(np.mean(yy_))\n    plt.plot(xx, yy, label=name)\n\nplt.legend(loc="upper right")\nplt.xlabel("Proportion train")\nplt.ylabel("Test Error Rate")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_comparison.html
Plot multi-class SGD on the iris dataset	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot decision surface of multi-class SGD on iris dataset. The hyperplanes corresponding to the three one-versus-all (OVA) classifiers are represented by the dashed lines.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.linear_model import SGDClassifier\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = iris.target\ncolors = "bry"\n\n# shuffle\nidx = np.arange(X.shape[0])\nnp.random.seed(13)\nnp.random.shuffle(idx)\nX = X[idx]\ny = y[idx]\n\n# standardize\nmean = X.mean(axis=0)\nstd = X.std(axis=0)\nX = (X - mean) / std\n\nh = .02  # step size in the mesh\n\nclf = SGDClassifier(alpha=0.001, n_iter=100).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.axis('tight')\n\n# Plot also the training points\nfor i, color in zip(clf.classes_, colors):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                cmap=plt.cm.Paired)\nplt.title("Decision surface of multi-class SGD")\nplt.axis('tight')\n\n# Plot the three one-against-all classifiers\nxmin, xmax = plt.xlim()\nymin, ymax = plt.ylim()\ncoef = clf.coef_\nintercept = clf.intercept_\n\n\ndef plot_hyperplane(c, color):\n    def line(x0):\n        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n\n    plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n             ls="--", color=color)\n\nfor i, color in zip(clf.classes_, colors):\n    plot_hyperplane(i, color)\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_iris.html
SGD: convex loss functions	A							http://scikit-learn.org/stable/auto_examples/index.html			A plot that compares the various convex loss functions supported by sklearn.linear_model.SGDClassifier .<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z >= -1] = (1 - z[z >= -1]) ** 2\n    loss[z >= 1.] = 0\n    return loss\n\n\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], 'k-',\n         label="Zero-one loss")\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0), 'g-',\n         label="Hinge loss")\nplt.plot(xx, -np.minimum(xx, 0), 'm-',\n         label="Perceptron loss")\nplt.plot(xx, np.log2(1 + np.exp(-xx)), 'r-',\n         label="Log loss")\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, 'b-',\n         label="Squared hinge loss")\nplt.plot(xx, modified_huber_loss(xx, 1), 'y--',\n         label="Modified Huber loss")\nplt.ylim((0, 8))\nplt.legend(loc="upper right")\nplt.xlabel(r"Decision function $f(x)$")\nplt.ylabel("$L(y, f(x))$")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html
SGD: Penalties	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the contours of the three penalties.<br><br><pre><code>from __future__ import division\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef l1(xs):\n    return np.array([np.sqrt((1 - np.sqrt(x ** 2.0)) ** 2.0) for x in xs])\n\n\ndef l2(xs):\n    return np.array([np.sqrt(1.0 - x ** 2.0) for x in xs])\n\n\ndef el(xs, z):\n    return np.array([(2 - 2 * x - 2 * z + 4 * x * z -\n                      (4 * z ** 2\n                       - 8 * x * z ** 2\n                       + 8 * x ** 2 * z ** 2\n                       - 16 * x ** 2 * z ** 3\n                       + 8 * x * z ** 3 + 4 * x ** 2 * z ** 4) ** (1. / 2)\n                      - 2 * x * z ** 2) / (2 - 4 * z) for x in xs])\n\n\ndef cross(ext):\n    plt.plot([-ext, ext], [0, 0], "k-")\n    plt.plot([0, 0], [-ext, ext], "k-")\n\nxs = np.linspace(0, 1, 100)\n\nalpha = 0.501  # 0.5 division throuh zero\n\ncross(1.2)\n\nplt.plot(xs, l1(xs), "r-", label="L1")\nplt.plot(xs, -1.0 * l1(xs), "r-")\nplt.plot(-1 * xs, l1(xs), "r-")\nplt.plot(-1 * xs, -1.0 * l1(xs), "r-")\n\nplt.plot(xs, l2(xs), "b-", label="L2")\nplt.plot(xs, -1.0 * l2(xs), "b-")\nplt.plot(-1 * xs, l2(xs), "b-")\nplt.plot(-1 * xs, -1.0 * l2(xs), "b-")\n\nplt.plot(xs, el(xs, alpha), "y-", label="Elastic Net")\nplt.plot(xs, -1.0 * el(xs, alpha), "y-")\nplt.plot(-1 * xs, el(xs, alpha), "y-")\nplt.plot(-1 * xs, -1.0 * el(xs, alpha), "y-")\n\nplt.xlabel(r"$w_0$")\nplt.ylabel(r"$w_1$")\nplt.legend()\n\nplt.axis("equal")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_penalties.html
SGD: Maximum margin separating hyperplane	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the maximum margin separating hyperplane within a two-class separable dataset using a linear Support Vector Machines classifier trained using SGD.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets.samples_generator import make_blobs\n\n# we create 50 separable points\nX, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n# fit the model\nclf = SGDClassifier(loss="hinge", alpha=0.01, n_iter=200, fit_intercept=True)\nclf.fit(X, Y)\n\n# plot the line, the points, and the nearest vectors to the plane\nxx = np.linspace(-1, 5, 10)\nyy = np.linspace(-1, 5, 10)\n\nX1, X2 = np.meshgrid(xx, yy)\nZ = np.empty(X1.shape)\nfor (i, j), val in np.ndenumerate(X1):\n    x1 = val\n    x2 = X2[i, j]\n    p = clf.decision_function([[x1, x2]])\n    Z[i, j] = p[0]\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = ['dashed', 'solid', 'dashed']\ncolors = 'k'\nplt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired)\n\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_separating_hyperplane.html
SGD: Weighted samples	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot decision function of a weighted dataset, where the size of points is proportional to its weight.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\n# we create 20 points\nnp.random.seed(0)\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\ny = [1] * 10 + [-1] * 10\nsample_weight = 100 * np.abs(np.random.randn(20))\n# and assign a bigger weight to the last 10 samples\nsample_weight[:10] *= 10\n\n# plot the weighted data points\nxx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\nplt.figure()\nplt.scatter(X[:, 0], X[:, 1], c=y, s=sample_weight, alpha=0.9,\n            cmap=plt.cm.bone)\n\n## fit the unweighted model\nclf = linear_model.SGDClassifier(alpha=0.01, n_iter=100)\nclf.fit(X, y)\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nno_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=['solid'])\n\n## fit the weighted model\nclf = linear_model.SGDClassifier(alpha=0.01, n_iter=100)\nclf.fit(X, y, sample_weight=sample_weight)\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nsamples_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=['dashed'])\n\nplt.legend([no_weights.collections[0], samples_weights.collections[0]],\n           ["no weights", "with weights"], loc="lower left")\n\nplt.xticks(())\nplt.yticks(())\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_weighted_samples.html
Sparse coding with a precomputed dictionary	A							http://scikit-learn.org/stable/auto_examples/index.html			Transform a signal as a sparse combination of Ricker wavelets. This example visually compares different sparse coding methods using the sklearn.decomposition.SparseCoder estimator. The Ricker (also known as Mexican hat or the second derivative of a Gaussian) is not a particularly good kernel to represent piecewise constant signals like this one. It can therefore be seen how much adding different...<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pylab as pl\n\nfrom sklearn.decomposition import SparseCoder\n\n\ndef ricker_function(resolution, center, width):\n    """Discrete sub-sampled Ricker (Mexican hat) wavelet"""\n    x = np.linspace(0, resolution - 1, resolution)\n    x = ((2 / ((np.sqrt(3 * width) * np.pi ** 1 / 4)))\n         * (1 - ((x - center) ** 2 / width ** 2))\n         * np.exp((-(x - center) ** 2) / (2 * width ** 2)))\n    return x\n\n\ndef ricker_matrix(width, resolution, n_components):\n    """Dictionary of Ricker (Mexican hat) wavelets"""\n    centers = np.linspace(0, resolution - 1, n_components)\n    D = np.empty((n_components, resolution))\n    for i, center in enumerate(centers):\n        D[i] = ricker_function(resolution, center, width)\n    D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]\n    return D\n\n\nresolution = 1024\nsubsampling = 3  # subsampling factor\nwidth = 100\nn_components = resolution / subsampling\n\n# Compute a wavelet dictionary\nD_fixed = ricker_matrix(width=width, resolution=resolution,\n                        n_components=n_components)\nD_multi = np.r_[tuple(ricker_matrix(width=w, resolution=resolution,\n                                    n_components=np.floor(n_components / 5))\n                for w in (10, 50, 100, 500, 1000))]\n\n# Generate a signal\ny = np.linspace(0, resolution - 1, resolution)\nfirst_quarter = y < resolution / 4\ny[first_quarter] = 3.\ny[np.logical_not(first_quarter)] = -1.\n\n# List the different sparse coding methods in the following format:\n# (title, transform_algorithm, transform_alpha, transform_n_nozero_coefs)\nestimators = [('OMP', 'omp', None, 15), ('Lasso', 'lasso_cd', 2, None), ]\n\npl.figure(figsize=(13, 6))\nfor subplot, (D, title) in enumerate(zip((D_fixed, D_multi),\n                                         ('fixed width', 'multiple widths'))):\n    pl.subplot(1, 2, subplot + 1)\n    pl.title('Sparse coding against %s dictionary' % title)\n    pl.plot(y, ls='dotted', label='Original signal')\n    # Do a wavelet approximation\n    for title, algo, alpha, n_nonzero in estimators:\n        coder = SparseCoder(dictionary=D, transform_n_nonzero_coefs=n_nonzero,\n                            transform_alpha=alpha, transform_algorithm=algo)\n        x = coder.transform(y.reshape(1, -1))\n        density = len(np.flatnonzero(x))\n        x = np.ravel(np.dot(x, D))\n        squared_error = np.sum((y - x) ** 2)\n        pl.plot(x, label='%s: %s nonzero coefs,\n%.2f error'\n                % (title, density, squared_error))\n\n    # Soft thresholding debiasing\n    coder = SparseCoder(dictionary=D, transform_algorithm='threshold',\n                        transform_alpha=20)\n    x = coder.transform(y.reshape(1, -1))\n    _, idx = np.where(x != 0)\n    x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y)\n    x = np.ravel(np.dot(x, D))\n    squared_error = np.sum((y - x) ** 2)\n    pl.plot(x,\n            label='Thresholding w/ debiasing:\n%d nonzero coefs, %.2f error' %\n            (len(idx), squared_error))\n    pl.axis('tight')\n    pl.legend()\npl.subplots_adjust(.04, .07, .97, .90, .09, .2)\npl.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/decomposition/plot_sparse_coding.html
Sparse inverse covariance estimation	A							http://scikit-learn.org/stable/auto_examples/index.html			Using the GraphLasso estimator to learn a covariance and sparse precision from a small number of samples.<br><br><pre><code>print(__doc__)\n# author: Gael Varoquaux <gael.varoquaux@inria.fr>\n# License: BSD 3 clause\n# Copyright: INRIA\n\nimport numpy as np\nfrom scipy import linalg\nfrom sklearn.datasets import make_sparse_spd_matrix\nfrom sklearn.covariance import GraphLassoCV, ledoit_wolf\nimport matplotlib.pyplot as plt\n\n##############################################################################\n# Generate the data\nn_samples = 60\nn_features = 20\n\nprng = np.random.RandomState(1)\nprec = make_sparse_spd_matrix(n_features, alpha=.98,\n                              smallest_coef=.4,\n                              largest_coef=.7,\n                              random_state=prng)\ncov = linalg.inv(prec)\nd = np.sqrt(np.diag(cov))\ncov /= d\ncov /= d[:, np.newaxis]\nprec *= d\nprec *= d[:, np.newaxis]\nX = prng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\n##############################################################################\n# Estimate the covariance\nemp_cov = np.dot(X.T, X) / n_samples\n\nmodel = GraphLassoCV()\nmodel.fit(X)\ncov_ = model.covariance_\nprec_ = model.precision_\n\nlw_cov_, _ = ledoit_wolf(X)\nlw_prec_ = linalg.inv(lw_cov_)\n\n##############################################################################\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.subplots_adjust(left=0.02, right=0.98)\n\n# plot the covariances\ncovs = [('Empirical', emp_cov), ('Ledoit-Wolf', lw_cov_),\n        ('GraphLasso', cov_), ('True', cov)]\nvmax = cov_.max()\nfor i, (name, this_cov) in enumerate(covs):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(this_cov, interpolation='nearest', vmin=-vmax, vmax=vmax,\n               cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title('%s covariance' % name)\n\n\n# plot the precisions\nprecs = [('Empirical', linalg.inv(emp_cov)), ('Ledoit-Wolf', lw_prec_),\n         ('GraphLasso', prec_), ('True', prec)]\nvmax = .9 * prec_.max()\nfor i, (name, this_prec) in enumerate(precs):\n    ax = plt.subplot(2, 4, i + 5)\n    plt.imshow(np.ma.masked_equal(this_prec, 0),\n               interpolation='nearest', vmin=-vmax, vmax=vmax,\n               cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title('%s precision' % name)\n    ax.set_axis_bgcolor('.7')\n\n# plot the model selection metric\nplt.figure(figsize=(4, 3))\nplt.axes([.2, .15, .75, .7])\nplt.plot(model.cv_alphas_, np.mean(model.grid_scores, axis=1), 'o-')\nplt.axvline(model.alpha_, color='.5')\nplt.title('Model selection')\nplt.ylabel('Cross-validation score')\nplt.xlabel('alpha')\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/covariance/plot_sparse_cov.html
Sparse recovery: feature selection for sparse linear models	A							http://scikit-learn.org/stable/auto_examples/index.html			Given a small number of observations, we want to recover which features of X are relevant to explain y. For this sparse linear models can outperform standard statistical tests if the true model is sparse, i.e. if a small fraction of the features are relevant.<br><br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort and Gael Varoquaux\n# License: BSD 3 clause\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import linalg\n\nfrom sklearn.linear_model import (RandomizedLasso, lasso_stability_path,\n                                  LassoLarsCV)\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import auc, precision_recall_curve\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.utils.extmath import pinvh\nfrom sklearn.utils import ConvergenceWarning\n\n\ndef mutual_incoherence(X_relevant, X_irelevant):\n    """Mutual incoherence, as defined by formula (26a) of [Wainwright2006].\n    """\n    projector = np.dot(np.dot(X_irelevant.T, X_relevant),\n                       pinvh(np.dot(X_relevant.T, X_relevant)))\n    return np.max(np.abs(projector).sum(axis=1))\n\n\nfor conditioning in (1, 1e-4):\n    ###########################################################################\n    # Simulate regression data with a correlated design\n    n_features = 501\n    n_relevant_features = 3\n    noise_level = .2\n    coef_min = .2\n    # The Donoho-Tanner phase transition is around n_samples=25: below we\n    # will completely fail to recover in the well-conditioned case\n    n_samples = 25\n    block_size = n_relevant_features\n\n    rng = np.random.RandomState(42)\n\n    # The coefficients of our model\n    coef = np.zeros(n_features)\n    coef[:n_relevant_features] = coef_min + rng.rand(n_relevant_features)\n\n    # The correlation of our design: variables correlated by blocs of 3\n    corr = np.zeros((n_features, n_features))\n    for i in range(0, n_features, block_size):\n        corr[i:i + block_size, i:i + block_size] = 1 - conditioning\n    corr.flat[::n_features + 1] = 1\n    corr = linalg.cholesky(corr)\n\n    # Our design\n    X = rng.normal(size=(n_samples, n_features))\n    X = np.dot(X, corr)\n    # Keep [Wainwright2006] (26c) constant\n    X[:n_relevant_features] /= np.abs(\n        linalg.svdvals(X[:n_relevant_features])).max()\n    X = StandardScaler().fit_transform(X.copy())\n\n    # The output variable\n    y = np.dot(X, coef)\n    y /= np.std(y)\n    # We scale the added noise as a function of the average correlation\n    # between the design and the output variable\n    y += noise_level * rng.normal(size=n_samples)\n    mi = mutual_incoherence(X[:, :n_relevant_features],\n                            X[:, n_relevant_features:])\n\n    ###########################################################################\n    # Plot stability selection path, using a high eps for early stopping\n    # of the path, to save computation time\n    alpha_grid, scores_path = lasso_stability_path(X, y, random_state=42,\n                                                   eps=0.05)\n\n    plt.figure()\n    # We plot the path as a function of alpha/alpha_max to the power 1/3: the\n    # power 1/3 scales the path less brutally than the log, and enables to\n    # see the progression along the path\n    hg = plt.plot(alpha_grid[1:] ** .333, scores_path[coef != 0].T[1:], 'r')\n    hb = plt.plot(alpha_grid[1:] ** .333, scores_path[coef == 0].T[1:], 'k')\n    ymin, ymax = plt.ylim()\n    plt.xlabel(r'$(\alpha / \alpha_{max})^{1/3}$')\n    plt.ylabel('Stability score: proportion of times selected')\n    plt.title('Stability Scores Path - Mutual incoherence: %.1f' % mi)\n    plt.axis('tight')\n    plt.legend((hg[0], hb[0]), ('relevant features', 'irrelevant features'),\n               loc='best')\n\n    ###########################################################################\n    # Plot the estimated stability scores for a given alpha\n\n    # Use 6-fold cross-validation rather than the default 3-fold: it leads to\n    # a better choice of alpha:\n    # Stop the user warnings outputs- they are not necessary for the example\n    # as it is specifically set up to be challenging.\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore', UserWarning)\n        warnings.simplefilter('ignore', ConvergenceWarning)\n        lars_cv = LassoLarsCV(cv=6).fit(X, y)\n\n    # Run the RandomizedLasso: we use a paths going down to .1*alpha_max\n    # to avoid exploring the regime in which very noisy variables enter\n    # the model\n    alphas = np.linspace(lars_cv.alphas_[0], .1 * lars_cv.alphas_[0], 6)\n    clf = RandomizedLasso(alpha=alphas, random_state=42).fit(X, y)\n    trees = ExtraTreesRegressor(100).fit(X, y)\n    # Compare with F-score\n    F, _ = f_regression(X, y)\n\n    plt.figure()\n    for name, score in [('F-test', F),\n                        ('Stability selection', clf.scores_),\n                        ('Lasso coefs', np.abs(lars_cv.coef_)),\n                        ('Trees', trees.feature_importances_),\n                        ]:\n        precision, recall, thresholds = precision_recall_curve(coef != 0,\n                                                               score)\n        plt.semilogy(np.maximum(score / np.max(score), 1e-4),\n                     label="%s. AUC: %.3f" % (name, auc(recall, precision)))\n\n    plt.plot(np.where(coef != 0)[0], [2e-4] * n_relevant_features, 'mo',\n             label="Ground truth")\n    plt.xlabel("Features")\n    plt.ylabel("Score")\n    # Plot only the 100 first coefficients\n    plt.xlim(0, 100)\n    plt.legend(loc='best')\n    plt.title('Feature selection scores - Mutual incoherence: %.1f'\n              % mi)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_recovery.html
Species distribution modeling	A							http://scikit-learn.org/stable/auto_examples/index.html			Modeling species’ geographic distributions is an important problem in conservation biology. In this example we model the geographic distribution of two south american mammals given past observations and 14 environmental variables. Since we have only positive examples (there are no unsuccessful observations), we cast this problem as a density estimation problem and use the OneClassSVM provided by...<br><br><pre><code># Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#          Jake Vanderplas <vanderplas@astro.washington.edu>\n#\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets.base import Bunch\nfrom sklearn.datasets import fetch_species_distributions\nfrom sklearn.datasets.species_distributions import construct_grids\nfrom sklearn import svm, metrics\n\n# if basemap is available, we'll use it.\n# otherwise, we'll improvise later...\ntry:\n    from mpl_toolkits.basemap import Basemap\n    basemap = True\nexcept ImportError:\n    basemap = False\n\nprint(__doc__)\n\n\ndef create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):\n    """Create a bunch with information about a particular organism\n\n    This will use the test/train record arrays to extract the\n    data specific to the given species name.\n    """\n    bunch = Bunch(name=' '.join(species_name.split("_")[:2]))\n    species_name = species_name.encode('ascii')\n    points = dict(test=test, train=train)\n\n    for label, pts in points.items():\n        # choose points associated with the desired species\n        pts = pts[pts['species'] == species_name]\n        bunch['pts_%s' % label] = pts\n\n        # determine coverage values for each of the training & testing points\n        ix = np.searchsorted(xgrid, pts['dd long'])\n        iy = np.searchsorted(ygrid, pts['dd lat'])\n        bunch['cov_%s' % label] = coverages[:, -iy, ix].T\n\n    return bunch\n\n\ndef plot_species_distribution(species=("bradypus_variegatus_0",\n                                       "microryzomys_minutus_0")):\n    """\n    Plot the species distribution.\n    """\n    if len(species) > 2:\n        print("Note: when more than two species are provided,"\n              " only the first two will be used")\n\n    t0 = time()\n\n    # Load the compressed data\n    data = fetch_species_distributions()\n\n    # Set up the data grid\n    xgrid, ygrid = construct_grids(data)\n\n    # The grid in x,y coordinates\n    X, Y = np.meshgrid(xgrid, ygrid[::-1])\n\n    # create a bunch for each species\n    BV_bunch = create_species_bunch(species[0],\n                                    data.train, data.test,\n                                    data.coverages, xgrid, ygrid)\n    MM_bunch = create_species_bunch(species[1],\n                                    data.train, data.test,\n                                    data.coverages, xgrid, ygrid)\n\n    # background points (grid coordinates) for evaluation\n    np.random.seed(13)\n    background_points = np.c_[np.random.randint(low=0, high=data.Ny,\n                                                size=10000),\n                              np.random.randint(low=0, high=data.Nx,\n                                                size=10000)].T\n\n    # We'll make use of the fact that coverages[6] has measurements at all\n    # land points.  This will help us decide between land and water.\n    land_reference = data.coverages[6]\n\n    # Fit, predict, and plot for each species.\n    for i, species in enumerate([BV_bunch, MM_bunch]):\n        print("_" * 80)\n        print("Modeling distribution of species '%s'" % species.name)\n\n        # Standardize features\n        mean = species.cov_train.mean(axis=0)\n        std = species.cov_train.std(axis=0)\n        train_cover_std = (species.cov_train - mean) / std\n\n        # Fit OneClassSVM\n        print(" - fit OneClassSVM ... ", end='')\n        clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.5)\n        clf.fit(train_cover_std)\n        print("done.")\n\n        # Plot map of South America\n        plt.subplot(1, 2, i + 1)\n        if basemap:\n            print(" - plot coastlines using basemap")\n            m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n                        urcrnrlat=Y.max(), llcrnrlon=X.min(),\n                        urcrnrlon=X.max(), resolution='c')\n            m.drawcoastlines()\n            m.drawcountries()\n        else:\n            print(" - plot coastlines from coverage")\n            plt.contour(X, Y, land_reference,\n                        levels=[-9999], colors="k",\n                        linestyles="solid")\n            plt.xticks([])\n            plt.yticks([])\n\n        print(" - predict species distribution")\n\n        # Predict species distribution using the training data\n        Z = np.ones((data.Ny, data.Nx), dtype=np.float64)\n\n        # We'll predict only for the land points.\n        idx = np.where(land_reference > -9999)\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\n\n        pred = clf.decision_function((coverages_land - mean) / std)[:, 0]\n        Z *= pred.min()\n        Z[idx[0], idx[1]] = pred\n\n        levels = np.linspace(Z.min(), Z.max(), 25)\n        Z[land_reference == -9999] = -9999\n\n        # plot contours of the prediction\n        plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n        plt.colorbar(format='%.2f')\n\n        # scatter training/testing points\n        plt.scatter(species.pts_train['dd long'], species.pts_train['dd lat'],\n                    s=2 ** 2, c='black',\n                    marker='^', label='train')\n        plt.scatter(species.pts_test['dd long'], species.pts_test['dd lat'],\n                    s=2 ** 2, c='black',\n                    marker='x', label='test')\n        plt.legend()\n        plt.title(species.name)\n        plt.axis('equal')\n\n        # Compute AUC with regards to background points\n        pred_background = Z[background_points[0], background_points[1]]\n        pred_test = clf.decision_function((species.cov_test - mean)\n                                          / std)[:, 0]\n        scores = np.r_[pred_test, pred_background]\n        y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]\n        fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n        roc_auc = metrics.auc(fpr, tpr)\n        plt.text(-35, -70, "AUC: %.3f" % roc_auc, ha="right")\n        print("\n Area under the ROC curve : %f" % roc_auc)\n\n    print("\ntime elapsed: %.2fs" % (time() - t0))\n\n\nplot_species_distribution()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_species_distribution_modeling.html
Kernel Density Estimate of Species Distributions	A							http://scikit-learn.org/stable/auto_examples/index.html			This shows an example of a neighbors-based query (in particular a kernel density estimate) on geospatial data, using a Ball Tree built upon the Haversine distance metric – i.e. distances over points in latitude/longitude. The dataset is provided by Phillips et. al. (2006). If available, the example uses basemap to plot the coast lines and national boundaries of South America.<br><br><pre><code># Author: Jake Vanderplas <jakevdp@cs.washington.edu>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_species_distributions\nfrom sklearn.datasets.species_distributions import construct_grids\nfrom sklearn.neighbors import KernelDensity\n\n# if basemap is available, we'll use it.\n# otherwise, we'll improvise later...\ntry:\n    from mpl_toolkits.basemap import Basemap\n    basemap = True\nexcept ImportError:\n    basemap = False\n\n# Get matrices/arrays of species IDs and locations\ndata = fetch_species_distributions()\nspecies_names = ['Bradypus Variegatus', 'Microryzomys Minutus']\n\nXtrain = np.vstack([data['train']['dd lat'],\n                    data['train']['dd long']]).T\nytrain = np.array([d.decode('ascii').startswith('micro')\n                  for d in data['train']['species']], dtype='int')\nXtrain *= np.pi / 180.  # Convert lat/long to radians\n\n# Set up the data grid for the contour plot\nxgrid, ygrid = construct_grids(data)\nX, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\nland_reference = data.coverages[6][::5, ::5]\nland_mask = (land_reference > -9999).ravel()\n\nxy = np.vstack([Y.ravel(), X.ravel()]).T\nxy = xy[land_mask]\nxy *= np.pi / 180.\n\n# Plot map of South America with distributions of each species\nfig = plt.figure()\nfig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n\nfor i in range(2):\n    plt.subplot(1, 2, i + 1)\n\n    # construct a kernel density estimate of the distribution\n    print(" - computing KDE in spherical coordinates")\n    kde = KernelDensity(bandwidth=0.04, metric='haversine',\n                        kernel='gaussian', algorithm='ball_tree')\n    kde.fit(Xtrain[ytrain == i])\n\n    # evaluate only on the land: -9999 indicates ocean\n    Z = -9999 + np.zeros(land_mask.shape[0])\n    Z[land_mask] = np.exp(kde.score_samples(xy))\n    Z = Z.reshape(X.shape)\n\n    # plot contours of the density\n    levels = np.linspace(0, Z.max(), 25)\n    plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n\n    if basemap:\n        print(" - plot coastlines using basemap")\n        m = Basemap(projection='cyl', llcrnrlat=Y.min(),\n                    urcrnrlat=Y.max(), llcrnrlon=X.min(),\n                    urcrnrlon=X.max(), resolution='c')\n        m.drawcoastlines()\n        m.drawcountries()\n    else:\n        print(" - plot coastlines from coverage")\n        plt.contour(X, Y, land_reference,\n                    levels=[-9999], colors="k",\n                    linestyles="solid")\n        plt.xticks([])\n        plt.yticks([])\n\n    plt.title(species_names[i])\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/neighbors/plot_species_kde.html
A demo of the Spectral Biclustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			This example demonstrates how to generate a checkerboard dataset and bicluster it using the Spectral Biclustering algorithm.<br><br><pre><code>print(__doc__)\n\n# Author: Kemal Eren <kemal@kemaleren.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_checkerboard\nfrom sklearn.datasets import samples_generator as sg\nfrom sklearn.cluster.bicluster import SpectralBiclustering\nfrom sklearn.metrics import consensus_score\n\nn_clusters = (4, 3)\ndata, rows, columns = make_checkerboard(\n    shape=(300, 300), n_clusters=n_clusters, noise=10,\n    shuffle=False, random_state=0)\n\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title("Original dataset")\n\ndata, row_idx, col_idx = sg._shuffle(data, random_state=0)\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title("Shuffled dataset")\n\nmodel = SpectralBiclustering(n_clusters=n_clusters, method='log',\n                             random_state=0)\nmodel.fit(data)\nscore = consensus_score(model.biclusters_,\n                        (rows[:, row_idx], columns[:, col_idx]))\n\nprint("consensus score: {:.1f}".format(score))\n\nfit_data = data[np.argsort(model.row_labels_)]\nfit_data = fit_data[:, np.argsort(model.column_labels_)]\n\nplt.matshow(fit_data, cmap=plt.cm.Blues)\nplt.title("After biclustering; rearranged to show biclusters")\n\nplt.matshow(np.outer(np.sort(model.row_labels_) + 1,\n                     np.sort(model.column_labels_) + 1),\n            cmap=plt.cm.Blues)\nplt.title("Checkerboard structure of rearranged data")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_biclustering.html
A demo of the Spectral Co-Clustering algorithm	A							http://scikit-learn.org/stable/auto_examples/index.html			This example demonstrates how to generate a dataset and bicluster it using the the Spectral Co-Clustering algorithm.<br><br><pre><code>print(__doc__)\n\n# Author: Kemal Eren <kemal@kemaleren.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_biclusters\nfrom sklearn.datasets import samples_generator as sg\nfrom sklearn.cluster.bicluster import SpectralCoclustering\nfrom sklearn.metrics import consensus_score\n\ndata, rows, columns = make_biclusters(\n    shape=(300, 300), n_clusters=5, noise=5,\n    shuffle=False, random_state=0)\n\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title("Original dataset")\n\ndata, row_idx, col_idx = sg._shuffle(data, random_state=0)\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title("Shuffled dataset")\n\nmodel = SpectralCoclustering(n_clusters=5, random_state=0)\nmodel.fit(data)\nscore = consensus_score(model.biclusters_,\n                        (rows[:, row_idx], columns[:, col_idx]))\n\nprint("consensus score: {:.3f}".format(score))\n\nfit_data = data[np.argsort(model.row_labels_)]\nfit_data = fit_data[:, np.argsort(model.column_labels_)]\n\nplt.matshow(fit_data, cmap=plt.cm.Blues)\nplt.title("After biclustering; rearranged to show biclusters")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/bicluster/plot_spectral_coclustering.html
Visualizing the stock market structure	A							http://scikit-learn.org/stable/auto_examples/index.html			This example employs several unsupervised learning techniques to extract the stock market structure from variations in historical quotes.<br><br><pre><code>print(__doc__)\n\n# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n# License: BSD 3 clause\n\nimport datetime\n\nimport numpy as np\nimport matplotlib.pyplot as plt\ntry:\n    from matplotlib.finance import quotes_historical_yahoo\nexcept ImportError:\n    from matplotlib.finance import quotes_historical_yahoo_ochl as quotes_historical_yahoo\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn import cluster, covariance, manifold\n\n###############################################################################\n# Retrieve the data from Internet\n\n# Choose a time period reasonnably calm (not too long ago so that we get\n# high-tech firms, and before the 2008 crash)\nd1 = datetime.datetime(2003, 1, 1)\nd2 = datetime.datetime(2008, 1, 1)\n\n# kraft symbol has now changed from KFT to MDLZ in yahoo\nsymbol_dict = {\n    'TOT': 'Total',\n    'XOM': 'Exxon',\n    'CVX': 'Chevron',\n    'COP': 'ConocoPhillips',\n    'VLO': 'Valero Energy',\n    'MSFT': 'Microsoft',\n    'IBM': 'IBM',\n    'TWX': 'Time Warner',\n    'CMCSA': 'Comcast',\n    'CVC': 'Cablevision',\n    'YHOO': 'Yahoo',\n    'DELL': 'Dell',\n    'HPQ': 'HP',\n    'AMZN': 'Amazon',\n    'TM': 'Toyota',\n    'CAJ': 'Canon',\n    'MTU': 'Mitsubishi',\n    'SNE': 'Sony',\n    'F': 'Ford',\n    'HMC': 'Honda',\n    'NAV': 'Navistar',\n    'NOC': 'Northrop Grumman',\n    'BA': 'Boeing',\n    'KO': 'Coca Cola',\n    'MMM': '3M',\n    'MCD': 'Mc Donalds',\n    'PEP': 'Pepsi',\n    'MDLZ': 'Kraft Foods',\n    'K': 'Kellogg',\n    'UN': 'Unilever',\n    'MAR': 'Marriott',\n    'PG': 'Procter Gamble',\n    'CL': 'Colgate-Palmolive',\n    'GE': 'General Electrics',\n    'WFC': 'Wells Fargo',\n    'JPM': 'JPMorgan Chase',\n    'AIG': 'AIG',\n    'AXP': 'American express',\n    'BAC': 'Bank of America',\n    'GS': 'Goldman Sachs',\n    'AAPL': 'Apple',\n    'SAP': 'SAP',\n    'CSCO': 'Cisco',\n    'TXN': 'Texas instruments',\n    'XRX': 'Xerox',\n    'LMT': 'Lookheed Martin',\n    'WMT': 'Wal-Mart',\n    'WBA': 'Walgreen',\n    'HD': 'Home Depot',\n    'GSK': 'GlaxoSmithKline',\n    'PFE': 'Pfizer',\n    'SNY': 'Sanofi-Aventis',\n    'NVS': 'Novartis',\n    'KMB': 'Kimberly-Clark',\n    'R': 'Ryder',\n    'GD': 'General Dynamics',\n    'RTN': 'Raytheon',\n    'CVS': 'CVS',\n    'CAT': 'Caterpillar',\n    'DD': 'DuPont de Nemours'}\n\nsymbols, names = np.array(list(symbol_dict.items())).T\n\nquotes = [quotes_historical_yahoo(symbol, d1, d2, asobject=True)\n          for symbol in symbols]\n\nopen = np.array([q.open for q in quotes]).astype(np.float)\nclose = np.array([q.close for q in quotes]).astype(np.float)\n\n# The daily variations of the quotes are what carry most information\nvariation = close - open\n\n###############################################################################\n# Learn a graphical structure from the correlations\nedge_model = covariance.GraphLassoCV()\n\n# standardize the time series: using correlations rather than covariance\n# is more efficient for structure recovery\nX = variation.copy().T\nX /= X.std(axis=0)\nedge_model.fit(X)\n\n###############################################################################\n# Cluster using affinity propagation\n\n_, labels = cluster.affinity_propagation(edge_model.covariance_)\nn_labels = labels.max()\n\nfor i in range(n_labels + 1):\n    print('Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i])))\n\n###############################################################################\n# Find a low-dimension embedding for visualization: find the best position of\n# the nodes (the stocks) on a 2D plane\n\n# We use a dense eigen_solver to achieve reproducibility (arpack is\n# initiated with random vectors that we don't control). In addition, we\n# use a large number of neighbors to capture the large-scale structure.\nnode_position_model = manifold.LocallyLinearEmbedding(\n    n_components=2, eigen_solver='dense', n_neighbors=6)\n\nembedding = node_position_model.fit_transform(X.T).T\n\n###############################################################################\n# Visualization\nplt.figure(1, facecolor='w', figsize=(10, 8))\nplt.clf()\nax = plt.axes([0., 0., 1., 1.])\nplt.axis('off')\n\n# Display a graph of the partial correlations\npartial_correlations = edge_model.precision_.copy()\nd = 1 / np.sqrt(np.diag(partial_correlations))\npartial_correlations *= d\npartial_correlations *= d[:, np.newaxis]\nnon_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n\n# Plot the nodes using the coordinates of our embedding\nplt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n            cmap=plt.cm.spectral)\n\n# Plot the edges\nstart_idx, end_idx = np.where(non_zero)\n#a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [[embedding[:, start], embedding[:, stop]]\n            for start, stop in zip(start_idx, end_idx)]\nvalues = np.abs(partial_correlations[non_zero])\nlc = LineCollection(segments,\n                    zorder=0, cmap=plt.cm.hot_r,\n                    norm=plt.Normalize(0, .7 * values.max()))\nlc.set_array(values)\nlc.set_linewidths(15 * values)\nax.add_collection(lc)\n\n# Add a label to each node. The challenge here is that we want to\n# position the labels to avoid overlap with other labels\nfor index, (name, label, (x, y)) in enumerate(\n        zip(names, labels, embedding.T)):\n\n    dx = x - embedding[0]\n    dx[index] = 1\n    dy = y - embedding[1]\n    dy[index] = 1\n    this_dx = dx[np.argmin(np.abs(dy))]\n    this_dy = dy[np.argmin(np.abs(dx))]\n    if this_dx > 0:\n        horizontalalignment = 'left'\n        x = x + .002\n    else:\n        horizontalalignment = 'right'\n        x = x - .002\n    if this_dy > 0:\n        verticalalignment = 'bottom'\n        y = y + .002\n    else:\n        verticalalignment = 'top'\n        y = y - .002\n    plt.text(x, y, name, size=10,\n             horizontalalignment=horizontalalignment,\n             verticalalignment=verticalalignment,\n             bbox=dict(facecolor='w',\n                       edgecolor=plt.cm.spectral(label / float(n_labels)),\n                       alpha=.6))\n\nplt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n         embedding[0].max() + .10 * embedding[0].ptp(),)\nplt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n         embedding[1].max() + .03 * embedding[1].ptp())\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_stock_market.html
SVM-Anova: SVM with univariate feature selection	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows how to perform univariate feature before running a SVC (support vector classifier) to improve the classification scores.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets, feature_selection, cross_validation\nfrom sklearn.pipeline import Pipeline\n\n###############################################################################\n# Import some data to play with\ndigits = datasets.load_digits()\ny = digits.target\n# Throw away data, to be in the curse of dimension settings\ny = y[:200]\nX = digits.data[:200]\nn_samples = len(y)\nX = X.reshape((n_samples, -1))\n# add 200 non-informative features\nX = np.hstack((X, 2 * np.random.random((n_samples, 200))))\n\n###############################################################################\n# Create a feature-selection transform and an instance of SVM that we\n# combine together to have an full-blown estimator\n\ntransform = feature_selection.SelectPercentile(feature_selection.f_classif)\n\nclf = Pipeline([('anova', transform), ('svc', svm.SVC(C=1.0))])\n\n###############################################################################\n# Plot the cross-validation score as a function of percentile of features\nscore_means = list()\nscore_stds = list()\npercentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n\nfor percentile in percentiles:\n    clf.set_params(anova__percentile=percentile)\n    # Compute cross-validation score using all CPUs\n    this_scores = cross_validation.cross_val_score(clf, X, y, n_jobs=1)\n    score_means.append(this_scores.mean())\n    score_stds.append(this_scores.std())\n\nplt.errorbar(percentiles, score_means, np.array(score_stds))\n\nplt.title(\n    'Performance of the SVM-Anova varying the percentile of features selected')\nplt.xlabel('Percentile')\nplt.ylabel('Prediction rate')\n\nplt.axis('tight')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_anova.html
SVM-Kernels	A							http://scikit-learn.org/stable/auto_examples/index.html			Three different types of SVM-Kernels are displayed below. The polynomial and RBF are especially useful when the data-points are not linearly separable.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\n# Our dataset and targets\nX = np.c_[(.4, -.7),\n          (-1.5, -1),\n          (-1.4, -.9),\n          (-1.3, -1.2),\n          (-1.1, -.2),\n          (-1.2, -.4),\n          (-.5, 1.2),\n          (-1.5, 2.1),\n          (1, 1),\n          # --\n          (1.3, .8),\n          (1.2, .5),\n          (.2, -2),\n          (.5, -2.4),\n          (.2, -2.3),\n          (0, -2.7),\n          (1.3, 2.1)].T\nY = [0] * 8 + [1] * 8\n\n# figure number\nfignum = 1\n\n# fit the model\nfor kernel in ('linear', 'poly', 'rbf'):\n    clf = svm.SVC(kernel=kernel, gamma=2)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors='none', zorder=10)\n    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)\n\n    plt.axis('tight')\n    x_min = -3\n    x_max = 3\n    y_min = -3\n    y_max = 3\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n                levels=[-.5, 0, .5])\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html
SVM Margins Example	A							http://scikit-learn.org/stable/auto_examples/index.html			The plots below illustrate the effect the parameter C has on the separation line. A large value of C basically tells our model that we do not have that much faith in our data’s distribution, and will only consider points close to line of separation.<br><br><pre><code>print(__doc__)\n\n\n# Code source: Gaël Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# we create 40 separable points\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# figure number\nfignum = 1\n\n# fit the model\nfor name, penalty in (('unreg', 1), ('reg', 0.05)):\n\n    clf = svm.SVC(kernel='linear', C=penalty)\n    clf.fit(X, Y)\n\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(-5, 5)\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n\n    # plot the parallels to the separating hyperplane that pass through the\n    # support vectors\n    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n    yy_down = yy + a * margin\n    yy_up = yy - a * margin\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    plt.plot(xx, yy, 'k-')\n    plt.plot(xx, yy_down, 'k--')\n    plt.plot(xx, yy_up, 'k--')\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors='none', zorder=10)\n    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)\n\n    plt.axis('tight')\n    x_min = -4.8\n    x_max = 4.2\n    y_min = -6\n    y_max = 6\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_margin.html
Non-linear SVM	A							http://scikit-learn.org/stable/auto_examples/index.html			Perform binary classification using non-linear SVC with RBF kernel. The target to predict is a XOR of the inputs.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\nxx, yy = np.meshgrid(np.linspace(-3, 3, 500),\n                     np.linspace(-3, 3, 500))\nnp.random.seed(0)\nX = np.random.randn(300, 2)\nY = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n\n# fit the model\nclf = svm.NuSVC()\nclf.fit(X, Y)\n\n# plot the decision function for each datapoint on the grid\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect='auto',\n           origin='lower', cmap=plt.cm.PuOr_r)\ncontours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,\n                       linetypes='--')\nplt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired)\nplt.xticks(())\nplt.yticks(())\nplt.axis([-3, 3, -3, 3])\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_nonlinear.html
Support Vector Regression (SVR) using linear and non-linear kernels	A							http://scikit-learn.org/stable/auto_examples/index.html			Toy example of 1D regression using linear, polynomial and RBF kernels.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\n\n###############################################################################\n# Generate sample data\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\n\n###############################################################################\n# Add noise to targets\ny[::5] += 3 * (0.5 - np.random.rand(8))\n\n###############################################################################\n# Fit regression model\nsvr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\nsvr_lin = SVR(kernel='linear', C=1e3)\nsvr_poly = SVR(kernel='poly', C=1e3, degree=2)\ny_rbf = svr_rbf.fit(X, y).predict(X)\ny_lin = svr_lin.fit(X, y).predict(X)\ny_poly = svr_poly.fit(X, y).predict(X)\n\n###############################################################################\n# look at the results\nplt.scatter(X, y, c='k', label='data')\nplt.hold('on')\nplt.plot(X, y_rbf, c='g', label='RBF model')\nplt.plot(X, y_lin, c='r', label='Linear model')\nplt.plot(X, y_poly, c='b', label='Polynomial model')\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('Support Vector Regression')\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html
Scaling the regularization parameter for SVCs	A							http://scikit-learn.org/stable/auto_examples/index.html			The following example illustrates the effect of scaling the regularization parameter when using Support Vector Machines for classification. For SVC classification, we are interested in a risk minimization for the equation:<br><br><pre><code>print(__doc__)\n\n\n# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#         Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.cross_validation import ShuffleSplit\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.utils import check_random_state\nfrom sklearn import datasets\n\n\nrnd = check_random_state(1)\n\n# set up dataset\nn_samples = 100\nn_features = 300\n\n# l1 data (only 5 informative features)\nX_1, y_1 = datasets.make_classification(n_samples=n_samples,\n                                        n_features=n_features, n_informative=5,\n                                        random_state=1)\n\n# l2 data: non sparse, but less features\ny_2 = np.sign(.5 - rnd.rand(n_samples))\nX_2 = rnd.randn(n_samples, n_features / 5) + y_2[:, np.newaxis]\nX_2 += 5 * rnd.randn(n_samples, n_features / 5)\n\nclf_sets = [(LinearSVC(penalty='l1', loss='squared_hinge', dual=False,\n                       tol=1e-3),\n             np.logspace(-2.3, -1.3, 10), X_1, y_1),\n            (LinearSVC(penalty='l2', loss='squared_hinge', dual=True,\n                       tol=1e-4),\n             np.logspace(-4.5, -2, 10), X_2, y_2)]\n\ncolors = ['b', 'g', 'r', 'c']\n\nfor fignum, (clf, cs, X, y) in enumerate(clf_sets):\n    # set up the plot for each regressor\n    plt.figure(fignum, figsize=(9, 10))\n\n    for k, train_size in enumerate(np.linspace(0.3, 0.7, 3)[::-1]):\n        param_grid = dict(C=cs)\n        # To get nice curve, we need a large number of iterations to\n        # reduce the variance\n        grid = GridSearchCV(clf, refit=False, param_grid=param_grid,\n                            cv=ShuffleSplit(n=n_samples, train_size=train_size,\n                                            n_iter=250, random_state=1))\n        grid.fit(X, y)\n        scores = [x[1] for x in grid.grid_scores_]\n\n        scales = [(1, 'No scaling'),\n                  ((n_samples * train_size), '1/n_samples'),\n                  ]\n\n        for subplotnum, (scaler, name) in enumerate(scales):\n            plt.subplot(2, 1, subplotnum + 1)\n            plt.xlabel('C')\n            plt.ylabel('CV Score')\n            grid_cs = cs * float(scaler)  # scale the C's\n            plt.semilogx(grid_cs, scores, label="fraction %.2f" %\n                         train_size)\n            plt.title('scaling=%s, penalty=%s, loss=%s' %\n                      (name, clf.penalty, clf.loss))\n\n    plt.legend(loc="best")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_svm_scale_c.html
Swiss Roll reduction with LLE	A							http://scikit-learn.org/stable/auto_examples/index.html			An illustration of Swiss Roll reduction with locally linear embedding<br><br><pre><code># Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n# License: BSD 3 clause (C) INRIA 2011\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\n\n# This import is needed to modify the way figure behaves\nfrom mpl_toolkits.mplot3d import Axes3D\nAxes3D\n\n#----------------------------------------------------------------------\n# Locally linear embedding of the swiss roll\n\nfrom sklearn import manifold, datasets\nX, color = datasets.samples_generator.make_swiss_roll(n_samples=1500)\n\nprint("Computing LLE embedding")\nX_r, err = manifold.locally_linear_embedding(X, n_neighbors=12,\n                                             n_components=2)\nprint("Done. Reconstruction error: %g" % err)\n\n#----------------------------------------------------------------------\n# Plot result\n\nfig = plt.figure()\ntry:\n    # compatibility matplotlib < 1.0\n    ax = fig.add_subplot(211, projection='3d')\n    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\nexcept:\n    ax = fig.add_subplot(211)\n    ax.scatter(X[:, 0], X[:, 2], c=color, cmap=plt.cm.Spectral)\n\nax.set_title("Original data")\nax = fig.add_subplot(212)\nax.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.axis('tight')\nplt.xticks([]), plt.yticks([])\nplt.title('Projected data')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/manifold/plot_swissroll.html
Theil-Sen Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			Computes a Theil-Sen Regression on a synthetic dataset.<br><br><pre><code># Author: Florian Wilhelm -- <florian.wilhelm@gmail.com>\n# License: BSD 3 clause\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, TheilSenRegressor\nfrom sklearn.linear_model import RANSACRegressor\n\nprint(__doc__)\n\nestimators = [('OLS', LinearRegression()),\n              ('Theil-Sen', TheilSenRegressor(random_state=42)),\n              ('RANSAC', RANSACRegressor(random_state=42)), ]\n\n##############################################################################\n# Outliers only in the y direction\n\nnp.random.seed(0)\nn_samples = 200\n# Linear model y = 3*x + N(2, 0.1**2)\nx = np.random.randn(n_samples)\nw = 3.\nc = 2.\nnoise = 0.1 * np.random.randn(n_samples)\ny = w * x + c + noise\n# 10% outliers\ny[-20:] += -20 * x[-20:]\nX = x[:, np.newaxis]\n\nplt.plot(x, y, 'k+', mew=2, ms=8)\nline_x = np.array([-3, 3])\nfor name, estimator in estimators:\n    t0 = time.time()\n    estimator.fit(X, y)\n    elapsed_time = time.time() - t0\n    y_pred = estimator.predict(line_x.reshape(2, 1))\n    plt.plot(line_x, y_pred,\n             label='%s (fit time: %.2fs)' % (name, elapsed_time))\n\nplt.axis('tight')\nplt.legend(loc='upper left')\n\n\n##############################################################################\n# Outliers in the X direction\n\nnp.random.seed(0)\n# Linear model y = 3*x + N(2, 0.1**2)\nx = np.random.randn(n_samples)\nnoise = 0.1 * np.random.randn(n_samples)\ny = 3 * x + 2 + noise\n# 10% outliers\nx[-20:] = 9.9\ny[-20:] += 22\nX = x[:, np.newaxis]\n\nplt.figure()\nplt.plot(x, y, 'k+', mew=2, ms=8)\n\nline_x = np.array([-3, 10])\nfor name, estimator in estimators:\n    t0 = time.time()\n    estimator.fit(X, y)\n    elapsed_time = time.time() - t0\n    y_pred = estimator.predict(line_x.reshape(2, 1))\n    plt.plot(line_x, y_pred,\n             label='%s (fit time: %.2fs)' % (name, elapsed_time))\n\nplt.axis('tight')\nplt.legend(loc='upper left')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/linear_model/plot_theilsen.html
Compressive sensing: tomography reconstruction with L1 prior (Lasso)	A							http://scikit-learn.org/stable/auto_examples/index.html			This example shows the reconstruction of an image from a set of parallel projections, acquired along different angles. Such a dataset is acquired in computed tomography (CT).<br><br><pre><code>print(__doc__)\n\n# Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import sparse\nfrom scipy import ndimage\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ndef _weights(x, dx=1, orig=0):\n    x = np.ravel(x)\n    floor_x = np.floor((x - orig) / dx)\n    alpha = (x - orig - floor_x * dx) / dx\n    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))\n\n\ndef _generate_center_coordinates(l_x):\n    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\n    center = l_x / 2.\n    X += 0.5 - center\n    Y += 0.5 - center\n    return X, Y\n\n\ndef build_projection_operator(l_x, n_dir):\n    """ Compute the tomography design matrix.\n\n    Parameters\n    ----------\n\n    l_x : int\n        linear size of image array\n\n    n_dir : int\n        number of angles at which projections are acquired.\n\n    Returns\n    -------\n    p : sparse matrix of shape (n_dir l_x, l_x**2)\n    """\n    X, Y = _generate_center_coordinates(l_x)\n    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\n    data_inds, weights, camera_inds = [], [], []\n    data_unravel_indices = np.arange(l_x ** 2)\n    data_unravel_indices = np.hstack((data_unravel_indices,\n                                      data_unravel_indices))\n    for i, angle in enumerate(angles):\n        Xrot = np.cos(angle) * X - np.sin(angle) * Y\n        inds, w = _weights(Xrot, dx=1, orig=X.min())\n        mask = np.logical_and(inds >= 0, inds < l_x)\n        weights += list(w[mask])\n        camera_inds += list(inds[mask] + i * l_x)\n        data_inds += list(data_unravel_indices[mask])\n    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\n    return proj_operator\n\n\ndef generate_synthetic_data():\n    """ Synthetic binary data """\n    rs = np.random.RandomState(0)\n    n_pts = 36.\n    x, y = np.ogrid[0:l, 0:l]\n    mask_outer = (x - l / 2) ** 2 + (y - l / 2) ** 2 < (l / 2) ** 2\n    mask = np.zeros((l, l))\n    points = l * rs.rand(2, n_pts)\n    mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n    res = np.logical_and(mask > mask.mean(), mask_outer)\n    return res - ndimage.binary_erosion(res)\n\n\n# Generate synthetic images, and projections\nl = 128\nproj_operator = build_projection_operator(l, l / 7.)\ndata = generate_synthetic_data()\nproj = proj_operator * data.ravel()[:, np.newaxis]\nproj += 0.15 * np.random.randn(*proj.shape)\n\n# Reconstruction with L2 (Ridge) penalization\nrgr_ridge = Ridge(alpha=0.2)\nrgr_ridge.fit(proj_operator, proj.ravel())\nrec_l2 = rgr_ridge.coef_.reshape(l, l)\n\n# Reconstruction with L1 (Lasso) penalization\n# the best value of alpha was determined using cross validation\n# with LassoCV\nrgr_lasso = Lasso(alpha=0.001)\nrgr_lasso.fit(proj_operator, proj.ravel())\nrec_l1 = rgr_lasso.coef_.reshape(l, l)\n\nplt.figure(figsize=(8, 3.3))\nplt.subplot(131)\nplt.imshow(data, cmap=plt.cm.gray, interpolation='nearest')\nplt.axis('off')\nplt.title('original image')\nplt.subplot(132)\nplt.imshow(rec_l2, cmap=plt.cm.gray, interpolation='nearest')\nplt.title('L2 penalization')\nplt.axis('off')\nplt.subplot(133)\nplt.imshow(rec_l1, cmap=plt.cm.gray, interpolation='nearest')\nplt.title('L1 penalization')\nplt.axis('off')\n\nplt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,\n                    right=1)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/plot_tomography_l1_reconstruction.html
Train error vs Test error	A							http://scikit-learn.org/stable/auto_examples/index.html			Illustration of how the performance of an estimator on unseen data (test data) is not the same as the performance on training data. As the regularization increases the performance on train decreases while the performance on test is optimal within a range of values of the regularization parameter. The example with an Elastic-Net regression model and the performance is measured using the explained...<br><br><pre><code>print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom sklearn import linear_model\n\n###############################################################################\n# Generate sample data\nn_samples_train, n_samples_test, n_features = 75, 150, 500\nnp.random.seed(0)\ncoef = np.random.randn(n_features)\ncoef[50:] = 0.0  # only the top 10 features are impacting the model\nX = np.random.randn(n_samples_train + n_samples_test, n_features)\ny = np.dot(X, coef)\n\n# Split train and test data\nX_train, X_test = X[:n_samples_train], X[n_samples_train:]\ny_train, y_test = y[:n_samples_train], y[n_samples_train:]\n\n###############################################################################\n# Compute train and test errors\nalphas = np.logspace(-5, 1, 60)\nenet = linear_model.ElasticNet(l1_ratio=0.7)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = np.argmax(test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint("Optimal regularization parameter : %s" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_\n\n###############################################################################\n# Plot results functions\n\nimport matplotlib.pyplot as plt\nplt.subplot(2, 1, 1)\nplt.semilogx(alphas, train_errors, label='Train')\nplt.semilogx(alphas, test_errors, label='Test')\nplt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',\n           linewidth=3, label='Optimum on test')\nplt.legend(loc='lower left')\nplt.ylim([0, 1.2])\nplt.xlabel('Regularization parameter')\nplt.ylabel('Performance')\n\n# Show estimated coef_ vs true coef\nplt.subplot(2, 1, 2)\nplt.plot(coef, label='True coef')\nplt.plot(coef_, label='Estimated coef')\nplt.legend()\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_train_error_vs_test_error.html
Decision Tree Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			A 1D regression with decision tree.<br><br><pre><code>print(__doc__)\n\n# Import the necessary modules and libraries\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c="k", label="data")\nplt.plot(X_test, y_1, c="g", label="max_depth=2", linewidth=2)\nplt.plot(X_test, y_2, c="r", label="max_depth=5", linewidth=2)\nplt.xlabel("data")\nplt.ylabel("target")\nplt.title("Decision Tree Regression")\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html
Multi-output Decision Tree Regression	A							http://scikit-learn.org/stable/auto_examples/index.html			An example to illustrate multi-output regression with decision tree.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(200 * rng.rand(100, 1) - 100, axis=0)\ny = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\ny[::5, :] += (0.5 - rng.rand(20, 2))\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_3 = DecisionTreeRegressor(max_depth=8)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\nregr_3.fit(X, y)\n\n# Predict\nX_test = np.arange(-100.0, 100.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\ny_3 = regr_3.predict(X_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(y[:, 0], y[:, 1], c="k", label="data")\nplt.scatter(y_1[:, 0], y_1[:, 1], c="g", label="max_depth=2")\nplt.scatter(y_2[:, 0], y_2[:, 1], c="r", label="max_depth=5")\nplt.scatter(y_3[:, 0], y_3[:, 1], c="b", label="max_depth=8")\nplt.xlim([-6, 6])\nplt.ylim([-6, 6])\nplt.xlabel("data")\nplt.ylabel("target")\nplt.title("Multi-output Decision Tree Regression")\nplt.legend()\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html
Underfitting vs. Overfitting	A							http://scikit-learn.org/stable/auto_examples/index.html			This example demonstrates the problems of underfitting and overfitting and how we can use linear regression with polynomial features to approximate nonlinear functions. The plot shows the function that we want to approximate, which is a part of the cosine function. In addition, the samples from the real function and the approximations of different models are displayed. The models have polynomial...<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import cross_validation\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\ntrue_fun = lambda X: np.cos(1.5 * np.pi * X)\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i],\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([("polynomial_features", polynomial_features),\n                         ("linear_regression", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_validation.cross_val_score(pipeline,\n        X[:, np.newaxis], y, scoring="mean_squared_error", cv=10)\n\n    X_test = np.linspace(0, 1, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")\n    plt.plot(X_test, true_fun(X_test), label="True function")\n    plt.scatter(X, y, label="Samples")\n    plt.xlabel("x")\n    plt.ylabel("y")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc="best")\n    plt.title("Degree {}\nMSE = {:.2e}(+/- {:.2e})".format(\n        degrees[i], -scores.mean(), scores.std()))\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html
Plotting Validation Curves	A							http://scikit-learn.org/stable/auto_examples/index.html			In this plot you can see the training scores and validation scores of an SVM for different values of the kernel parameter gamma. For very low values of gamma, you can see that both the training score and the validation score are low. This is called underfitting. Medium values of gamma will result in high values for both scores, i.e. the classifier is performing fairly well. If gamma is too high,...<br><br><pre><code>print(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom sklearn.learning_curve import validation_curve\n\ndigits = load_digits()\nX, y = digits.data, digits.target\n\nparam_range = np.logspace(-6, -1, 5)\ntrain_scores, test_scores = validation_curve(\n    SVC(), X, y, param_name="gamma", param_range=param_range,\n    cv=10, scoring="accuracy", n_jobs=1)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.title("Validation Curve with SVM")\nplt.xlabel("$\gamma$")\nplt.ylabel("Score")\nplt.ylim(0.0, 1.1)\nplt.semilogx(param_range, train_scores_mean, label="Training score", color="r")\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2, color="r")\nplt.semilogx(param_range, test_scores_mean, label="Cross-validation score",\n             color="g")\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2, color="g")\nplt.legend(loc="best")\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html
Plot the decision boundaries of a VotingClassifier	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the decision boundaries of a VotingClassifier for two features of the Iris dataset.<br><br><pre><code>print(__doc__)\n\nfrom itertools import product\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0, 2]]\ny = iris.target\n\n# Training classifiers\nclf1 = DecisionTreeClassifier(max_depth=4)\nclf2 = KNeighborsClassifier(n_neighbors=7)\nclf3 = SVC(kernel='rbf', probability=True)\neclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2),\n                                    ('svc', clf3)],\n                        voting='soft', weights=[2, 1, 2])\n\nclf1.fit(X, y)\nclf2.fit(X, y)\nclf3.fit(X, y)\neclf.fit(X, y)\n\n# Plotting decision regions\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        [clf1, clf2, clf3, eclf],\n                        ['Decision Tree (depth=4)', 'KNN (k=7)',\n                         'Kernel SVM', 'Soft Voting']):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html
Plot class probabilities calculated by the VotingClassifier	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot the class probabilities of the first sample in a toy dataset predicted by three different classifiers and averaged by the VotingClassifier.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nclf1 = LogisticRegression(random_state=123)\nclf2 = RandomForestClassifier(random_state=123)\nclf3 = GaussianNB()\nX = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\ny = np.array([1, 1, 2, 2])\n\neclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n                        voting='soft',\n                        weights=[1, 1, 5])\n\n# predict class probabilities for all classifiers\nprobas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n\n# get class probabilities for the first sample in the dataset\nclass1_1 = [pr[0, 0] for pr in probas]\nclass2_1 = [pr[0, 1] for pr in probas]\n\n\n# plotting\n\nN = 4  # number of groups\nind = np.arange(N)  # group positions\nwidth = 0.35  # bar width\n\nfig, ax = plt.subplots()\n\n# bars for classifier 1-3\np1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color='green')\np2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color='lightgreen')\n\n# bars for VotingClassifier\np3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue')\np4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')\n\n# plot annotations\nplt.axvline(2.8, color='k', linestyle='dashed')\nax.set_xticks(ind + width)\nax.set_xticklabels(['LogisticRegression\nweight 1',\n                    'GaussianNB\nweight 1',\n                    'RandomForestClassifier\nweight 5',\n                    'VotingClassifier\n(average probabilities)'],\n                   rotation=40,\n                   ha='right')\nplt.ylim([0, 1])\nplt.title('Class probabilities for sample 1 by different classifiers')\nplt.legend([p1[0], p2[0]], ['class 1', 'class 2'], loc='upper left')\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html
Hierarchical clustering: structured vs unstructured ward	A							http://scikit-learn.org/stable/auto_examples/index.html			Example builds a swiss roll dataset and runs hierarchical clustering on their position.<br><br><pre><code># Authors : Vincent Michel, 2010\n#           Alexandre Gramfort, 2010\n#           Gael Varoquaux, 2010\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport time as time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d.axes3d as p3\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets.samples_generator import make_swiss_roll\n\n###############################################################################\n# Generate data (swiss roll dataset)\nn_samples = 1500\nnoise = 0.05\nX, _ = make_swiss_roll(n_samples, noise)\n# Make it thinner\nX[:, 1] *= .5\n\n###############################################################################\n# Compute clustering\nprint("Compute unstructured hierarchical clustering...")\nst = time.time()\nward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(X)\nelapsed_time = time.time() - st\nlabel = ward.labels_\nprint("Elapsed time: %.2fs" % elapsed_time)\nprint("Number of points: %i" % label.size)\n\n###############################################################################\n# Plot result\nfig = plt.figure()\nax = p3.Axes3D(fig)\nax.view_init(7, -80)\nfor l in np.unique(label):\n    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n              'o', color=plt.cm.jet(np.float(l) / np.max(label + 1)))\nplt.title('Without connectivity constraints (time %.2fs)' % elapsed_time)\n\n\n###############################################################################\n# Define the structure A of the data. Here a 10 nearest neighbors\nfrom sklearn.neighbors import kneighbors_graph\nconnectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\n\n###############################################################################\n# Compute clustering\nprint("Compute structured hierarchical clustering...")\nst = time.time()\nward = AgglomerativeClustering(n_clusters=6, connectivity=connectivity,\n                               linkage='ward').fit(X)\nelapsed_time = time.time() - st\nlabel = ward.labels_\nprint("Elapsed time: %.2fs" % elapsed_time)\nprint("Number of points: %i" % label.size)\n\n###############################################################################\n# Plot result\nfig = plt.figure()\nax = p3.Axes3D(fig)\nax.view_init(7, -80)\nfor l in np.unique(label):\n    ax.plot3D(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n              'o', color=plt.cm.jet(float(l) / np.max(label + 1)))\nplt.title('With connectivity constraints (time %.2fs)' % elapsed_time)\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html
SVM: Weighted samples	A							http://scikit-learn.org/stable/auto_examples/index.html			Plot decision function of a weighted dataset, where the size of points is proportional to its weight.<br><br><pre><code>print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\ndef plot_decision_function(classifier, sample_weight, axis, title):\n    # plot the decision function\n    xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\n\n    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\n    axis.scatter(X[:, 0], X[:, 1], c=Y, s=100 * sample_weight, alpha=0.9,\n                 cmap=plt.cm.bone)\n\n    axis.axis('off')\n    axis.set_title(title)\n\n\n# we create 20 points\nnp.random.seed(0)\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\nY = [1] * 10 + [-1] * 10\nsample_weight_last_ten = abs(np.random.randn(len(X)))\nsample_weight_constant = np.ones(len(X))\n# and bigger weights to some outliers\nsample_weight_last_ten[15:] *= 5\nsample_weight_last_ten[9] *= 15\n\n# for reference, first fit without class weights\n\n# fit the model\nclf_weights = svm.SVC()\nclf_weights.fit(X, Y, sample_weight=sample_weight_last_ten)\n\nclf_no_weights = svm.SVC()\nclf_no_weights.fit(X, Y)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nplot_decision_function(clf_no_weights, sample_weight_constant, axes[0],\n                       "Constant weights")\nplot_decision_function(clf_weights, sample_weight_last_ten, axes[1],\n                       "Modified weights")\n\nplt.show()</code></pre>	http://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html
Comparing randomized search and grid search for hyperparameter estimation	A							http://scikit-learn.org/stable/auto_examples/index.html			Compare randomized search and grid search for optimizing hyperparameters of a random forest. All parameters that influence the learning are searched simultaneously (except for the number of estimators, which poses a time / quality tradeoff).<br><br><pre><code>print(__doc__)\n\nimport numpy as np\n\nfrom time import time\nfrom operator import itemgetter\nfrom scipy.stats import randint as sp_randint\n\nfrom sklearn.grid_search import GridSearchCV, RandomizedSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\n\n# get some data\ndigits = load_digits()\nX, y = digits.data, digits.target\n\n# build a classifier\nclf = RandomForestClassifier(n_estimators=20)\n\n\n# Utility function to report best scores\ndef report(grid_scores, n_top=3):\n    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n    for i, score in enumerate(top_scores):\n        print("Model with rank: {0}".format(i + 1))\n        print("Mean validation score: {0:.3f} (std: {1:.3f})".format(\n              score.mean_validation_score,\n              np.std(score.cv_validation_scores)))\n        print("Parameters: {0}".format(score.parameters))\n        print("")\n\n\n# specify parameters and distributions to sample from\nparam_dist = {"max_depth": [3, None],\n              "max_features": sp_randint(1, 11),\n              "min_samples_split": sp_randint(1, 11),\n              "min_samples_leaf": sp_randint(1, 11),\n              "bootstrap": [True, False],\n              "criterion": ["gini", "entropy"]}\n\n# run randomized search\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search)\n\nstart = time()\nrandom_search.fit(X, y)\nprint("RandomizedSearchCV took %.2f seconds for %d candidates"\n      " parameter settings." % ((time() - start), n_iter_search))\nreport(random_search.grid_scores_)\n\n# use a full grid over all parameters\nparam_grid = {"max_depth": [3, None],\n              "max_features": [1, 3, 10],\n              "min_samples_split": [1, 3, 10],\n              "min_samples_leaf": [1, 3, 10],\n              "bootstrap": [True, False],\n              "criterion": ["gini", "entropy"]}\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid)\nstart = time()\ngrid_search.fit(X, y)\n\nprint("GridSearchCV took %.2f seconds for %d candidate parameter settings."\n      % (time() - start, len(grid_search.grid_scores_)))\nreport(grid_search.grid_scores_)</code></pre>	http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html
Libsvm GUI	A							http://scikit-learn.org/stable/auto_examples/index.html			A simple graphical frontend for Libsvm mainly intended for didactic purposes. You can create data points by point and click and visualize the decision region induced by different kernels and parameter settings.<br><br><pre><code>from __future__ import division, print_function\n\nprint(__doc__)\n\n# Author: Peter Prettenhoer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport matplotlib\nmatplotlib.use('TkAgg')\n\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nfrom matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg\nfrom matplotlib.figure import Figure\nfrom matplotlib.contour import ContourSet\n\nimport Tkinter as Tk\nimport sys\nimport numpy as np\n\nfrom sklearn import svm\nfrom sklearn.datasets import dump_svmlight_file\nfrom sklearn.externals.six.moves import xrange\n\ny_min, y_max = -50, 50\nx_min, x_max = -50, 50\n\n\nclass Model(object):\n    """The Model which hold the data. It implements the\n    observable in the observer pattern and notifies the\n    registered observers on change event.\n    """\n\n    def __init__(self):\n        self.observers = []\n        self.surface = None\n        self.data = []\n        self.cls = None\n        self.surface_type = 0\n\n    def changed(self, event):\n        """Notify the observers. """\n        for observer in self.observers:\n            observer.update(event, self)\n\n    def add_observer(self, observer):\n        """Register an observer. """\n        self.observers.append(observer)\n\n    def set_surface(self, surface):\n        self.surface = surface\n\n    def dump_svmlight_file(self, file):\n        data = np.array(self.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        dump_svmlight_file(X, y, file)\n\n\nclass Controller(object):\n    def __init__(self, model):\n        self.model = model\n        self.kernel = Tk.IntVar()\n        self.surface_type = Tk.IntVar()\n        # Whether or not a model has been fitted\n        self.fitted = False\n\n    def fit(self):\n        print("fit the model")\n        train = np.array(self.model.data)\n        X = train[:, 0:2]\n        y = train[:, 2]\n\n        C = float(self.complexity.get())\n        gamma = float(self.gamma.get())\n        coef0 = float(self.coef0.get())\n        degree = int(self.degree.get())\n        kernel_map = {0: "linear", 1: "rbf", 2: "poly"}\n        if len(np.unique(y)) == 1:\n            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                                  gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X)\n        else:\n            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                          gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X, y)\n        if hasattr(clf, 'score'):\n            print("Accuracy:", clf.score(X, y) * 100)\n        X1, X2, Z = self.decision_surface(clf)\n        self.model.clf = clf\n        self.model.set_surface((X1, X2, Z))\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed("surface")\n\n    def decision_surface(self, cls):\n        delta = 1\n        x = np.arange(x_min, x_max + delta, delta)\n        y = np.arange(y_min, y_max + delta, delta)\n        X1, X2 = np.meshgrid(x, y)\n        Z = cls.decision_function(np.c_[X1.ravel(), X2.ravel()])\n        Z = Z.reshape(X1.shape)\n        return X1, X2, Z\n\n    def clear_data(self):\n        self.model.data = []\n        self.fitted = False\n        self.model.changed("clear")\n\n    def add_example(self, x, y, label):\n        self.model.data.append((x, y, label))\n        self.model.changed("example_added")\n\n        # update decision surface if already fitted.\n        self.refit()\n\n    def refit(self):\n        """Refit the model if already fitted. """\n        if self.fitted:\n            self.fit()\n\n\nclass View(object):\n    """Test docstring. """\n    def __init__(self, root, controller):\n        f = Figure()\n        ax = f.add_subplot(111)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlim((x_min, x_max))\n        ax.set_ylim((y_min, y_max))\n        canvas = FigureCanvasTkAgg(f, master=root)\n        canvas.show()\n        canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas.mpl_connect('button_press_event', self.onclick)\n        toolbar = NavigationToolbar2TkAgg(canvas, root)\n        toolbar.update()\n        self.controllbar = ControllBar(root, controller)\n        self.f = f\n        self.ax = ax\n        self.canvas = canvas\n        self.controller = controller\n        self.contours = []\n        self.c_labels = None\n        self.plot_kernels()\n\n    def plot_kernels(self):\n        self.ax.text(-50, -60, "Linear: $u^T v$")\n        self.ax.text(-20, -60, "RBF: $\exp (-\gamma \| u-v \|^2)$")\n        self.ax.text(10, -60, "Poly: $(\gamma \, u^T v + r)^d$")\n\n    def onclick(self, event):\n        if event.xdata and event.ydata:\n            if event.button == 1:\n                self.controller.add_example(event.xdata, event.ydata, 1)\n            elif event.button == 3:\n                self.controller.add_example(event.xdata, event.ydata, -1)\n\n    def update_example(self, model, idx):\n        x, y, l = model.data[idx]\n        if l == 1:\n            color = 'w'\n        elif l == -1:\n            color = 'k'\n        self.ax.plot([x], [y], "%so" % color, scalex=0.0, scaley=0.0)\n\n    def update(self, event, model):\n        if event == "examples_loaded":\n            for i in xrange(len(model.data)):\n                self.update_example(model, i)\n\n        if event == "example_added":\n            self.update_example(model, -1)\n\n        if event == "clear":\n            self.ax.clear()\n            self.ax.set_xticks([])\n            self.ax.set_yticks([])\n            self.contours = []\n            self.c_labels = None\n            self.plot_kernels()\n\n        if event == "surface":\n            self.remove_surface()\n            self.plot_support_vectors(model.clf.support_vectors_)\n            self.plot_decision_surface(model.surface, model.surface_type)\n\n        self.canvas.draw()\n\n    def remove_surface(self):\n        """Remove old decision surface."""\n        if len(self.contours) > 0:\n            for contour in self.contours:\n                if isinstance(contour, ContourSet):\n                    for lineset in contour.collections:\n                        lineset.remove()\n                else:\n                    contour.remove()\n            self.contours = []\n\n    def plot_support_vectors(self, support_vectors):\n        """Plot the support vectors by placing circles over the\n        corresponding data points and adds the circle collection\n        to the contours list."""\n        cs = self.ax.scatter(support_vectors[:, 0], support_vectors[:, 1],\n                             s=80, edgecolors="k", facecolors="none")\n        self.contours.append(cs)\n\n    def plot_decision_surface(self, surface, type):\n        X1, X2, Z = surface\n        if type == 0:\n            levels = [-1.0, 0.0, 1.0]\n            linestyles = ['dashed', 'solid', 'dashed']\n            colors = 'k'\n            self.contours.append(self.ax.contour(X1, X2, Z, levels,\n                                                 colors=colors,\n                                                 linestyles=linestyles))\n        elif type == 1:\n            self.contours.append(self.ax.contourf(X1, X2, Z, 10,\n                                                  cmap=matplotlib.cm.bone,\n                                                  origin='lower', alpha=0.85))\n            self.contours.append(self.ax.contour(X1, X2, Z, [0.0], colors='k',\n                                                 linestyles=['solid']))\n        else:\n            raise ValueError("surface type unknown")\n\n\nclass ControllBar(object):\n    def __init__(self, root, controller):\n        fm = Tk.Frame(root)\n        kernel_group = Tk.Frame(fm)\n        Tk.Radiobutton(kernel_group, text="Linear", variable=controller.kernel,\n                       value=0, command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(kernel_group, text="RBF", variable=controller.kernel,\n                       value=1, command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(kernel_group, text="Poly", variable=controller.kernel,\n                       value=2, command=controller.refit).pack(anchor=Tk.W)\n        kernel_group.pack(side=Tk.LEFT)\n\n        valbox = Tk.Frame(fm)\n        controller.complexity = Tk.StringVar()\n        controller.complexity.set("1.0")\n        c = Tk.Frame(valbox)\n        Tk.Label(c, text="C:", anchor="e", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(c, width=6, textvariable=controller.complexity).pack(\n            side=Tk.LEFT)\n        c.pack()\n\n        controller.gamma = Tk.StringVar()\n        controller.gamma.set("0.01")\n        g = Tk.Frame(valbox)\n        Tk.Label(g, text="gamma:", anchor="e", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(g, width=6, textvariable=controller.gamma).pack(side=Tk.LEFT)\n        g.pack()\n\n        controller.degree = Tk.StringVar()\n        controller.degree.set("3")\n        d = Tk.Frame(valbox)\n        Tk.Label(d, text="degree:", anchor="e", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(d, width=6, textvariable=controller.degree).pack(side=Tk.LEFT)\n        d.pack()\n\n        controller.coef0 = Tk.StringVar()\n        controller.coef0.set("0")\n        r = Tk.Frame(valbox)\n        Tk.Label(r, text="coef0:", anchor="e", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(r, width=6, textvariable=controller.coef0).pack(side=Tk.LEFT)\n        r.pack()\n        valbox.pack(side=Tk.LEFT)\n\n        cmap_group = Tk.Frame(fm)\n        Tk.Radiobutton(cmap_group, text="Hyperplanes",\n                       variable=controller.surface_type, value=0,\n                       command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(cmap_group, text="Surface",\n                       variable=controller.surface_type, value=1,\n                       command=controller.refit).pack(anchor=Tk.W)\n\n        cmap_group.pack(side=Tk.LEFT)\n\n        train_button = Tk.Button(fm, text='Fit', width=5,\n                                 command=controller.fit)\n        train_button.pack()\n        fm.pack(side=Tk.LEFT)\n        Tk.Button(fm, text='Clear', width=5,\n                  command=controller.clear_data).pack(side=Tk.LEFT)\n\n\ndef get_parser():\n    from optparse import OptionParser\n    op = OptionParser()\n    op.add_option("--output",\n                  action="store", type="str", dest="output",\n                  help="Path where to dump data.")\n    return op\n\n\ndef main(argv):\n    op = get_parser()\n    opts, args = op.parse_args(argv[1:])\n    root = Tk.Tk()\n    model = Model()\n    controller = Controller(model)\n    root.wm_title("Scikit-learn Libsvm GUI")\n    view = View(root, controller)\n    model.add_observer(view)\n    Tk.mainloop()\n\n    if opts.output:\n        model.dump_svmlight_file(opts.output)\n\nif __name__ == "__main__":\n    main(sys.argv)</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/svm_gui.html
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation	A							http://scikit-learn.org/stable/auto_examples/index.html			This is an example of applying Non-negative Matrix Factorization and Latent Dirichlet Allocation on a corpus of documents and extract additive models of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).<br><br><pre><code># Author: Olivier Grisel <olivier.grisel@ensta.org>\n#         Lars Buitinck <L.J.Buitinck@uva.nl>\n#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\nfrom time import time\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.datasets import fetch_20newsgroups\n\nn_samples = 2000\nn_features = 1000\nn_topics = 10\nn_top_words = 20\n\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print("Topic #%d:" % topic_idx)\n        print(" ".join([feature_names[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n    print()\n\n\n# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n# to filter out useless terms early on: the posts are stripped of headers,\n# footers and quoted replies, and common English words, words occurring in\n# only one document or in at least 95% of the documents are removed.\n\nprint("Loading dataset...")\nt0 = time()\ndataset = fetch_20newsgroups(shuffle=True, random_state=1,\n                             remove=('headers', 'footers', 'quotes'))\ndata_samples = dataset.data\nprint("done in %0.3fs." % (time() - t0))\n\n# Use tf-idf features for NMF.\nprint("Extracting tf-idf features for NMF...")\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, #max_features=n_features,\n                                   stop_words='english')\nt0 = time()\ntfidf = tfidf_vectorizer.fit_transform(data_samples)\nprint("done in %0.3fs." % (time() - t0))\n\n# Use tf (raw term count) features for LDA.\nprint("Extracting tf features for LDA...")\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n                                stop_words='english')\nt0 = time()\ntf = tf_vectorizer.fit_transform(data_samples)\nprint("done in %0.3fs." % (time() - t0))\n\n# Fit the NMF model\nprint("Fitting the NMF model with tf-idf features,"\n      "n_samples=%d and n_features=%d..."\n      % (n_samples, n_features))\nt0 = time()\nnmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\nexit()\nprint("done in %0.3fs." % (time() - t0))\n\nprint("\nTopics in NMF model:")\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, n_top_words)\n\nprint("Fitting LDA models with tf features, n_samples=%d and n_features=%d..."\n      % (n_samples, n_features))\nlda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n                                learning_method='online', learning_offset=50.,\n                                random_state=0)\nt0 = time()\nlda.fit(tf)\nprint("done in %0.3fs." % (time() - t0))\n\nprint("\nTopics in LDA model:")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf_lda.html
Wikipedia principal eigenvector	A							http://scikit-learn.org/stable/auto_examples/index.html			A classical way to assert the relative importance of vertices in a graph is to compute the principal eigenvector of the adjacency matrix so as to assign to each vertex the values of the components of the first eigenvector as a centrality score:<br><br><pre><code># Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nfrom __future__ import print_function\n\nfrom bz2 import BZ2File\nimport os\nfrom datetime import datetime\nfrom pprint import pprint\nfrom time import time\n\nimport numpy as np\n\nfrom scipy import sparse\n\nfrom sklearn.decomposition import randomized_svd\nfrom sklearn.externals.joblib import Memory\nfrom sklearn.externals.six.moves.urllib.request import urlopen\nfrom sklearn.externals.six import iteritems\n\n\nprint(__doc__)\n\n###############################################################################\n# Where to download the data, if not already on disk\nredirects_url = "http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2"\nredirects_filename = redirects_url.rsplit("/", 1)[1]\n\npage_links_url = "http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2"\npage_links_filename = page_links_url.rsplit("/", 1)[1]\n\nresources = [\n    (redirects_url, redirects_filename),\n    (page_links_url, page_links_filename),\n]\n\nfor url, filename in resources:\n    if not os.path.exists(filename):\n        print("Downloading data from '%s', please wait..." % url)\n        opener = urlopen(url)\n        open(filename, 'wb').write(opener.read())\n        print()\n\n\n###############################################################################\n# Loading the redirect files\n\nmemory = Memory(cachedir=".")\n\n\ndef index(redirects, index_map, k):\n    """Find the index of an article name after redirect resolution"""\n    k = redirects.get(k, k)\n    return index_map.setdefault(k, len(index_map))\n\n\nDBPEDIA_RESOURCE_PREFIX_LEN = len("http://dbpedia.org/resource/")\nSHORTNAME_SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)\n\n\ndef short_name(nt_uri):\n    """Remove the < and > URI markers and the common URI prefix"""\n    return nt_uri[SHORTNAME_SLICE]\n\n\ndef get_redirects(redirects_filename):\n    """Parse the redirections and build a transitively closed map out of it"""\n    redirects = {}\n    print("Parsing the NT redirect file")\n    for l, line in enumerate(BZ2File(redirects_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print("ignoring malformed line: " + line)\n            continue\n        redirects[short_name(split[0])] = short_name(split[2])\n        if l % 1000000 == 0:\n            print("[%s] line: %08d" % (datetime.now().isoformat(), l))\n\n    # compute the transitive closure\n    print("Computing the transitive closure of the redirect relation")\n    for l, source in enumerate(redirects.keys()):\n        transitive_target = None\n        target = redirects[source]\n        seen = set([source])\n        while True:\n            transitive_target = target\n            target = redirects.get(target)\n            if target is None or target in seen:\n                break\n            seen.add(target)\n        redirects[source] = transitive_target\n        if l % 1000000 == 0:\n            print("[%s] line: %08d" % (datetime.now().isoformat(), l))\n\n    return redirects\n\n\n# disabling joblib as the pickling of large dicts seems much too slow\n#@memory.cache\ndef get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):\n    """Extract the adjacency graph as a scipy sparse matrix\n\n    Redirects are resolved first.\n\n    Returns X, the scipy sparse adjacency matrix, redirects as python\n    dict from article names to article names and index_map a python dict\n    from article names to python int (article indexes).\n    """\n\n    print("Computing the redirect map")\n    redirects = get_redirects(redirects_filename)\n\n    print("Computing the integer index map")\n    index_map = dict()\n    links = list()\n    for l, line in enumerate(BZ2File(page_links_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print("ignoring malformed line: " + line)\n            continue\n        i = index(redirects, index_map, short_name(split[0]))\n        j = index(redirects, index_map, short_name(split[2]))\n        links.append((i, j))\n        if l % 1000000 == 0:\n            print("[%s] line: %08d" % (datetime.now().isoformat(), l))\n\n        if limit is not None and l >= limit - 1:\n            break\n\n    print("Computing the adjacency matrix")\n    X = sparse.lil_matrix((len(index_map), len(index_map)), dtype=np.float32)\n    for i, j in links:\n        X[i, j] = 1.0\n    del links\n    print("Converting to CSR representation")\n    X = X.tocsr()\n    print("CSR conversion done")\n    return X, redirects, index_map\n\n\n# stop after 5M links to make it possible to work in RAM\nX, redirects, index_map = get_adjacency_matrix(\n    redirects_filename, page_links_filename, limit=5000000)\nnames = dict((i, name) for name, i in iteritems(index_map))\n\nprint("Computing the principal singular vectors using randomized_svd")\nt0 = time()\nU, s, V = randomized_svd(X, 5, n_iter=3)\nprint("done in %0.3fs" % (time() - t0))\n\n# print the names of the wikipedia related strongest compenents of the the\n# principal singular vector which should be similar to the highest eigenvector\nprint("Top wikipedia pages according to principal singular vectors")\npprint([names[i] for i in np.abs(U.T[0]).argsort()[-10:]])\npprint([names[i] for i in np.abs(V[0]).argsort()[-10:]])\n\n\ndef centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):\n    """Power iteration computation of the principal eigenvector\n\n    This method is also known as Google PageRank and the implementation\n    is based on the one from the NetworkX project (BSD licensed too)\n    with copyrights by:\n\n      Aric Hagberg <hagberg@lanl.gov>\n      Dan Schult <dschult@colgate.edu>\n      Pieter Swart <swart@lanl.gov>\n    """\n    n = X.shape[0]\n    X = X.copy()\n    incoming_counts = np.asarray(X.sum(axis=1)).ravel()\n\n    print("Normalizing the graph")\n    for i in incoming_counts.nonzero()[0]:\n        X.data[X.indptr[i]:X.indptr[i + 1]] *= 1.0 / incoming_counts[i]\n    dangle = np.asarray(np.where(X.sum(axis=1) == 0, 1.0 / n, 0)).ravel()\n\n    scores = np.ones(n, dtype=np.float32) / n  # initial guess\n    for i in range(max_iter):\n        print("power iteration #%d" % i)\n        prev_scores = scores\n        scores = (alpha * (scores * X + np.dot(dangle, prev_scores))\n                  + (1 - alpha) * prev_scores.sum() / n)\n        # check convergence: normalized l_inf norm\n        scores_max = np.abs(scores).max()\n        if scores_max == 0.0:\n            scores_max = 1.0\n        err = np.abs(scores - prev_scores).max() / scores_max\n        print("error: %0.6f" % err)\n        if err < n * tol:\n            return scores\n\n    return scores\n\nprint("Computing principal eigenvector score using a power iteration method")\nt0 = time()\nscores = centrality_scores(X, max_iter=100, tol=1e-10)\nprint("done in %0.3fs" % (time() - t0))\npprint([names[i] for i in np.abs(scores).argsort()[-10:]])</code></pre>	http://scikit-learn.org/stable/auto_examples/applications/wikipedia_principal_eigenvector.html
